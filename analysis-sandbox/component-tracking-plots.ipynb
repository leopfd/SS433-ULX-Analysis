{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd04a3e-0dfd-4812-85fc-19b6741a3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import defaultdict\n",
    "\n",
    "# plotting style configuration\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# analysis configuration\n",
    "base_dir = '/Users/leodrake/Documents/MIT/ss433/HRC_2024/2Dfits'\n",
    "center_pixel = 80.5\n",
    "g1_component = 'core'\n",
    "\n",
    "# dynamic file configuration\n",
    "num_comps = 4\n",
    "sigma_val = 1\n",
    "bin_size = 0.25 \n",
    "\n",
    "# signifiers: add or remove strings here (e.g., 'jittercorr', 'empPSF', 'mcmc')\n",
    "# they will be automatically joined by dashes.\n",
    "signifiers = ['empPSF', 'mcmc']\n",
    "\n",
    "# automatic filename construction\n",
    "# create the bin string (e.g., 0.25 -> \"0p25\", 1 -> \"1\")\n",
    "bin_str = str(bin_size).replace('.', 'p')\n",
    "\n",
    "# create the signifiers string (e.g., \"jittercorr-empPSF\")\n",
    "signifiers_str = \"-\".join(signifiers)\n",
    "\n",
    "# construct the common suffix used by all files\n",
    "# format: ncomp-nsigma-{signifiers}-bin{n}\n",
    "# example: 4comp-1sigma-jittercorr-empPSF-bin0p25\n",
    "file_identifier = f\"{num_comps}comp-{sigma_val}sigma-{signifiers_str}-bin{bin_str}\"\n",
    "\n",
    "# calculate pixel scale based on bin size\n",
    "pixscale_arcsec = 0.13175 * bin_size \n",
    "\n",
    "print(f\"file id set to: {file_identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb4dc1-53d5-4b87-9f62-1a409ffe7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sherpa_log_to_dataframe(filename):\n",
    "    \"\"\"\n",
    "    reads raw sherpa log, dynamically renames components \n",
    "    (core, east, west) based on physics, and returns a dataframe.\n",
    "    automatically handles both standard confidence bounds and mcmc (emcee) results.\n",
    "    \"\"\"\n",
    "    # compiled regex for performance\n",
    "    obs_id_re = re.compile(r\"Observation:\\s*(\\d+)\")\n",
    "    date_re = re.compile(r\"Date:\\s*([\\d\\.]+).*?Exptime:\\s*([\\d\\.]+)\")\n",
    "    \n",
    "    # regex for standard confidence bounds\n",
    "    # format: name | val | low | up\n",
    "    conf_re = re.compile(r\"^\\s*(g\\d+|c\\d+)\\.(?P<param>[a-z0-9]+)\\s+(?P<val>[-\\d.eE]+)\\s+(?P<low>[-\\d.eE]+|-------)\\s+(?P<up>[-\\d.eE]+|-------)\", re.M)\n",
    "\n",
    "    # regex for mcmc (emcee) results\n",
    "    # format: name | val | median | low (-error) | up (+error)\n",
    "    # we capture val, low, up and ignore median\n",
    "    mcmc_re = re.compile(r\"^\\s*(g\\d+|c\\d+)\\.(?P<param>[a-z0-9]+)\\s+(?P<val>[-\\d.eE]+)\\s+(?:[-\\d.eE]+)\\s+(?P<low>[-\\d.eE]+)\\s+(?P<up>[-\\d.eE]+)\", re.M)\n",
    "    \n",
    "    # regex for count rates\n",
    "    rate_line_re = re.compile(r\"^\\s*(g\\d+|c\\d+)\\s*:\\s*(?P<val>[-\\d.eE]+)\\s*\\((?P<low>[-\\d.eE+]+)\\s*/\\s*(?P<up>[-\\d.eE+]+)\\)\", re.M)\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"error: file not found at {filename}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # split into blocks by observation\n",
    "    obs_blocks = re.split(r'(?=Observation:)', raw_text)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for block in obs_blocks:\n",
    "        if not block.strip(): continue\n",
    "\n",
    "        # 1. extract metadata\n",
    "        obs_match = obs_id_re.search(block)\n",
    "        if not obs_match: continue\n",
    "        obs_id = int(obs_match.group(1))\n",
    "        \n",
    "        date_match = date_re.search(block)\n",
    "        if date_match:\n",
    "            mjd, exptime = float(date_match.group(1)), float(date_match.group(2))\n",
    "        else:\n",
    "            mjd, exptime = (np.nan, np.nan)\n",
    "\n",
    "        # 2. extract components and parameters\n",
    "        comps = defaultdict(dict)\n",
    "        \n",
    "        # determine which regex to use based on block content\n",
    "        if \"emcee Results\" in block:\n",
    "            target_re = mcmc_re\n",
    "        else:\n",
    "            target_re = conf_re\n",
    "            \n",
    "        for match in target_re.finditer(block):\n",
    "            c_id = match.group(1)\n",
    "            param = match.group('param')\n",
    "            val = float(match.group('val'))\n",
    "            low_str = match.group('low')\n",
    "            up_str = match.group('up')\n",
    "            comps[c_id][param] = (val, low_str, up_str)\n",
    "\n",
    "        # 3. extract count rates\n",
    "        rates = {}\n",
    "        for match in rate_line_re.finditer(block):\n",
    "            c_id = match.group(1)\n",
    "            val = float(match.group('val'))\n",
    "            low_val = float(match.group('low'))\n",
    "            up_val = float(match.group('up'))\n",
    "            \n",
    "            # calculate absolute errors for rates\n",
    "            if low_val < 0:\n",
    "                minus_err = abs(low_val)\n",
    "                plus_err = abs(up_val)\n",
    "            else:\n",
    "                minus_err = abs(up_val)\n",
    "                plus_err = abs(low_val)\n",
    "            rates[c_id] = (val, minus_err, plus_err)\n",
    "\n",
    "        # 4. logic: identify core, east, west based on relative position\n",
    "        g_ids = [k for k in comps.keys() if k.startswith('g')]\n",
    "        if not g_ids: continue \n",
    "\n",
    "        def get_val(cid, p): return comps[cid].get(p, (0,0,0))[0]\n",
    "\n",
    "        # a. identify core (max amplitude)\n",
    "        core_id = max(g_ids, key=lambda c: get_val(c, 'ampl'))\n",
    "        core_x = get_val(core_id, 'xpos')\n",
    "        \n",
    "        mapping = {core_id: 'core'}\n",
    "        \n",
    "        # b. split remaining into left (east candidates) and right (west candidates)\n",
    "        remaining = [c for c in g_ids if c != core_id]\n",
    "        left_cands = [c for c in remaining if get_val(c, 'xpos') < core_x]\n",
    "        right_cands = [c for c in remaining if get_val(c, 'xpos') >= core_x]\n",
    "        \n",
    "        extras = []\n",
    "\n",
    "        # c. assign east\n",
    "        if left_cands:\n",
    "            left_sorted = sorted(left_cands, key=lambda c: get_val(c, 'xpos'), reverse=True)\n",
    "            mapping[left_sorted[0]] = 'east'\n",
    "            if len(left_sorted) > 1:\n",
    "                extras.extend(left_sorted[1:])\n",
    "        \n",
    "        # d. assign west\n",
    "        if right_cands:\n",
    "            right_sorted = sorted(right_cands, key=lambda c: get_val(c, 'xpos'))\n",
    "            mapping[right_sorted[0]] = 'west'\n",
    "            if len(right_sorted) > 1:\n",
    "                extras.extend(right_sorted[1:])\n",
    "                \n",
    "        # e. label extras\n",
    "        extras_sorted = sorted(extras, key=lambda c: get_val(c, 'xpos'))\n",
    "        for i, eid in enumerate(extras_sorted, start=1):\n",
    "            mapping[eid] = f'extra_{i}'\n",
    "        \n",
    "        # f. background\n",
    "        for c_id in comps:\n",
    "            if c_id.startswith('c'):\n",
    "                mapping[c_id] = 'bkg'\n",
    "\n",
    "        # 5. build rows\n",
    "        for old_id, new_name in mapping.items():\n",
    "            if old_id not in comps: continue\n",
    "            \n",
    "            row = {\n",
    "                'obs_id': obs_id,\n",
    "                'mjd': mjd,\n",
    "                'exptime': exptime,\n",
    "                'component': new_name\n",
    "            }\n",
    "            \n",
    "            # flatten parameters\n",
    "            for param, (val, low_s, up_s) in comps[old_id].items():\n",
    "                row[param] = val\n",
    "                row[f'{param}_minus'] = low_s\n",
    "                row[f'{param}_plus'] = up_s\n",
    "            \n",
    "            # add count rate data\n",
    "            if old_id in rates:\n",
    "                r_val, r_min, r_plus = rates[old_id]\n",
    "                row['nominal'] = r_val\n",
    "                row['minus_err'] = r_min\n",
    "                row['plus_err'] = r_plus\n",
    "            else:\n",
    "                row['nominal'] = np.nan\n",
    "                row['minus_err'] = np.nan\n",
    "                row['plus_err'] = np.nan\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 6. cleanup types\n",
    "    err_cols = [c for c in df.columns if c.endswith('_minus') or c.endswith('_plus')]\n",
    "    for col in err_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').abs()\n",
    "    \n",
    "    if df['mjd'].isna().all() and 'obs_id' in df.columns:\n",
    "        df['mjd'] = df['obs_id'].astype('category').cat.codes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12614a-b7b4-4922-8491-94711cd74095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data directly from raw log\n",
    "# construct the raw filename using the identifier from cell 1\n",
    "raw_filename = f'multi-comp-fit-results-{file_identifier}.txt'\n",
    "raw_file_path = os.path.join(base_dir, 'multi comp fit results', raw_filename)\n",
    "\n",
    "print(f\"loading and processing: {raw_filename}\")\n",
    "df = load_sherpa_log_to_dataframe(raw_file_path)\n",
    "\n",
    "# check if data loaded\n",
    "if df.empty:\n",
    "    raise ValueError(\"dataframe is empty. check if the file exists and has correct format.\")\n",
    "\n",
    "# save 1: component tracker table (raw positions)\n",
    "# destination: 2dfits/comp tracker tables/\n",
    "tracker_dir = os.path.join(base_dir, 'comp tracker tables')\n",
    "os.makedirs(tracker_dir, exist_ok=True)\n",
    "\n",
    "tracker_filename = f'comp-tracker-table-{file_identifier}.csv'\n",
    "tracker_file_path = os.path.join(tracker_dir, tracker_filename)\n",
    "\n",
    "df.to_csv(tracker_file_path, index=False)\n",
    "print(f\"tracker table (raw) saved to: {tracker_file_path}\")\n",
    "\n",
    "# processing: recenter and calculate physics\n",
    "\n",
    "# vectorized recenter on core\n",
    "# isolate reference (core) positions\n",
    "ref_df = df[df['component'] == g1_component][['obs_id', 'xpos', 'ypos']]\n",
    "ref_df = ref_df.rename(columns={'xpos': 'ref_x', 'ypos': 'ref_y'})\n",
    "\n",
    "# merge reference positions back into the main dataframe\n",
    "df = df.merge(ref_df, on='obs_id', how='left')\n",
    "\n",
    "# fill missing reference positions with center pixel\n",
    "df['ref_x'] = df['ref_x'].fillna(center_pixel)\n",
    "df['ref_y'] = df['ref_y'].fillna(center_pixel)\n",
    "\n",
    "# calculate offsets vectorized\n",
    "df['dx'] = df['ref_x'] - center_pixel\n",
    "df['dy'] = df['ref_y'] - center_pixel\n",
    "\n",
    "# apply displacement\n",
    "df['xpos'] -= df['dx']\n",
    "df['ypos'] -= df['dy']\n",
    "\n",
    "# cleanup columns\n",
    "df.drop(columns=['dx', 'dy', 'ref_x', 'ref_y'], inplace=True)\n",
    "\n",
    "# calculate offsets, pa, radius, and propagate errors\n",
    "df['xoff'] = df['xpos'] - center_pixel\n",
    "df['yoff'] = df['ypos'] - center_pixel\n",
    "\n",
    "pa_rad = np.arctan2(-df['xoff'], df['yoff'])\n",
    "df['PA'] = np.degrees(pa_rad)\n",
    "df['pa_rad'] = pa_rad \n",
    "\n",
    "d2 = df['xoff']**2 + df['yoff']**2\n",
    "dpa_dx = np.divide(-df['yoff'], d2, out=np.full_like(d2, np.nan), where=d2 != 0)\n",
    "dpa_dy = np.divide(df['xoff'], d2, out=np.full_like(d2, np.nan), where=d2 != 0)\n",
    "\n",
    "df['PA_err_plus'] = np.degrees(np.sqrt((dpa_dx * df['xpos_plus'])**2 + (dpa_dy * df['ypos_plus'])**2))\n",
    "df['PA_err_minus'] = np.degrees(np.sqrt((dpa_dx * df['xpos_minus'])**2 + (dpa_dy * df['ypos_minus'])**2))\n",
    "\n",
    "df['radius'] = np.hypot(df['xoff'], df['yoff']) * pixscale_arcsec\n",
    "\n",
    "r_pix = df['radius'] / pixscale_arcsec\n",
    "is_zero = np.isclose(r_pix, 0)\n",
    "df['radius_plus_err'] = np.where(is_zero, np.hypot(df['xpos_plus'], df['ypos_plus']), np.sqrt((df['xoff']*df['xpos_plus'])**2 + (df['yoff']*df['ypos_plus'])**2)/r_pix) * pixscale_arcsec\n",
    "df['radius_minus_err'] = np.where(is_zero, np.hypot(df['xpos_minus'], df['ypos_minus']), np.sqrt((df['xoff']*df['xpos_minus'])**2 + (df['yoff']*df['ypos_minus'])**2)/r_pix) * pixscale_arcsec\n",
    "\n",
    "# sort by time\n",
    "df.sort_values('mjd', inplace=True)\n",
    "df['flag'] = 'clean'\n",
    "\n",
    "# save 2: data table (processed physics)\n",
    "# destination: 2dfits/data tables/\n",
    "data_dir = os.path.join(base_dir, 'data tables')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "data_filename = f'data-table-{file_identifier}.csv'\n",
    "data_file_path = os.path.join(data_dir, data_filename)\n",
    "\n",
    "df.to_csv(data_file_path, index=False)\n",
    "print(f\"data table (processed) saved to: {data_file_path}\")\n",
    "\n",
    "# create pivoted views for plotting\n",
    "pivoted = df.pivot_table(index='mjd', columns='component', values=['nominal', 'plus_err', 'minus_err'])\n",
    "df_nom, df_plus, df_minus = [pivoted[val].sort_index() for val in ['nominal', 'plus_err', 'minus_err']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecc88b-85f4-40ed-9d4e-2fa1e1e25b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_comp = df.groupby('component')\n",
    "\n",
    "# exclude the reference component (core) from the list\n",
    "comps = [c for c in df_nom.columns if c != g1_component]\n",
    "\n",
    "# define specific colormaps for each component type\n",
    "# east -> blues, west -> reds, extra -> greens\n",
    "comp_cmaps = {}\n",
    "for c in comps:\n",
    "    if c == 'east':\n",
    "        comp_cmaps[c] = plt.cm.Blues\n",
    "    elif c == 'west':\n",
    "        comp_cmaps[c] = plt.cm.Reds\n",
    "    elif c.startswith('extra'):\n",
    "        comp_cmaps[c] = plt.cm.Greens\n",
    "    else:\n",
    "        # fallback\n",
    "        comp_cmaps[c] = plt.cm.Purples\n",
    "\n",
    "# setup time normalization\n",
    "time_min = df['mjd'].min()\n",
    "time_max = df['mjd'].max()\n",
    "time_norm = plt.Normalize(vmin=time_min, vmax=time_max)\n",
    "\n",
    "n = len(comps)\n",
    "delta = 0.02\n",
    "offsets = {c: (i - (n - 1) / 2) * delta for i, c in enumerate(comps)}\n",
    "\n",
    "# dynamic pdf name\n",
    "plots_dir = os.path.join(base_dir, 'comp tracker plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "pdf_name = f'comp-tracker-plots-{file_identifier}.pdf'\n",
    "pdf_filename = os.path.join(plots_dir, pdf_name)\n",
    "\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    # first figure: pa vs time + stacked count rates\n",
    "    # create a discrete color cycle for the standard plots\n",
    "    discrete_colors = ['dodgerblue', 'mediumseagreen', 'mediumslateblue', 'lightcoral']\n",
    "    comp_discrete_map = {comp: discrete_colors[i % len(discrete_colors)] for i, comp in enumerate(comps)}\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs  = GridSpec(n, 2, figure=fig, width_ratios=[1,1], hspace=0, wspace=0.3)\n",
    "\n",
    "    # left: pa vs time\n",
    "    ax_pa = fig.add_subplot(gs[:,0])\n",
    "    for comp, color in comp_discrete_map.items():\n",
    "        if comp in grouped_by_comp.groups:\n",
    "            grp = grouped_by_comp.get_group(comp)\n",
    "            \n",
    "            ax_pa.errorbar(\n",
    "                grp['mjd'], grp['PA'],\n",
    "                yerr=[grp['PA_err_minus'], grp['PA_err_plus']],\n",
    "                marker='.', linestyle='-', capsize=3, color=color, label=comp\n",
    "            )\n",
    "            \n",
    "    ax_pa.set_ylabel('Position Angle (°)')\n",
    "    ax_pa.set_xlabel('MJD')\n",
    "    ax_pa.set_title('Position Angle vs Time')\n",
    "    ax_pa.set_ylim(-180,180)\n",
    "    ax_pa.grid(True)\n",
    "    ax_pa.legend()\n",
    "\n",
    "    # right: stacked count-rate panels\n",
    "    ax_bottom = None\n",
    "    for i_comp, focus in reversed(list(enumerate(comps))):\n",
    "        if i_comp == n - 1:\n",
    "            ax = fig.add_subplot(gs[i_comp, 1])\n",
    "            ax.set_xlabel('MJD')\n",
    "            ax_bottom = ax\n",
    "        else:\n",
    "            ax = fig.add_subplot(gs[i_comp, 1], sharex=ax_bottom)\n",
    "            ax.tick_params(labelbottom=False)\n",
    "        ax.grid(True, zorder=0)\n",
    "        \n",
    "        for comp_name, current_color in comp_discrete_map.items():\n",
    "            if comp_name not in df_nom.columns: continue\n",
    "            \n",
    "            x_val = df_nom.index + offsets[comp_name]\n",
    "            y_val = df_nom[comp_name]\n",
    "            y_err_val = [df_minus[comp_name], df_plus[comp_name]]\n",
    "            \n",
    "            alpha_val, line_style, label_text, z_order = (1.0, '-', comp_name, 10) if comp_name == focus else (0.3, '', None, 1)\n",
    "            \n",
    "            ax.errorbar(\n",
    "                x_val, y_val, yerr=y_err_val, color=current_color,\n",
    "                marker='.', linestyle=line_style, capsize=3,\n",
    "                alpha=alpha_val, label=label_text, zorder=z_order\n",
    "            )\n",
    "\n",
    "        ax.set_yticks([0.1,0.3])\n",
    "        if ax.has_data():\n",
    "             ax.legend(loc='upper left')\n",
    "\n",
    "    fig.text(0.73, 0.885, 'Component Count Rates', ha='center', va='bottom', fontsize=17)\n",
    "    fig.text(0.495, 0.5, 'Count rate (counts/s)', va='center', rotation='vertical')\n",
    "    \n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig) \n",
    "\n",
    "    # second figure: polar plot of pa on sky\n",
    "    fig_polar = plt.figure(figsize=(10, 8))\n",
    "    ax_polar = fig_polar.add_subplot(111, projection='polar')\n",
    "    ax_polar.set_theta_zero_location('N')\n",
    "    ax_polar.set_theta_direction(1)\n",
    "    ax_polar.set_thetamin(-180)\n",
    "    ax_polar.set_thetamax(180)\n",
    "    ax_polar.set_rlabel_position(135)\n",
    "\n",
    "    # plot loop\n",
    "    for comp in comps:\n",
    "        if comp in grouped_by_comp.groups:\n",
    "            grp = grouped_by_comp.get_group(comp)\n",
    "            cmap = comp_cmaps.get(comp, plt.cm.Greys)\n",
    "            \n",
    "            # iterate row by row to color each point by its time\n",
    "            for _, row in grp.iterrows():\n",
    "                # map mjd to range 0.4-1.0 to ensure visibility\n",
    "                normalized_t = time_norm(row['mjd'])\n",
    "                color_idx = 0.4 + 0.6 * normalized_t\n",
    "                t_color = cmap(color_idx)\n",
    "                \n",
    "                ax_polar.errorbar(\n",
    "                    row['pa_rad'], row['radius'],\n",
    "                    xerr=[[np.deg2rad(row['PA_err_minus'])], [np.deg2rad(row['PA_err_plus'])]],\n",
    "                    yerr=[[row['radius_minus_err']], [row['radius_plus_err']]],\n",
    "                    marker='.', linestyle='', color=t_color, capsize=2, markersize=8\n",
    "                )\n",
    "\n",
    "    # add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Greys, norm=time_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax_polar, pad=0.1)\n",
    "    cbar.set_label('MJD (Light=Early, Dark=Late)')\n",
    "\n",
    "    # add legend for component colors\n",
    "    legend_elements = []\n",
    "    for c in comps:\n",
    "        cm = comp_cmaps.get(c, plt.cm.Greys)\n",
    "        c_color = cm(0.7) \n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], marker='.', color=c_color, label=c, markerfacecolor=c_color, markersize=8, linestyle='None')\n",
    "        )\n",
    "        \n",
    "    ax_polar.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.3, 1.1), title=\"Components\")\n",
    "\n",
    "    angles = np.arange(-150, 180, 30)\n",
    "    angles = np.append(angles, 180)\n",
    "    ax_polar.set_rmin(0)\n",
    "    ax_polar.set_thetagrids(angles, [f\"{int(a)}°\" for a in angles])\n",
    "    ax_polar.set_title('On Sky Component Positions (arcsec)')\n",
    "\n",
    "    pdf.savefig(fig_polar)\n",
    "    plt.close(fig_polar)\n",
    "    \n",
    "print(f\"plots saved to: {pdf_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CIAO-4.17)",
   "language": "python",
   "name": "ciao-4.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
