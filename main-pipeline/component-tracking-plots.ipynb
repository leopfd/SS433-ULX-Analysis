{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd04a3e-0dfd-4812-85fc-19b6741a3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import defaultdict\n",
    "\n",
    "# plotting style configuration\n",
    "plt.rcParams['figure.dpi'] = 400\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# analysis configuration\n",
    "base_dir = '/Users/leodrake/Documents/MIT/ss433/HRC_2024/2Dfits'\n",
    "center_pixel = 80.5\n",
    "g1_component = 'core'\n",
    "\n",
    "# dynamic file configuration\n",
    "num_comps = 4\n",
    "sigma_val = 1\n",
    "bin_size = 0.25 \n",
    "\n",
    "# signifiers: add or remove strings here (e.g., 'jittercorr', 'empPSF', 'mcmc')\n",
    "# they will be automatically joined by dashes.\n",
    "signifiers = ['empPSF', 'mcmc']\n",
    "\n",
    "# automatic filename construction\n",
    "# create the bin string (e.g., 0.25 -> \"0p25\", 1 -> \"1\")\n",
    "bin_str = str(bin_size).replace('.', 'p')\n",
    "\n",
    "# create the signifiers string (e.g., \"jittercorr-empPSF\")\n",
    "signifiers_str = \"-\".join(signifiers)\n",
    "\n",
    "# construct the common suffix used by all files\n",
    "# format: ncomp-nsigma-{signifiers}-bin{n}\n",
    "# example: 4comp-1sigma-jittercorr-empPSF-bin0p25\n",
    "file_identifier = f\"{num_comps}comp-{sigma_val}sigma-{signifiers_str}-bin{bin_str}\"\n",
    "\n",
    "# calculate pixel scale based on bin size\n",
    "pixscale_arcsec = 0.13175 * bin_size \n",
    "\n",
    "print(f\"file id set to: {file_identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb4dc1-53d5-4b87-9f62-1a409ffe7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sherpa_log_to_dataframe(filename):\n",
    "    \"\"\"\n",
    "    reads raw sherpa log, dynamically renames components \n",
    "    (core, east, west) based on physics, and returns a dataframe.\n",
    "    \"\"\"\n",
    "    # compiled regex for performance\n",
    "    obs_id_re = re.compile(r\"Observation:\\s*(\\d+)\")\n",
    "    date_re = re.compile(r\"Date:\\s*([\\d\\.]+).*?Exptime:\\s*([\\d\\.]+)\")\n",
    "    \n",
    "    # regex for parameters (captures g1.xpos, g2.ampl, etc.)\n",
    "    param_line_re = re.compile(r\"^\\s*(g\\d+|c\\d+)\\.(?P<param>[a-z0-9]+)\\s+(?P<val>[-\\d.eE]+)\\s+(?P<low>[-\\d.eE]+|-------)\\s+(?P<up>[-\\d.eE]+|-------)\", re.M)\n",
    "    \n",
    "    # NEW: regex for count rates to get nominal, plus_err, minus_err\n",
    "    # Matches lines like: \"  g1    :  1.5056  (-0.2768/+0.3018)\"\n",
    "    rate_line_re = re.compile(r\"^\\s*(g\\d+|c\\d+)\\s*:\\s*(?P<val>[-\\d.eE]+)\\s*\\((?P<low>[-\\d.eE+]+)\\s*/\\s*(?P<up>[-\\d.eE+]+)\\)\", re.M)\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"error: file not found at {filename}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # split into blocks by observation\n",
    "    obs_blocks = re.split(r'(?=Observation:)', raw_text)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for block in obs_blocks:\n",
    "        if not block.strip(): continue\n",
    "\n",
    "        # 1. extract metadata\n",
    "        obs_match = obs_id_re.search(block)\n",
    "        if not obs_match: continue\n",
    "        obs_id = int(obs_match.group(1))\n",
    "        \n",
    "        date_match = date_re.search(block)\n",
    "        if date_match:\n",
    "            mjd, exptime = float(date_match.group(1)), float(date_match.group(2))\n",
    "        else:\n",
    "            mjd, exptime = (np.nan, np.nan)\n",
    "\n",
    "        # 2. extract components and parameters into temp dict\n",
    "        # comps[comp_id][param] = (val, low, up)\n",
    "        comps = defaultdict(dict)\n",
    "        \n",
    "        for match in param_line_re.finditer(block):\n",
    "            c_id = match.group(1)\n",
    "            param = match.group('param')\n",
    "            val = float(match.group('val'))\n",
    "            low_str = match.group('low')\n",
    "            up_str = match.group('up')\n",
    "            # if inputs are absolute bounds (lower, upper), calculate deltas here.\n",
    "            # if revert is needed, replace this block with:\n",
    "            # comps[c_id][param] = (val, low_str, up_str)\n",
    "            try:\n",
    "                low_abs = float(low_str)\n",
    "                up_abs = float(up_str)\n",
    "                # calculate delta from best fit value\n",
    "                # minus_err = |val - lower|\n",
    "                # plus_err = |upper - val|\n",
    "                low_delta = abs(val - low_abs)\n",
    "                up_delta = abs(up_abs - val)\n",
    "                comps[c_id][param] = (val, low_delta, up_delta)\n",
    "            except ValueError:\n",
    "                # handles '-------' cases\n",
    "                comps[c_id][param] = (val, low_str, up_str)\n",
    "\n",
    "        # 3. extract count rates into temp dict\n",
    "        # rates[comp_id] = (val, low, up)\n",
    "        rates = {}\n",
    "        for match in rate_line_re.finditer(block):\n",
    "            c_id = match.group(1)\n",
    "            val = float(match.group('val'))\n",
    "            low_val = float(match.group('low'))\n",
    "            up_val = float(match.group('up'))\n",
    "            # Calculate absolute errors\n",
    "            if low_val < 0:\n",
    "                minus_err = abs(low_val)\n",
    "                plus_err = abs(up_val)\n",
    "            else:\n",
    "                minus_err = abs(up_val)\n",
    "                plus_err = abs(low_val)\n",
    "            rates[c_id] = (val, minus_err, plus_err)\n",
    "\n",
    "        # 4. logic: identify core, east, west\n",
    "        g_ids = [k for k in comps.keys() if k.startswith('g')]\n",
    "        if not g_ids: continue \n",
    "\n",
    "        def get_val(cid, p): return comps[cid].get(p, (0,0,0))[0]\n",
    "\n",
    "        # identify core (max amplitude)\n",
    "        core_id = max(g_ids, key=lambda c: get_val(c, 'ampl'))\n",
    "        \n",
    "        # identify jets (remaining sorted by xpos)\n",
    "        remaining = [c for c in g_ids if c != core_id]\n",
    "        remaining_sorted = sorted(remaining, key=lambda c: get_val(c, 'xpos'))\n",
    "        \n",
    "        mapping = {core_id: 'core'}\n",
    "        if remaining_sorted:\n",
    "            mapping[remaining_sorted[0]] = 'east' \n",
    "        if len(remaining_sorted) > 1:\n",
    "            mapping[remaining_sorted[-1]] = 'west'\n",
    "            \n",
    "        if len(remaining_sorted) > 2:\n",
    "            extras = remaining_sorted[1:-1]\n",
    "            for i, extra_id in enumerate(extras, start=1):\n",
    "                mapping[extra_id] = f'extra_{i}'\n",
    "        \n",
    "        for c_id in comps:\n",
    "            if c_id.startswith('c'):\n",
    "                mapping[c_id] = 'bkg'\n",
    "\n",
    "        # 5. build rows for dataframe\n",
    "        for old_id, new_name in mapping.items():\n",
    "            if old_id not in comps: continue\n",
    "            \n",
    "            row = {\n",
    "                'obs_id': obs_id,\n",
    "                'mjd': mjd,\n",
    "                'exptime': exptime,\n",
    "                'component': new_name\n",
    "            }\n",
    "            \n",
    "            # flatten parameters\n",
    "            for param, (val, low_s, up_s) in comps[old_id].items():\n",
    "                row[param] = val\n",
    "                row[f'{param}_minus'] = low_s\n",
    "                row[f'{param}_plus'] = up_s\n",
    "            \n",
    "            # Add count rate data (nominal, plus_err, minus_err)\n",
    "            if old_id in rates:\n",
    "                r_val, r_min, r_plus = rates[old_id]\n",
    "                row['nominal'] = r_val\n",
    "                row['minus_err'] = r_min\n",
    "                row['plus_err'] = r_plus\n",
    "            else:\n",
    "                # Fallback if rate block missing\n",
    "                row['nominal'] = np.nan\n",
    "                row['minus_err'] = np.nan\n",
    "                row['plus_err'] = np.nan\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 6. cleanup types\n",
    "    err_cols = [c for c in df.columns if c.endswith('_minus') or c.endswith('_plus')]\n",
    "    for col in err_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').abs()\n",
    "    \n",
    "    if df['mjd'].isna().all() and 'obs_id' in df.columns:\n",
    "        df['mjd'] = df['obs_id'].astype('category').cat.codes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12614a-b7b4-4922-8491-94711cd74095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data directly from raw log\n",
    "# construct the raw filename using the identifier from cell 1\n",
    "raw_filename = f'multi-comp-fit-results-{file_identifier}.txt'\n",
    "raw_file_path = os.path.join(base_dir, 'multi comp fit results', raw_filename)\n",
    "\n",
    "print(f\"loading and processing: {raw_filename}\")\n",
    "df = load_sherpa_log_to_dataframe(raw_file_path)\n",
    "\n",
    "# check if data loaded\n",
    "if df.empty:\n",
    "    raise ValueError(\"dataframe is empty. check if the file exists and has correct format.\")\n",
    "\n",
    "# save 1: component tracker table (raw positions)\n",
    "# destination: 2dfits/comp tracker tables/\n",
    "tracker_dir = os.path.join(base_dir, 'comp tracker tables')\n",
    "os.makedirs(tracker_dir, exist_ok=True)\n",
    "\n",
    "tracker_filename = f'comp-tracker-table-{file_identifier}.csv'\n",
    "tracker_file_path = os.path.join(tracker_dir, tracker_filename)\n",
    "\n",
    "df.to_csv(tracker_file_path, index=False)\n",
    "print(f\"tracker table (raw) saved to: {tracker_file_path}\")\n",
    "\n",
    "# processing: recenter and calculate physics\n",
    "\n",
    "# vectorized recenter on core\n",
    "# isolate reference (core) positions\n",
    "ref_df = df[df['component'] == g1_component][['obs_id', 'xpos', 'ypos']]\n",
    "ref_df = ref_df.rename(columns={'xpos': 'ref_x', 'ypos': 'ref_y'})\n",
    "\n",
    "# merge reference positions back into the main dataframe\n",
    "df = df.merge(ref_df, on='obs_id', how='left')\n",
    "\n",
    "# fill missing reference positions with center pixel\n",
    "df['ref_x'] = df['ref_x'].fillna(center_pixel)\n",
    "df['ref_y'] = df['ref_y'].fillna(center_pixel)\n",
    "\n",
    "# calculate offsets vectorized\n",
    "df['dx'] = df['ref_x'] - center_pixel\n",
    "df['dy'] = df['ref_y'] - center_pixel\n",
    "\n",
    "# apply displacement\n",
    "df['xpos'] -= df['dx']\n",
    "df['ypos'] -= df['dy']\n",
    "\n",
    "# cleanup columns\n",
    "df.drop(columns=['dx', 'dy', 'ref_x', 'ref_y'], inplace=True)\n",
    "\n",
    "# calculate offsets, pa, radius, and propagate errors\n",
    "df['xoff'] = df['xpos'] - center_pixel\n",
    "df['yoff'] = df['ypos'] - center_pixel\n",
    "\n",
    "pa_rad = np.arctan2(-df['xoff'], df['yoff'])\n",
    "df['PA'] = np.degrees(pa_rad)\n",
    "df['pa_rad'] = pa_rad \n",
    "\n",
    "d2 = df['xoff']**2 + df['yoff']**2\n",
    "dpa_dx = np.divide(-df['yoff'], d2, out=np.full_like(d2, np.nan), where=d2 != 0)\n",
    "dpa_dy = np.divide(df['xoff'], d2, out=np.full_like(d2, np.nan), where=d2 != 0)\n",
    "\n",
    "df['PA_err_plus'] = np.degrees(np.sqrt((dpa_dx * df['xpos_plus'])**2 + (dpa_dy * df['ypos_plus'])**2))\n",
    "df['PA_err_minus'] = np.degrees(np.sqrt((dpa_dx * df['xpos_minus'])**2 + (dpa_dy * df['ypos_minus'])**2))\n",
    "\n",
    "df['radius'] = np.hypot(df['xoff'], df['yoff']) * pixscale_arcsec\n",
    "\n",
    "r_pix = df['radius'] / pixscale_arcsec\n",
    "is_zero = np.isclose(r_pix, 0)\n",
    "df['radius_plus_err'] = np.where(is_zero, np.hypot(df['xpos_plus'], df['ypos_plus']), np.sqrt((df['xoff']*df['xpos_plus'])**2 + (df['yoff']*df['ypos_plus'])**2)/r_pix) * pixscale_arcsec\n",
    "df['radius_minus_err'] = np.where(is_zero, np.hypot(df['xpos_minus'], df['ypos_minus']), np.sqrt((df['xoff']*df['xpos_minus'])**2 + (df['yoff']*df['ypos_minus'])**2)/r_pix) * pixscale_arcsec\n",
    "\n",
    "# sort by time\n",
    "df.sort_values('mjd', inplace=True)\n",
    "df['flag'] = 'clean'\n",
    "\n",
    "# save 2: data table (processed physics)\n",
    "# destination: 2dfits/data tables/\n",
    "data_dir = os.path.join(base_dir, 'data tables')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "data_filename = f'data-table-{file_identifier}.csv'\n",
    "data_file_path = os.path.join(data_dir, data_filename)\n",
    "\n",
    "df.to_csv(data_file_path, index=False)\n",
    "print(f\"data table (processed) saved to: {data_file_path}\")\n",
    "\n",
    "# create pivoted views for plotting\n",
    "pivoted = df.pivot_table(index='mjd', columns='component', values=['nominal', 'plus_err', 'minus_err'])\n",
    "df_nom, df_plus, df_minus = [pivoted[val].sort_index() for val in ['nominal', 'plus_err', 'minus_err']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecc88b-85f4-40ed-9d4e-2fa1e1e25b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_comp = df.groupby('component')\n",
    "\n",
    "# create a dictionary to map components to colors\n",
    "# exclude the reference component (core) from the color loop\n",
    "comps = [c for c in df_nom.columns if c != g1_component]\n",
    "colors = ['dodgerblue', 'mediumseagreen',  'mediumslateblue','lightcoral']\n",
    "comp_colors = {comp: colors[i % len(colors)] for i, comp in enumerate(comps)}\n",
    "\n",
    "n = len(comps)\n",
    "delta = 0.02\n",
    "offsets = {c: (i - (n - 1) / 2) * delta for i, c in enumerate(comps)}\n",
    "\n",
    "# dynamic pdf name\n",
    "# destination: 2dfits/comp tracker plots/\n",
    "plots_dir = os.path.join(base_dir, 'comp tracker plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "pdf_name = f'comp-tracker-plots-{file_identifier}.pdf'\n",
    "pdf_filename = os.path.join(plots_dir, pdf_name)\n",
    "\n",
    "with PdfPages(pdf_filename) as pdf:\n",
    "    # first figure: pa vs time + stacked count rates\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    gs  = GridSpec(n, 2, figure=fig, width_ratios=[1,1], hspace=0, wspace=0.3)\n",
    "\n",
    "    # left: pa vs time\n",
    "    ax_pa = fig.add_subplot(gs[:,0])\n",
    "    for comp, color in comp_colors.items():\n",
    "        if comp in grouped_by_comp.groups:\n",
    "            grp = grouped_by_comp.get_group(comp)\n",
    "            \n",
    "            ax_pa.errorbar(\n",
    "                grp['mjd'], grp['PA'],\n",
    "                yerr=[grp['PA_err_minus'], grp['PA_err_plus']],\n",
    "                marker='.', linestyle='-', capsize=3, color=color, label=comp\n",
    "            )\n",
    "            \n",
    "    ax_pa.set_ylabel('Position Angle (°)')\n",
    "    ax_pa.set_xlabel('MJD')\n",
    "    ax_pa.set_title('Position Angle vs Time')\n",
    "    ax_pa.set_ylim(-180,180)\n",
    "    ax_pa.grid(True)\n",
    "    ax_pa.legend()\n",
    "\n",
    "    # right: stacked count-rate panels\n",
    "    ax_bottom = None\n",
    "    for i_comp, focus in reversed(list(enumerate(comps))):\n",
    "        if i_comp == n - 1:\n",
    "            ax = fig.add_subplot(gs[i_comp, 1])\n",
    "            ax.set_xlabel('MJD')\n",
    "            ax_bottom = ax\n",
    "        else:\n",
    "            ax = fig.add_subplot(gs[i_comp, 1], sharex=ax_bottom)\n",
    "            ax.tick_params(labelbottom=False)\n",
    "        ax.grid(True, zorder=0)\n",
    "        \n",
    "        for comp_name, current_color in comp_colors.items():\n",
    "            if comp_name not in df_nom.columns: continue\n",
    "            \n",
    "            x_val = df_nom.index + offsets[comp_name]\n",
    "            y_val = df_nom[comp_name]\n",
    "            y_err_val = [df_minus[comp_name], df_plus[comp_name]]\n",
    "            \n",
    "            alpha_val, line_style, label_text, z_order = (1.0, '-', comp_name, 10) if comp_name == focus else (0.3, '', None, 1)\n",
    "            \n",
    "            ax.errorbar(\n",
    "                x_val, y_val, yerr=y_err_val, color=current_color,\n",
    "                marker='.', linestyle=line_style, capsize=3,\n",
    "                alpha=alpha_val, label=label_text, zorder=z_order\n",
    "            )\n",
    "\n",
    "        ax.set_yticks([0.1,0.3])\n",
    "        if ax.has_data():\n",
    "             ax.legend(loc='upper left')\n",
    "\n",
    "    fig.text(0.73, 0.885, 'Component Count Rates', ha='center', va='bottom', fontsize=17)\n",
    "    fig.text(0.495, 0.5, 'Count rate (counts/s)', va='center', rotation='vertical')\n",
    "    \n",
    "    pdf.savefig(fig)\n",
    "    plt.close(fig) \n",
    "\n",
    "    # second figure: polar plot of pa on sky\n",
    "    fig_polar = plt.figure(figsize=(8,6))\n",
    "    ax_polar = fig_polar.add_subplot(111, projection='polar')\n",
    "    ax_polar.set_theta_zero_location('N')\n",
    "    ax_polar.set_theta_direction(1)\n",
    "    ax_polar.set_thetamin(-180)\n",
    "    ax_polar.set_thetamax(180)\n",
    "    ax_polar.set_rlabel_position(135)\n",
    "\n",
    "    for comp, color in comp_colors.items():\n",
    "        if comp in grouped_by_comp.groups:\n",
    "            grp = grouped_by_comp.get_group(comp)\n",
    "            \n",
    "            ax_polar.errorbar(\n",
    "                grp['pa_rad'], grp['radius'],\n",
    "                xerr=[np.deg2rad(grp['PA_err_minus'].fillna(0)), np.deg2rad(grp['PA_err_plus'].fillna(0))],\n",
    "                yerr=[grp['radius_minus_err'].fillna(0), grp['radius_plus_err'].fillna(0)],\n",
    "                marker='.', linestyle='', color=color, capsize=3, label=comp\n",
    "            )\n",
    "\n",
    "    angles = np.arange(-150, 180, 30)\n",
    "    angles = np.append(angles, 180)\n",
    "    ax_polar.set_rmin(0)\n",
    "    ax_polar.set_thetagrids(angles, [f\"{int(a)}°\" for a in angles])\n",
    "    ax_polar.set_title('On Sky Component Positions (arcsec)')\n",
    "    ax_polar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "\n",
    "    pdf.savefig(fig_polar)\n",
    "    plt.close(fig_polar)\n",
    "    \n",
    "print(f\"plots saved to: {pdf_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962407a-7819-4683-b5cf-a5122c21c18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CIAO-4.17)",
   "language": "python",
   "name": "ciao-4.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
