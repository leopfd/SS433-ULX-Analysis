{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed5217-8efe-4d40-9871-5d5f9337e37f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "# Core scientific and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# MCMC, PDF compilation, and Parallelization\n",
    "import corner\n",
    "from PIL import Image\n",
    "from multiprocess import Pool, Manager\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set default Matplotlib styles\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 400,\n",
    "    'font.size': 18,\n",
    "    'image.origin': 'lower',\n",
    "})\n",
    "\n",
    "# Suppress Sherpa info messages\n",
    "logger = logging.getLogger(\"sherpa\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f1ed7-4825-4ea7-a983-721da25a296b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quickpos(x, y, x0, y0, iterations=1, size_list=None, binsize_list=None, doplot=False):\n",
    "    \"\"\"\n",
    "    Iteratively refines the centroid position using 1D histogram fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to create step-plot coordinates from histograms\n",
    "    def step_plot(x, y, binwidth):\n",
    "        xsteps = np.ravel(np.column_stack((x - binwidth/2, x + binwidth/2)))\n",
    "        ysteps = np.repeat(y, 2)\n",
    "        return xsteps, ysteps\n",
    "\n",
    "    # Set default window/bin sizes if none are provided\n",
    "    if size_list is None:\n",
    "        size_list = [np.max(x) - np.min(x)] * iterations\n",
    "    if binsize_list is None:\n",
    "        binsize_list = [1.0] * iterations\n",
    "\n",
    "    # Initialize variables\n",
    "    fig_list = []\n",
    "    current_x0, current_y0 = x0, y0\n",
    "    cnt = None\n",
    "    best_x0, best_y0 = current_x0, current_y0 \n",
    "\n",
    "    # Loop for each refinement iteration\n",
    "    for i in range(iterations):\n",
    "        size = size_list[i]\n",
    "        binsize = binsize_list[i]\n",
    "\n",
    "        # Get all points within the current window\n",
    "        ob = np.where((np.abs(x - current_x0) < size) & (np.abs(y - current_y0) < size))\n",
    "        \n",
    "        # Skip if no points are found\n",
    "        if len(ob[0]) == 0:\n",
    "            # print(f\"Warning: No points found in window for iteration {i+1}. Using previous values.\")\n",
    "            continue \n",
    "\n",
    "        # Create 1D histograms for X and Y\n",
    "        xbins = np.arange(current_x0 - size, current_x0 + size + binsize, binsize)\n",
    "        ybins = np.arange(current_y0 - size, current_y0 + size + binsize, binsize)\n",
    "\n",
    "        xhist, xedges = np.histogram(x[ob], bins=xbins)\n",
    "        yhist, yedges = np.histogram(y[ob], bins=ybins)\n",
    "\n",
    "        # Get bin centers\n",
    "        xval = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "        yval = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "\n",
    "        # Define 1D Gaussian model for fitting\n",
    "        def gaussian(x, a, mu, sigma, offset):\n",
    "            return a * np.exp(-((x - mu)**2) / (2 * sigma**2)) + offset\n",
    "\n",
    "        # Fit Gaussian to x histogram\n",
    "        try:\n",
    "            xmax = np.max(xhist)\n",
    "            x0_new = xval[np.argmax(xhist)]\n",
    "            xestpar = [xmax, x0_new, 2 * binsize, 0]\n",
    "            xpar, _ = curve_fit(gaussian, xval, xhist, p0=xestpar)\n",
    "            best_x0 = xpar[1] \n",
    "            xcnt = xpar[0] * xpar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: X-fit failed for iteration {i+1}: {e}. Using previous X value.\")\n",
    "            xcnt = 0\n",
    "\n",
    "        # Fit Gaussian to y histogram\n",
    "        try:\n",
    "            ymax = np.max(yhist)\n",
    "            y0_new = yval[np.argmax(yhist)]\n",
    "            yestpar = [ymax, y0_new, 2 * binsize, 0]\n",
    "            ypar, _ = curve_fit(gaussian, yval, yhist, p0=yestpar)\n",
    "            best_y0 = ypar[1] \n",
    "            ycnt = ypar[0] * ypar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: Y-fit failed for iteration {i+1}: {e}. Using previous Y value.\")\n",
    "            ycnt = 0\n",
    "\n",
    "        # Estimate counts and update centroid for next iteration\n",
    "        cnt = 0.5 * (xcnt + ycnt)\n",
    "        current_x0 = best_x0\n",
    "        current_y0 = best_y0\n",
    "\n",
    "    return best_x0, best_y0, cnt, fig_list\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def data_extract_quickpos_iter(infile, iters=3, sizes=[10, 5, 1.5], binsizes=[0.1, 0.1, 0.05]):\n",
    "    \"\"\"\n",
    "    Extracts data from a FITS file and runs quickpos to get an initial centroid.\n",
    "    \"\"\"\n",
    "    # Open the FITS file\n",
    "    with fits.open(infile) as obs:\n",
    "        hdr = obs[1].header\n",
    "        data = obs[1].data\n",
    "        \n",
    "        # Extract essential header info\n",
    "        scale = hdr['tcdlt20']\n",
    "        xc = hdr['tcrpx20']\n",
    "        exptime = hdr['exposure']\n",
    "            \n",
    "        # Form modified julian date for this obs\n",
    "        mjd_start = hdr['mjd-obs']\n",
    "        half_expos = 0.5 * (hdr['tstop']-hdr['tstart'])\n",
    "        date = mjd_start + half_expos / 86400\n",
    "        \n",
    "        # Convert event positions to arcsec\n",
    "        x = (data['x'] - xc) * scale * 3600\n",
    "        y = (data['y'] - xc) * scale * 3600\n",
    "        \n",
    "        # Filter a 20 arcsec radius region\n",
    "        rr = np.sqrt(x**2 + y**2)\n",
    "        ok = np.where(rr < 20)\n",
    "        \n",
    "        # Get a rough starting estimate of the centroid    \n",
    "        x0_est = np.average(x[ok])\n",
    "        y0_est = np.average(y[ok])\n",
    "\n",
    "    # Set iteration parameters\n",
    "    iterations = iters\n",
    "    size_list = sizes\n",
    "    binsize_list = binsizes\n",
    "    \n",
    "    # Run the iterative centroid refinement\n",
    "    x0_best, y0_best, cnt, qp_figs = quickpos(x[ok], y[ok], x0_est, y0_est, iterations, size_list, binsize_list)\n",
    "    \n",
    "    # Convert best-fit arcsec position back to pixels\n",
    "    pixel_x0_best = x0_best / (scale * 3600) + xc\n",
    "    pixel_y0_best = y0_best / (scale * 3600) + xc\n",
    "\n",
    "    return date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def rotate_psf_array(psf_file, match_file, outfile):\n",
    "    \"\"\"\n",
    "    Rotates a PSF image array based on a match file's ROLL_NOM\n",
    "    and saves it with the *original* PSF's header.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the roll angle from the observation's event file\n",
    "    try:\n",
    "        with fits.open(match_file) as hdu_match:\n",
    "            # Check the primary header (HDU 0)\n",
    "            if 'ROLL_NOM' in hdu_match[0].header:\n",
    "                roll_nom = hdu_match[0].header['ROLL_NOM']\n",
    "            # If not, check the EVENTS header (HDU 1)\n",
    "            elif hdu_match[1].header and 'ROLL_NOM' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_NOM']\n",
    "            # Fallback to ROLL_PNT in HDU 1\n",
    "            elif hdu_match[1].header and 'ROLL_PNT' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_PNT']\n",
    "                # print(\"  Note: Using 'ROLL_PNT' as 'ROLL_NOM' was not found.\")\n",
    "            else:\n",
    "                print(f\"  ERROR: Could not find 'ROLL_NOM' or 'ROLL_PNT' in {match_file}.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Match file not found: {match_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read match file header: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the rotation angle (scipy rotates counter-clockwise)\n",
    "    angle_to_rotate = roll_nom - 45.0\n",
    "\n",
    "    # Open the empirical PSF file\n",
    "    try:\n",
    "        with fits.open(psf_file) as hdu_psf:\n",
    "            # Get the main image data and header\n",
    "            if hdu_psf[0].data is None:\n",
    "                # Handle cases where data is in HDU 1\n",
    "                psf_data = hdu_psf[1].data\n",
    "                psf_header = hdu_psf[1].header\n",
    "            else:\n",
    "                psf_data = hdu_psf[0].data\n",
    "                psf_header = hdu_psf[0].header\n",
    "                \n",
    "            if psf_data is None:\n",
    "                print(f\"ERROR: No image data found in HDU 0 or 1 of {psf_file}.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: PSF file not found: {psf_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read PSF file data/header: {e}\")\n",
    "        return\n",
    "\n",
    "    # Rotate the PSF data array\n",
    "    rotated_psf_data = rotate(\n",
    "        psf_data,\n",
    "        angle_to_rotate,\n",
    "        reshape=False,       # Keep the same array shape\n",
    "        cval=0.0,            # Fill new pixels with 0\n",
    "        order=3              # Cubic interpolation\n",
    "    )\n",
    "\n",
    "    # Save the new, rotated data\n",
    "    \n",
    "    # Add a HISTORY card to document the rotation\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    psf_header.add_history(f\"Rotated by {angle_to_rotate:.4f} deg (ROLL_NOM={roll_nom:.4f} - 45.0)\")\n",
    "    psf_header.add_history(f\"Rotation applied by script on {timestamp}\")\n",
    "\n",
    "    # Write the new FITS file\n",
    "    hdu_out = fits.PrimaryHDU(data=rotated_psf_data, header=psf_header)\n",
    "    try:\n",
    "        hdu_out.writeto(outfile, overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not write output file: {e}\")\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def process_mcmc_results(covar_results, chains, burn_in_frac=0.2, sigma=1):\n",
    "    \"\"\"\n",
    "    Processes raw MCMC chains to calculate median and error bounds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transpose the chains array to (niter, nparams)\n",
    "    chains = chains.T\n",
    "    \n",
    "    # Set percentile for the requested sigma level\n",
    "    if sigma == 1:\n",
    "        # 1-sigma (68.27%)\n",
    "        q_low = 15.865\n",
    "        q_high = 84.135\n",
    "    elif sigma == 2:\n",
    "        # 2-sigma (95.45%)\n",
    "        q_low = 2.275\n",
    "        q_high = 97.725\n",
    "    elif sigma == 3:\n",
    "        # 3-sigma (99.73%)\n",
    "        q_low = 0.135\n",
    "        q_high = 99.865\n",
    "    else:\n",
    "        raise ValueError(\"Only sigma=1, 2, or 3 is supported.\")\n",
    "\n",
    "    # Calculate number of burn-in steps\n",
    "    n_iter, n_params = chains.shape\n",
    "    burn_in = int(n_iter * burn_in_frac)\n",
    "    \n",
    "    # Handle case where burn_in is too high\n",
    "    if burn_in >= n_iter:\n",
    "        # print(f\"Warning: burn_in_frac ({burn_in_frac}) is too high, resulting in 0 valid chains.\")\n",
    "        # print(f\"         Resetting burn_in to 0 for this run.\")\n",
    "        burn_in = 0\n",
    "        \n",
    "    # Get the valid chains after discarding burn-in\n",
    "    valid_chains = chains[burn_in:, :]\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    mcmc_results = {\n",
    "        'parnames': [],\n",
    "        'parvals': [],  # Median (50th percentile)\n",
    "        'parmins': [],  # Lower error bound (e.g., 16th percentile)\n",
    "        'parmaxes': []  # Upper error bound (e.g., 84th percentile)\n",
    "    }\n",
    "\n",
    "    # Get thawed parameter names from the covariance results\n",
    "    thawed_parnames = covar_results.parnames\n",
    "\n",
    "    # Check for parameter/chain dimension mismatch\n",
    "    if len(thawed_parnames) != n_params:\n",
    "        # print(f\"Warning: Number of thawed params ({len(thawed_parnames)}) does not match chain dims ({n_params}).\")\n",
    "        thawed_parnames = covar_results.parnames[:n_params]\n",
    "\n",
    "    # Iterate over each parameter's chain\n",
    "    for i, parname in enumerate(thawed_parnames):\n",
    "        chain_col = valid_chains[:, i]\n",
    "        \n",
    "        # Calculate quantiles (median, 1-sigma lower, 1-sigma upper)\n",
    "        p_low, p_mid, p_high = np.percentile(chain_col, [q_low, 50, q_high])\n",
    "        \n",
    "        # Store results\n",
    "        mcmc_results['parnames'].append(parname)\n",
    "        mcmc_results['parvals'].append(p_mid)\n",
    "        mcmc_results['parmins'].append(p_low)\n",
    "        mcmc_results['parmaxes'].append(p_high)\n",
    "\n",
    "    # Return the summary dict, valid chains, and param names\n",
    "    return mcmc_results, valid_chains, thawed_parnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cc560-3f3b-4889-b921-cfbb5bf2d322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_observation(infile, progress_queue, obsid_coords, mcmc_scale_factors, emp_psf_file,\n",
    "                        n_components_multi, run_mcmc_multi, mcmc_iter_multi,\n",
    "                        mcmc_n_walkers, mcmc_ball_size, progress_chunks=50, recalc=False):\n",
    "    \"\"\"\n",
    "    Worker function to process a single observation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process Local Imports\n",
    "    from sherpa.astro.ui import (\n",
    "        load_data, image_close, image_data, load_psf, set_psf, image_psf,\n",
    "        gauss2d, const2d, set_source, freeze, link, show_model, set_stat,\n",
    "        set_method, set_method_opt, fit, get_fit_results, thaw, set_sampler,\n",
    "        set_sampler_opt, covar, get_covar_results, get_draws,\n",
    "        get_model_component_image, get_data_image, get_model_image, get_data,\n",
    "        clean, calc_stat\n",
    "    )\n",
    "    \n",
    "    from ciao_contrib.runtool import dmcopy, reproject_image, dmhedit\n",
    "    from coords.format import ra2deg, dec2deg\n",
    "    \n",
    "    # Import plotting and math libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib.lines import Line2D\n",
    "    import corner\n",
    "    import emcee\n",
    "    from emcee.backends import HDFBackend\n",
    "    import numpy as np\n",
    "    from astropy.io import fits\n",
    "    from scipy.stats import chi2\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy.ndimage import rotate\n",
    "    import datetime\n",
    "    import time\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    logging.getLogger(\"sherpa\").setLevel(logging.WARNING)\n",
    "\n",
    "    \n",
    "    # Process Local Helper Functions\n",
    "    \n",
    "    def write_pixelscale(file: str, nx: int, ny: int, ra: str, dec: str, hrc_pscale_arcsec: float = 0.13175):\n",
    "        \"\"\"Adds a WCS header to a FITS file using dmhedit.\"\"\"\n",
    "        x_pix_ctr = (nx / 2.0) + 0.5\n",
    "        y_pix_ctr = (ny / 2.0) + 0.5\n",
    "        hrc_pscale_deg = hrc_pscale_arcsec / 3600.\n",
    "        x_platescale = -abs(hrc_pscale_deg / 4.)\n",
    "        y_platescale = abs(hrc_pscale_deg / 4.)\n",
    "        ra_deg = ra2deg(ra)\n",
    "        dec_deg = dec2deg(dec)\n",
    "        wcs_params = [\n",
    "            (\"WCSAXES\", 2, \"short\", None), (\"CRPIX1\", x_pix_ctr, \"float\", None),\n",
    "            (\"CRPIX2\", y_pix_ctr, \"float\", None), (\"CDELT1\", x_platescale, \"float\", \"deg\"),\n",
    "            (\"CDELT2\", y_platescale, \"float\", \"deg\"), (\"CUNIT1\", \"deg\", \"string\", None),\n",
    "            (\"CUNIT2\", \"deg\", \"string\", None), (\"CTYPE1\", \"RA---TAN\", \"string\", None),\n",
    "            (\"CTYPE2\", \"DEC--TAN\", \"string\", None), (\"CRVAL1\", ra_deg, \"float\", \"deg\"),\n",
    "            (\"CRVAL2\", dec_deg, \"float\", \"deg\"), (\"LONPOLE\", 180.0, \"float\", \"deg\"),\n",
    "            (\"LATPOLE\", 0, \"float\", \"deg\"), (\"RADESYS\", \"ICRS\", \"string\", None),\n",
    "        ]\n",
    "        try:\n",
    "            for key, value, dtype, unit in wcs_params:\n",
    "                dmhedit(infile=file, op=\"add\", key=key, value=value, datatype=dtype, unit=unit)\n",
    "        except Exception as e:\n",
    "            obsid = os.path.basename(os.path.dirname(file))\n",
    "            print(f\"  ERROR (ObsID {obsid}): dmhedit failed: {e}\")\n",
    "\n",
    "    def compute_split_rhat(chain):\n",
    "        \"\"\"\n",
    "        Calculates the Split-Rhat statistic for convergence.\n",
    "        Chain shape: (n_steps, n_walkers, n_params)\n",
    "        \"\"\"\n",
    "        n_steps, n_walkers, n_params = chain.shape\n",
    "        \n",
    "        # Split chains in half to check stationarity within chains\n",
    "        half = n_steps // 2\n",
    "        split_chain = np.concatenate((chain[:half], chain[half:]), axis=1)\n",
    "        \n",
    "        N = half\n",
    "        M = n_walkers * 2\n",
    "        \n",
    "        # Calculate Within-Chain Variance (W)\n",
    "        # Variance of each chain, then mean across chains\n",
    "        var_within = np.var(split_chain, axis=0, ddof=1)\n",
    "        W = np.mean(var_within, axis=0)\n",
    "        \n",
    "        # Calculate Between-Chain Variance (B)\n",
    "        # Mean of each chain\n",
    "        mean_chains = np.mean(split_chain, axis=0)\n",
    "        # Variance of those means, multiplied by N\n",
    "        B = N * np.var(mean_chains, axis=0, ddof=1)\n",
    "        \n",
    "        # Calculate Var+ (Marginal Posterior Variance)\n",
    "        var_plus = ((N - 1) / N) * W + (1 / N) * B\n",
    "        \n",
    "        # R-hat\n",
    "        rhat = np.sqrt(var_plus / W)\n",
    "        return rhat\n",
    "\n",
    "    def src_psf_images(obsid, infile, x0, y0, diameter, wcs_ra, wcs_dec, binsize=0.25, shape='square', psfimg=True, showimg=False, empirical_psf=None):\n",
    "        \"\"\"Creates and loads source and (optionally) PSF images into Sherpa.\"\"\"\n",
    "        if shape.lower() == 'circle':\n",
    "            region_str = f\"circle({x0},{y0},{diameter/2})\"\n",
    "        elif shape.lower() == 'square':\n",
    "            region_str = f\"box({x0},{y0},{diameter},{diameter},0)\"\n",
    "            img_region_str = f\"box(256.5,256.5,{diameter*4},{diameter*4},0)\"\n",
    "        else:\n",
    "            region_str = shape.lower()\n",
    "            \n",
    "        logical_width = diameter/binsize\n",
    "        imagefile=f'{obsid}/src_image_{shape}_{int(logical_width)}pixel.fits'\n",
    "        psf_rotated = f'{obsid}/psf_rotated.fits'\n",
    "        psf_rotated_cut = f'{obsid}/psf_rotated_cut.fits'\n",
    "        emp_psf_imagefile = f'{obsid}/psf_image_{shape}_empirical_{int(logical_width)}pixel.fits'\n",
    "        \n",
    "        dmcopy.punlearn()\n",
    "        dmcopy.clobber = 'yes'\n",
    "        reproject_image.punlearn()\n",
    "        reproject_image.clobber = 'yes'\n",
    "\n",
    "        dmcopy.infile = f'{infile}[sky={region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "        dmcopy.outfile = imagefile\n",
    "        dmcopy()\n",
    "        load_data(imagefile)\n",
    "\n",
    "        if empirical_psf is not None:\n",
    "            try:\n",
    "                with fits.open(infile) as hdu_match:\n",
    "                    if 'ROLL_NOM' in hdu_match[0].header:\n",
    "                        roll_nom = hdu_match[0].header['ROLL_NOM']\n",
    "                    elif hdu_match[1].header and 'ROLL_NOM' in hdu_match[1].header:\n",
    "                        roll_nom = hdu_match[1].header['ROLL_NOM']\n",
    "                    elif hdu_match[1].header and 'ROLL_PNT' in hdu_match[1].header:\n",
    "                        roll_nom = hdu_match[1].header['ROLL_PNT']\n",
    "                    else:\n",
    "                        print(f\"  ERROR: Could not find 'ROLL_NOM' or 'ROLL_PNT' in {infile}\")\n",
    "                        return\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  ERROR: Match file not found: {infile}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not read match file header: {e}\")\n",
    "                return\n",
    "            angle_to_rotate = roll_nom - 45.0\n",
    "            try:\n",
    "                with fits.open(empirical_psf) as hdu_psf:\n",
    "                    if hdu_psf[0].data is None:\n",
    "                        psf_data = hdu_psf[1].data\n",
    "                        psf_header = hdu_psf[1].header\n",
    "                    else:\n",
    "                        psf_data = hdu_psf[0].data\n",
    "                        psf_header = hdu_psf[0].header\n",
    "                    if psf_data is None:\n",
    "                        print(f\"ERROR: No image data found in {empirical_psf}.\")\n",
    "                        return\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  ERROR: PSF file not found: {empirical_psf}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not read PSF file data/header: {e}\")\n",
    "                return\n",
    "            rotated_psf_data = rotate(\n",
    "                psf_data, angle_to_rotate, reshape=False, cval=0.0, order=3\n",
    "            )\n",
    "            timestamp = datetime.datetime.now().isoformat()\n",
    "            psf_header.add_history(f\"Rotated by {angle_to_rotate:.4f} deg (ROLL_NOM={roll_nom:.4f} - 45.0)\")\n",
    "            psf_header.add_history(f\"Rotation applied by script on {timestamp}\")\n",
    "            hdu_out = fits.PrimaryHDU(data=rotated_psf_data, header=psf_header)\n",
    "            try:\n",
    "                hdu_out.writeto(psf_rotated, overwrite=True)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not write output file: {psf_rotated}\")\n",
    "            \n",
    "            try:\n",
    "                with fits.open(psf_rotated) as hdu_rot:\n",
    "                    nx = hdu_rot[0].header['NAXIS1']\n",
    "                    ny = hdu_rot[0].header['NAXIS2']\n",
    "                write_pixelscale(file=psf_rotated, nx=nx, ny=ny, ra=str(wcs_ra), dec=str(wcs_dec))\n",
    "            except Exception as e:\n",
    "                print(f\"!!! ERROR (ObsID {obsid}): WCS stamping failed: {e}\")\n",
    "\n",
    "            dmcopy.infile = f'{psf_rotated}[{img_region_str}][bin x=::{binsize*4},y=::{binsize*4}]'\n",
    "            dmcopy.outfile = psf_rotated_cut\n",
    "            dmcopy()\n",
    "            reproject_image.infile = psf_rotated_cut\n",
    "            reproject_image.matchfile = imagefile\n",
    "            reproject_image.outfile = emp_psf_imagefile\n",
    "            reproject_image.method = 'sum'\n",
    "            reproject_image()\n",
    "            load_psf(f'centr_psf{obsid}', emp_psf_imagefile)\n",
    "            set_psf(f'centr_psf{obsid}')\n",
    "        elif psfimg:\n",
    "            psf_infile = f'{obsid}/raytrace_projrays.fits'\n",
    "            psf_imagefile = f'{obsid}/psf_image_{shape}_raytrace_{int(logical_width)}pixel.fits'\n",
    "            dmcopy.infile = f'{psf_infile}[sky={region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "            dmcopy.outfile = psf_imagefile\n",
    "            dmcopy()\n",
    "            load_psf(f'centr_psf{obsid}', psf_imagefile)\n",
    "            set_psf(f'centr_psf{obsid}')\n",
    "\n",
    "        return binsize\n",
    "\n",
    "    def gaussian_image_fit(observation, n_components, position, ampl, fwhm,\n",
    "                           background=0, pos_min=(0, 0), pos_max=None, exptime=None, lock_fwhm=True,\n",
    "                           freeze_components=None, use_mcmc=True, mcmc_iter=5000, mcmc_burn_in_frac=0.2,\n",
    "                           n_walkers=32, ball_size=1e-4, \n",
    "                           prefix=\"g\", confirm=True, imgfit=False, progress_chunks=50):\n",
    "        \"\"\"\n",
    "        Fits multi-component 2D Gaussian models using Sherpa for optimization\n",
    "        and emcee for error estimation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Helper to expand single value inputs\n",
    "        def process_numeric_param(param, name):\n",
    "            if isinstance(param, (int, float)): return [param] * n_components\n",
    "            elif isinstance(param, list):\n",
    "                if len(param) != n_components: raise ValueError(f\"List of {name} must have length {n_components}.\")\n",
    "                return param\n",
    "            else: raise ValueError(f\"{name} must be a number or a list.\")\n",
    "\n",
    "        # Helper to expand single tuple inputs\n",
    "        def process_tuple_param(param, name):\n",
    "            if isinstance(param, (tuple, list)) and len(param) == 2 and all(isinstance(x, (int, float)) for x in param):\n",
    "                return [param] * n_components\n",
    "            elif isinstance(param, list):\n",
    "                if len(param) != n_components: raise ValueError(f\"List of {name} must have length {n_components}.\")\n",
    "                return param\n",
    "            else: raise ValueError(f\"{name} must be a tuple (x, y) or a list.\")\n",
    "\n",
    "        # Process parameters and Build Model\n",
    "        positions = process_tuple_param(position, \"position\")\n",
    "        ampls = process_numeric_param(ampl, \"ampl\")\n",
    "        fwhms = process_numeric_param(fwhm, \"fwhm\")\n",
    "        pos_mins = process_tuple_param(pos_min, \"pos_min\")\n",
    "        pos_maxs = [None] * n_components if pos_max is None else process_tuple_param(pos_max, \"pos_max\")\n",
    "\n",
    "        comp_names = []\n",
    "        gaussian_components = []\n",
    "        model_components = []\n",
    "        for i in range(1, n_components + 1):\n",
    "            comp_name = f\"{prefix}{i}\"\n",
    "            comp_names.append(comp_name)\n",
    "            comp = gauss2d(comp_name)\n",
    "            gaussian_components.append(comp)\n",
    "            model_components.append(comp)\n",
    "        \n",
    "        bkg_comp = None\n",
    "        if background > 0:\n",
    "            bkg_comp = const2d(\"c1\")\n",
    "            model_components.append(bkg_comp)\n",
    "        \n",
    "        if model_components: set_source(sum(model_components))\n",
    "        else: raise ValueError(\"Model expression is empty.\")\n",
    "\n",
    "        freeze_list = (freeze_components if isinstance(freeze_components, list) else ([freeze_components] if freeze_components is not None else []))\n",
    "        for i, comp in enumerate(gaussian_components):\n",
    "            comp_number = i + 1\n",
    "            comp.xpos = positions[i][0]; comp.ypos = positions[i][1]\n",
    "            comp.ampl = ampls[i]; comp.fwhm = fwhms[i]\n",
    "            if hasattr(comp.xpos, 'min'): comp.xpos.min = pos_mins[i][0]\n",
    "            if hasattr(comp.ypos, 'min'): comp.ypos.min = pos_mins[i][1]\n",
    "            if pos_maxs[i] is not None:\n",
    "                if hasattr(comp.xpos, 'max'): comp.xpos.max = pos_maxs[i][0]\n",
    "                if hasattr(comp.ypos, 'max'): comp.ypos.max = pos_maxs[i][1]\n",
    "            if hasattr(comp.ampl, 'min'): comp.ampl.min = 0\n",
    "            if comp_number in freeze_list: freeze(comp)\n",
    "\n",
    "        central_component = 1\n",
    "        if lock_fwhm:\n",
    "            master = gaussian_components[central_component-1].fwhm\n",
    "            for idx, comp in enumerate(gaussian_components):\n",
    "                if idx != (central_component-1): link(comp.fwhm, master)\n",
    "\n",
    "        if bkg_comp is not None:\n",
    "            bkg_comp.c0 = background\n",
    "            if hasattr(bkg_comp.c0, 'min'): bkg_comp.c0.min = 0\n",
    "\n",
    "        if confirm:\n",
    "            show_model()\n",
    "            if input(f\"  (ObsID {observation}) Proceed with fit? (y/n): \").lower() != \"y\": return None, None, None, None, None\n",
    "\n",
    "        # Optimization\n",
    "        set_stat('cstat')\n",
    "        set_method('moncar'); set_method_opt('numcores', 1)\n",
    "        set_method_opt('population_size', 10 * 16 * (n_components * 3 + 1)); set_method_opt('xprob', 0.5); set_method_opt('weighting_factor', 0.5)\n",
    "        fit()\n",
    "        set_method('simplex'); fit()\n",
    "        fit_results = get_fit_results()\n",
    "\n",
    "        # Identify Free Parameters\n",
    "        thawed_pars = []\n",
    "        thawed_par_names = []\n",
    "        for i, comp in enumerate(gaussian_components):\n",
    "            comp_number = i + 1\n",
    "            if comp_number not in freeze_list:\n",
    "                thaw(comp.ampl); thawed_pars.append(comp.ampl); thawed_par_names.append(comp.ampl.fullname)\n",
    "                if not (lock_fwhm and comp_number != central_component):\n",
    "                     thaw(comp.fwhm); thawed_pars.append(comp.fwhm); thawed_par_names.append(comp.fwhm.fullname)\n",
    "                thaw(comp.xpos, comp.ypos)\n",
    "                thawed_pars.append(comp.xpos); thawed_par_names.append(comp.xpos.fullname)\n",
    "                thawed_pars.append(comp.ypos); thawed_par_names.append(comp.ypos.fullname)\n",
    "        if bkg_comp is not None and not bkg_comp.c0.frozen:\n",
    "            thawed_pars.append(bkg_comp.c0); thawed_par_names.append(bkg_comp.c0.fullname)\n",
    "\n",
    "        # Capture Best Fit Values (Simplex)\n",
    "        best_fit_values = [p.val for p in thawed_pars]\n",
    "        best_fit_stat = fit_results.statval\n",
    "\n",
    "        mcmc_results = None\n",
    "        walker_map_fig = None\n",
    "        corner_fig = None\n",
    "        mcmc_duration_str = \"\"\n",
    "        flux_results = None\n",
    "        \n",
    "        if use_mcmc:\n",
    "            mcmc_start_time = time.time()\n",
    "            ndim = len(thawed_pars)\n",
    "            \n",
    "            def log_probability(theta):\n",
    "                for param, value in zip(thawed_pars, theta):\n",
    "                    if value < param.min or value > param.max: return -np.inf\n",
    "                for param, value in zip(thawed_pars, theta): param.val = value\n",
    "                return -0.5 * calc_stat()\n",
    "\n",
    "            current_n_walkers = n_walkers if n_walkers >= 2 * ndim else 2 * ndim + 2\n",
    "            \n",
    "            # Chain Storage and Resumption Logic\n",
    "            # Format: mcmc-chain-4comp-nwalkers-nsteps-0p0001ball\n",
    "            ball_str = str(ball_size).replace('.', 'p')\n",
    "            \n",
    "            param_folder_name = (f\"mcmc-chain-{n_components}comp-\"\n",
    "                                 f\"{current_n_walkers}walkers-\"\n",
    "                                 f\"{mcmc_iter}steps-\"\n",
    "                                 f\"{ball_str}ball\")\n",
    "            \n",
    "            chain_dir = os.path.join(os.getcwd(), \"2Dfits\", \"emcee_chains\", param_folder_name)\n",
    "            os.makedirs(chain_dir, exist_ok=True)\n",
    "            chain_filename = os.path.join(chain_dir, f\"{obsid}_chain.h5\")\n",
    "            \n",
    "            # Use compression to reduce file size\n",
    "            backend = HDFBackend(chain_filename, compression=\"gzip\", compression_opts=4)\n",
    "            \n",
    "            # Force Recalculation Logic\n",
    "            # If recalc=True (passed from main), we wipe the backend to start fresh\n",
    "            if recalc:\n",
    "                backend.reset(current_n_walkers, ndim)\n",
    "\n",
    "            # Determine if we are resuming or starting fresh\n",
    "            run_sampler = True\n",
    "            p0 = None\n",
    "            \n",
    "            # If file exists and has data, check status\n",
    "            if os.path.exists(chain_filename) and backend.iteration > 0:\n",
    "                if backend.iteration >= mcmc_iter:\n",
    "                    print(f\"  (ObsID {observation}) Found completed chain ({backend.iteration} steps). Loading results...\")\n",
    "                    run_sampler = False\n",
    "                else:\n",
    "                    print(f\"  (ObsID {observation}) Resuming chain from step {backend.iteration}...\")\n",
    "                    p0 = None # Setting p0 to None tells emcee to resume from backend\n",
    "            else:\n",
    "                # Start fresh: Generate initial walker positions\n",
    "                backend.reset(current_n_walkers, ndim)\n",
    "                best_fit_pos = np.array(best_fit_values)\n",
    "                p0 = best_fit_pos + ball_size * np.random.randn(current_n_walkers, ndim)\n",
    "                for i, param in enumerate(thawed_pars):\n",
    "                    p0[:, i] = np.clip(p0[:, i], param.min + 1e-6, param.max - 1e-6)\n",
    "\n",
    "            # Outer try block to catch MCMC and Plotting errors\n",
    "            try:\n",
    "                # Initialize Sampler\n",
    "                sampler = emcee.EnsembleSampler(current_n_walkers, ndim, log_probability, backend=backend)\n",
    "                \n",
    "                if run_sampler:\n",
    "                    # Inner try to catch sampler crashes specifically\n",
    "                    try:\n",
    "                        # Calculate how many steps are left\n",
    "                        current_step = backend.iteration\n",
    "                        remaining_steps = mcmc_iter - current_step\n",
    "                        \n",
    "                        update_interval = max(1, int(mcmc_iter / progress_chunks))\n",
    "                        \n",
    "                        # Run loop (p0 is None if resuming)\n",
    "                        for i, sample in enumerate(sampler.sample(p0, iterations=remaining_steps, progress=False)):\n",
    "                            if (i + 1) % update_interval == 0: progress_queue.put(1)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ERROR (ObsID {observation}) Sampler crashed: {e}\")\n",
    "                        # Note: We proceed to try loading whatever data we have\n",
    "                \n",
    "                # Load chains for analysis (discarding burn-in)\n",
    "                discard = int(mcmc_iter * mcmc_burn_in_frac)\n",
    "                \n",
    "                # Check if we have enough samples to discard\n",
    "                if sampler.iteration < discard:\n",
    "                     discard = 0\n",
    "                     \n",
    "                flat_samples = sampler.get_chain(discard=discard, flat=True)\n",
    "                raw_chain = sampler.get_chain(discard=discard) \n",
    "                \n",
    "                # Robust Convergence Statistics\n",
    "                # 1. Autocorrelation Time (tau)\n",
    "                try:\n",
    "                    # tol=0 prevents error if chain is short\n",
    "                    tau = sampler.get_autocorr_time(tol=0) \n",
    "                    tau_max = np.max(tau)\n",
    "                    # Effective Sample Size (ESS)\n",
    "                    ess = (raw_chain.shape[0] * raw_chain.shape[1]) / tau_max\n",
    "                except Exception:\n",
    "                    tau = [np.nan] * ndim\n",
    "                    tau_max = np.nan\n",
    "                    ess = 0\n",
    "\n",
    "                # 2. Gelman-Rubin (Split-Rhat)\n",
    "                try:\n",
    "                    rhat_vals = compute_split_rhat(raw_chain)\n",
    "                    rhat_max = np.max(rhat_vals)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: R-hat calc failed: {e}\")\n",
    "                    rhat_vals = [np.nan] * ndim\n",
    "                    rhat_max = np.nan\n",
    "\n",
    "                # Convergence String for Text Output\n",
    "                conv_str = (\n",
    "                    f\"Convergence Stats:\\n\"\n",
    "                    f\"  Max Autocorr Time (tau): {tau_max:.1f} steps\\n\"\n",
    "                    f\"  Max Split-Rhat:          {rhat_max:.4f} (Goal < 1.1)\\n\"\n",
    "                    f\"  Effective Samples (ESS): {int(ess)}\\n\"\n",
    "                    f\"  Chain Length / tau:      {raw_chain.shape[0] / tau_max:.1f} (Goal > 50)\\n\\n\"\n",
    "                )\n",
    "\n",
    "                # Compute MCMC Stats\n",
    "                q_low, q_mid, q_high = 15.865, 50.0, 84.135\n",
    "                mcmc_results_data = {'parnames': [], 'parvals': [], 'parmins': [], 'parmaxes': []}\n",
    "                for i, name in enumerate(thawed_par_names):\n",
    "                    mcmc_vals = flat_samples[:, i]\n",
    "                    p_low, p_mid, p_high = np.percentile(mcmc_vals, [q_low, q_mid, q_high])\n",
    "                    mcmc_results_data['parnames'].append(name)\n",
    "                    mcmc_results_data['parvals'].append(p_mid)\n",
    "                    mcmc_results_data['parmins'].append(p_low)\n",
    "                    mcmc_results_data['parmaxes'].append(p_high)\n",
    "                mcmc_results = mcmc_results_data\n",
    "\n",
    "                # Get log_prob for all samples in flat chain\n",
    "                log_probs = sampler.get_log_prob(discard=discard, flat=True)\n",
    "                max_idx = np.argmax(log_probs)\n",
    "                \n",
    "                # Evaluate stat for MCMC best vs Simplex best\n",
    "                best_mcmc_stat = -2.0 * log_probs[max_idx]\n",
    "                \n",
    "                if best_mcmc_stat < best_fit_stat:\n",
    "                    # MCMC found a better minimum, updating King of the Hill\n",
    "                    best_fit_values = list(flat_samples[max_idx])\n",
    "                    best_fit_stat = best_mcmc_stat\n",
    "\n",
    "                # Flux and count-rate uncertainties\n",
    "                flux_results = {}\n",
    "                if exptime is not None:\n",
    "                    fwhm_master_name = gaussian_components[central_component - 1].fwhm.fullname\n",
    "                    if fwhm_master_name in thawed_par_names:\n",
    "                        f_idx_master = thawed_par_names.index(fwhm_master_name)\n",
    "                        F_chain = flat_samples[:, f_idx_master]\n",
    "                        for comp in gaussian_components:\n",
    "                            amp_name = comp.ampl.fullname\n",
    "                            if amp_name in thawed_par_names:\n",
    "                                a_idx = thawed_par_names.index(amp_name)\n",
    "                                A_chain = flat_samples[:, a_idx]\n",
    "                                flux_chain = A_chain * (F_chain**2)\n",
    "                                F_low, F_mid, F_high = np.percentile(flux_chain, [q_low, q_mid, q_high])\n",
    "                                flux_results[comp.name] = (F_low, F_mid, F_high)\n",
    "\n",
    "                # RESTORE BEST FIT for Plots\n",
    "                for param, val in zip(thawed_pars, best_fit_values): param.val = val\n",
    "\n",
    "                # Walker Spatial Map (Shaded Density Contour Version)\n",
    "                walker_map_fig, ax = plt.subplots(1, 1, figsize=(19, 19))\n",
    "                \n",
    "                data_img = get_data_image(); data_vals = data_img.y\n",
    "                \n",
    "                # Get dimensions for proper extent\n",
    "                ny, nx = data_vals.shape\n",
    "                # Astronomical convention: Center of 1st pixel is (1,1)\n",
    "                plot_extent = [0.5, nx + 0.5, 0.5, ny + 0.5]\n",
    "                \n",
    "                min_pos = np.min(data_vals[data_vals > 0]) if np.any(data_vals > 0) else 1e-9\n",
    "                display_floor = min_pos / 10.0\n",
    "                data_masked = np.maximum(data_vals, display_floor)\n",
    "                log_norm = mcolors.LogNorm(vmin=display_floor, vmax=np.max(data_vals))\n",
    "                \n",
    "                # Pass extent to imshow so it aligns with 1-based MCMC coordinates\n",
    "                im_data = ax.imshow(data_masked, origin='lower', cmap='gray_r', norm=log_norm, \n",
    "                                    interpolation='nearest', extent=plot_extent)\n",
    "                \n",
    "                colors =      ['cyan', 'lime',      'magenta', 'orange',        'yellow']\n",
    "                # Darker colors for Best Fit and Median markers, including xkcd colors\n",
    "                dark_colors = ['navy', 'darkgreen', 'indigo',  'xkcd:burgundy', 'xkcd:shit']\n",
    "                \n",
    "                for i, comp_name in enumerate(comp_names):\n",
    "                    x_name = f\"{comp_name}.xpos\"; y_name = f\"{comp_name}.ypos\"\n",
    "                    \n",
    "                    if x_name in thawed_par_names and y_name in thawed_par_names:\n",
    "                        x_idx = thawed_par_names.index(x_name)\n",
    "                        y_idx = thawed_par_names.index(y_name)\n",
    "                        \n",
    "                        # Extract ALL points for this component\n",
    "                        x_pts = raw_chain[:, :, x_idx].flatten()\n",
    "                        y_pts = raw_chain[:, :, y_idx].flatten()\n",
    "                        \n",
    "                        # Generate 2D Density (Histogram)\n",
    "                        # Range matches the plot extent exactly\n",
    "                        H, xedges, yedges = np.histogram2d(\n",
    "                            y_pts, x_pts, \n",
    "                            bins=[ny, nx], \n",
    "                            range=[[0.5, ny + 0.5], [0.5, nx + 0.5]]\n",
    "                        )\n",
    "                        \n",
    "                        # Plot Contours if data exists\n",
    "                        if np.sum(H) > 0:\n",
    "                            comp_color = colors[i % len(colors)]\n",
    "                            dark_c = dark_colors[i % len(dark_colors)]\n",
    "                            base_rgb = mcolors.to_rgb(comp_color)\n",
    "                            \n",
    "                            peak = H.max()\n",
    "                            levels = [peak * 0.1, peak * 0.3, peak * 0.5, peak * 0.7, peak * 0.9]\n",
    "                            \n",
    "                            fill_colors = [\n",
    "                                (*base_rgb, 0.1), \n",
    "                                (*base_rgb, 0.3), \n",
    "                                (*base_rgb, 0.5),\n",
    "                                (*base_rgb, 0.7),\n",
    "                                (*base_rgb, 0.9)\n",
    "                            ]\n",
    "                            \n",
    "                            # Filled Contours with Gradient Shading\n",
    "                            ax.contourf(H, levels=levels, colors=fill_colors, extend='max', extent=plot_extent)\n",
    "                            # Line Contours (Thinner)\n",
    "                            ax.contour(H, levels=levels, colors=[comp_color], linewidths=1.0, alpha=0.9, extent=plot_extent)\n",
    "\n",
    "                            # Plot Best Fit Position\n",
    "                            bf_x = best_fit_values[x_idx]; bf_y = best_fit_values[y_idx]\n",
    "                            ax.scatter(bf_x, bf_y, marker='o', color=dark_c, s=100, zorder=20, \n",
    "                                       edgecolors='white', label=f\"{comp_name} Best Fit\")\n",
    "\n",
    "                            # Plot Median Position\n",
    "                            if mcmc_results is not None:\n",
    "                                med_x = mcmc_results['parvals'][x_idx]\n",
    "                                med_y = mcmc_results['parvals'][y_idx]\n",
    "                                ax.scatter(med_x, med_y, marker='x', color=dark_c, s=200, linewidth=3, zorder=19,\n",
    "                                           label=f\"{comp_name} Median\")\n",
    "\n",
    "                ax.set_title(f\"Walker Density Map - ObsID {observation}\"); ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "                \n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                by_label = dict(zip(labels, handles))\n",
    "                \n",
    "                # Check if Median is present, if not add proxy\n",
    "                if not any(\"Median\" in l for l in labels) and mcmc_results is not None:\n",
    "                     p_med = Line2D([0], [0], color='black', marker='x', linestyle='None', \n",
    "                                    markersize=10, markeredgewidth=3, label='Median')\n",
    "                     by_label['Median'] = p_med\n",
    "\n",
    "                ax.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "                walker_map_fig.colorbar(im_data, ax=ax, label=\"Counts\", shrink=0.8); walker_map_fig.tight_layout()\n",
    "\n",
    "                # Corner Figure (Conditional Downsampling)\n",
    "                total_samples = flat_samples.shape[0]\n",
    "                # Threshold: 1,000,000\n",
    "                threshold = 1000000 \n",
    "                \n",
    "                if total_samples > threshold:\n",
    "                    stride = int(total_samples / threshold)\n",
    "                    plot_samples = flat_samples[::stride]\n",
    "                    title_suffix = f\"(Downsampled {stride}x)\"\n",
    "                else:\n",
    "                    plot_samples = flat_samples\n",
    "                    title_suffix = \"(Full Chain)\"\n",
    "\n",
    "                corner_fig = corner.corner(\n",
    "                    plot_samples,\n",
    "                    labels=thawed_par_names, quantiles=[0.16, 0.5, 0.84],\n",
    "                    show_titles=True, title_fmt=\".3f\",\n",
    "                    truths=best_fit_values, truth_color='red',\n",
    "                    quiet=True\n",
    "                )\n",
    "                corner_fig.suptitle(f\"Corner Plot {title_suffix} - ObsID {observation}\", y=1.02)\n",
    "\n",
    "                mcmc_end_time = time.time()\n",
    "                mcmc_duration_min = (mcmc_end_time - mcmc_start_time) / 60.0\n",
    "                mcmc_duration_str = (f\"emcee execution time = {mcmc_duration_min:.2f} minutes\\n\"\n",
    "                                     f\"Mean acceptance fraction = {np.mean(sampler.acceptance_fraction):.3f}\\n\"\n",
    "                                     f\"{conv_str}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                mcmc_results = None\n",
    "                mcmc_duration_str = f\"emcee FAILED: {e}\\n\\n\"\n",
    "\n",
    "        # Fit Summary Text\n",
    "        fit_summary = (\n",
    "            f\"Method = {fit_results.methodname}\\nStatistic = {fit_results.statname}\\n\"\n",
    "            f\"Final C-stat = {best_fit_stat:.2f} (Simplex+MCMC)\\n\" \n",
    "            f\"Reduced statistic = {fit_results.rstat:.5f}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        def fmt_val(val): return \"------\" if val is None else f\"{val:>10.3f}\"\n",
    "            \n",
    "        if mcmc_results is not None:\n",
    "            param_table = [\n",
    "                f\"emcee Results (Red Line = Best Fit, Black = Median):\",\n",
    "                f\"{'Param':<12} {'Best Fit':>10} {'Median':>10} {'-Error':>10} {'+Error':>10}\",\n",
    "                f\"{'-'*5:<12} {'-'*8:>10} {'-'*8:>10} {'-'*8:>10} {'-'*8:>10}\"\n",
    "            ]\n",
    "            for name, median, low, high, best in zip(mcmc_results['parnames'], \n",
    "                                                     mcmc_results['parvals'], \n",
    "                                                     mcmc_results['parmins'], \n",
    "                                                     mcmc_results['parmaxes'],\n",
    "                                                     best_fit_values):\n",
    "                # Calculate explicit +/- errors relative to median\n",
    "                err_minus = median - low\n",
    "                err_plus = high - median\n",
    "                display_name = name.split('.')[-2] + '.' + name.split('.')[-1] if '.' in name else name\n",
    "                param_table.append(f\"{display_name:<12} {fmt_val(best)} {fmt_val(median)} {fmt_val(err_minus)} {fmt_val(err_plus)}\")\n",
    "            param_table = \"\\n\".join(param_table)\n",
    "        else:\n",
    "            param_table = \"Best-Fit Values (No MCMC):\\n\" + \"\\n\".join([f\"{n:<12} {fmt_val(v)}\" for n, v in zip(fit_results.parnames, fit_results.parvals)])\n",
    "                \n",
    "        summary_output = fit_summary + mcmc_duration_str + param_table + '\\n'\n",
    "\n",
    "        # Component count-rate block uses full MCMC flux (A * FWHM^2)\n",
    "        if exptime and use_mcmc and mcmc_results is not None:\n",
    "            rate_block_rows = [\"Component count rates (counts/s):\"]\n",
    "            \n",
    "            for comp in gaussian_components:\n",
    "                short = comp.name.split('.')[-1]\n",
    "                if flux_results is not None and comp.name in flux_results:\n",
    "                    F_low, F_mid, F_high = flux_results[comp.name]\n",
    "                    rate_mid = F_mid / exptime\n",
    "                    rate_minus = (F_mid - F_low) / exptime\n",
    "                    rate_plus = (F_high - F_mid) / exptime\n",
    "                    rate_block_rows.append(\n",
    "                        f\"  {short:<6}: {rate_mid:7.4f}  (-{rate_minus:6.4f}/+{rate_plus:6.4f})\"\n",
    "                    )\n",
    "                else:\n",
    "                    # Fallback: use best-fit model component counts with no propagated error\n",
    "                    comp_img = get_model_component_image(comp.name)\n",
    "                    total_cts = comp_img.y.sum()\n",
    "                    rate = total_cts / exptime\n",
    "                    rate_block_rows.append(\n",
    "                        f\"  {short:<6}: {rate:7.4f}  (no MCMC rate errors)\"\n",
    "                    )\n",
    "            summary_output += \"\\n\" + \"\\n\".join(rate_block_rows) + \"\\n\"\n",
    "        else:\n",
    "            summary_output = fit_summary + param_table + '\\n\\n\\n\\n'\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(30, 15))\n",
    "        plot_idx = 0\n",
    "        data_img = get_data_image(); data_vals = data_img.y\n",
    "        min_pos = np.min(data_vals[data_vals > 0]) if np.any(data_vals > 0) else 1e-9\n",
    "        display_floor = min_pos / 10.0\n",
    "        data_masked = np.maximum(data_vals, display_floor)\n",
    "        model_img = get_model_image(); model_vals = model_img.y\n",
    "        model_masked = np.maximum(model_vals, display_floor)\n",
    "        D = 2.0 * (data_masked * np.log(data_masked / model_masked) - (data_masked - model_masked))\n",
    "        resid_dev = np.sign(data_vals - model_vals) * np.sqrt(np.abs(D))\n",
    "        log_norm = mcolors.LogNorm(vmin=display_floor, vmax=np.max(data_vals))\n",
    "        \n",
    "        ax = axs[0]\n",
    "        im = ax.imshow(data_masked, origin='lower', cmap='gnuplot2', norm=log_norm, interpolation='nearest')\n",
    "        \n",
    "        base_colors = ['white', 'cyan', 'lime', 'xkcd:periwinkle']\n",
    "        legend_elements = []\n",
    "        \n",
    "        for i, comp_name in enumerate(comp_names):\n",
    "            comp_vals = get_model_component_image(comp_name).y\n",
    "            if not np.any(comp_vals > 0): continue\n",
    "\n",
    "            color = base_colors[i % len(base_colors)]\n",
    "            \n",
    "            ax.contour(comp_vals, levels=[0.2 * np.max(comp_vals)], colors=[color], linestyles=['--'], linewidths=2)\n",
    "            \n",
    "            # Add to legend list\n",
    "            legend_elements.append(Line2D([0], [0], lw=2, linestyle='--', color=color, label=f\"{comp_name}\"))\n",
    "        \n",
    "        if legend_elements:\n",
    "            ax.legend(handles=legend_elements, loc='upper right')\n",
    "                \n",
    "        ax.set_title(f\"{observation} Data + Best Fit Overlay\"); fig.colorbar(im, ax=ax, label=\"Counts\", shrink=0.53)\n",
    "\n",
    "        ax = axs[1]\n",
    "        im = ax.imshow(model_masked, origin='lower', cmap='gnuplot2', norm=log_norm, interpolation='nearest')\n",
    "        ax.set_title(\"Best Fit Model\"); fig.colorbar(im, ax=ax, label=\"Model Counts\", shrink=0.53)\n",
    "\n",
    "        ax = axs[2]\n",
    "        im = ax.imshow(np.abs(resid_dev), origin='lower', cmap='gnuplot2', norm=mcolors.Normalize(vmin=0, vmax=5), interpolation='nearest')\n",
    "        ax.set_title(\"Poisson Deviance (Best Fit)\"); fig.colorbar(im, ax=ax, label=\"|Residuals|\", shrink=0.53)\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        return summary_output, fig, corner_fig, walker_map_fig\n",
    "\n",
    "    # Main Worker Logic\n",
    "    \n",
    "    # Initialize lists to store temp file paths\n",
    "    pdf_out_files = []\n",
    "    multi_pdf_out_files = []\n",
    "    \n",
    "    # Extract observation directory name\n",
    "    obsid = os.path.dirname(os.path.dirname(infile))\n",
    "\n",
    "    # set random seed based on obsid for reproducible chains\n",
    "    try:\n",
    "        np.random.seed(int(obsid))\n",
    "    except ValueError:\n",
    "        # fallback for non integer obsids\n",
    "        np.random.seed(hash(obsid) % (2**32 - 1))\n",
    "\n",
    "    # Get ObsID specific coordinates\n",
    "    if obsid not in obsid_coords:\n",
    "        print(f\"!!! WARNING: ObsID {obsid} not in coordinate lookup table. Skipping.\")\n",
    "        return (obsid, \"\", \"\", \"\", \"\", \"\", [], [])\n",
    "    current_ra, current_dec = obsid_coords[obsid]\n",
    "    \n",
    "    # Run initial data extraction and centroiding\n",
    "    date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs = data_extract_quickpos_iter(infile)\n",
    "    \n",
    "    # Store the header text\n",
    "    header_text = (\n",
    "        f\"Observation: {obsid}\\n\"\n",
    "        f\"Infile: {infile}\\n\"\n",
    "        f\"Date: {date}, Exptime: {exptime}\\n\"\n",
    "    )\n",
    "\n",
    "    # Stage 1: Centroid Fit\n",
    "    img_width = 40\n",
    "    cent_binsize = 1.0\n",
    "    src_psf_images(\n",
    "        obsid, infile, pixel_x0_best, pixel_y0_best, img_width,\n",
    "        wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "        binsize=cent_binsize, \n",
    "        psfimg=False, \n",
    "        empirical_psf=None\n",
    "    )\n",
    "    logical_width = img_width / cent_binsize\n",
    "    img_center = logical_width / 2.0 + 0.5\n",
    "    \n",
    "    centroid_fit_summary, centroid_fit_fig, _, _ = gaussian_image_fit(\n",
    "        obsid, 1, (img_center, img_center), cnt, (1.0 / cent_binsize),\n",
    "        prefix=\"centrg\", background=0.1, pos_max=(logical_width, logical_width),\n",
    "        use_mcmc=False, confirm=False\n",
    "    )\n",
    "    \n",
    "    if centroid_fit_summary is None:\n",
    "        print(f\"Centroid fit for {obsid} canceled. Skipping.\")\n",
    "        clean()\n",
    "        return (obsid, \"\", \"\", \"\", \"\", \"\", [], [])\n",
    "\n",
    "    # Save plot to a temporary PNG\n",
    "    temp_cent_fit_png = f\"2Dfits/temp_{obsid}_cent_fit.png\"\n",
    "    centroid_fit_fig.savefig(temp_cent_fit_png)\n",
    "    plt.close(centroid_fit_fig)\n",
    "    pdf_out_files.append(temp_cent_fit_png)\n",
    "    progress_queue.put(1)\n",
    "\n",
    "    # Get best fit physical coordinates\n",
    "    d = get_data()\n",
    "    crval_x, crval_y = d.sky.crval\n",
    "    crpix_x, crpix_y = d.sky.crpix\n",
    "    cdelt_x, cdelt_y = d.sky.cdelt\n",
    "    \n",
    "    # Access the dynamically created model object\n",
    "    xphys_best = crval_x + (globals()['centrg1'].xpos.val - crpix_x) * cdelt_x\n",
    "    yphys_best = crval_y + (globals()['centrg1'].ypos.val - crpix_y) * cdelt_y\n",
    "\n",
    "    # Stage 2: Single Component Source Fit\n",
    "    img_width = 10\n",
    "    src_binsize = 0.25\n",
    "    src_psf_images(\n",
    "        obsid, infile, xphys_best, yphys_best, img_width,\n",
    "        wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "        binsize=src_binsize, \n",
    "        psfimg=True, \n",
    "        empirical_psf=emp_psf_file\n",
    "    )\n",
    "    logical_width = img_width / src_binsize \n",
    "    img_center = logical_width / 2.0 + 0.5 \n",
    "    pixel_scale_guess = 1.0 / src_binsize \n",
    "    scaled_cnt_guess = cnt / (pixel_scale_guess**2)\n",
    "    scaled_fwhm_guess = 1.0 * pixel_scale_guess \n",
    "    \n",
    "    src_fit_summary, src_fit_fig, _, _ = gaussian_image_fit(\n",
    "        obsid, 1, (img_center, img_center), scaled_cnt_guess, scaled_fwhm_guess,\n",
    "        prefix=\"srcg\", pos_max=(logical_width, logical_width),\n",
    "        use_mcmc=False, confirm=False\n",
    "    )\n",
    "    \n",
    "    if src_fit_summary is None:\n",
    "        print(f\"Source fit for {obsid} canceled. Skipping.\")\n",
    "        clean()\n",
    "        return (obsid, header_text, centroid_fit_summary, \"\", \"\", \"\", pdf_out_files, [])\n",
    "\n",
    "    # Save plot to temporary PNG\n",
    "    temp_src_fit_png = f\"2Dfits/temp_{obsid}_src_fit.png\"\n",
    "    src_fit_fig.savefig(temp_src_fit_png)\n",
    "    plt.close(src_fit_fig)\n",
    "    pdf_out_files.append(temp_src_fit_png)\n",
    "    progress_queue.put(1) \n",
    "\n",
    "    # Stage 3: Multi Component Fit\n",
    "    # Access the dynamically created model object\n",
    "    srcfit_off_x = globals()['srcg1'].xpos.val - img_center \n",
    "    srcfit_off_y = globals()['srcg1'].ypos.val - img_center \n",
    "    src_ampl = globals()['srcg1'].ampl.val\n",
    "    src_fwhm = globals()['srcg1'].fwhm.val\n",
    "    \n",
    "    # This is your desired multi fit setup\n",
    "    img_width = 40 \n",
    "    multi_binsize = 0.25\n",
    "    \n",
    "    src_psf_images(\n",
    "        obsid, infile, xphys_best, yphys_best, img_width,\n",
    "        wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "        binsize=multi_binsize, \n",
    "        empirical_psf=emp_psf_file,\n",
    "    )\n",
    "    \n",
    "    logical_width = img_width / multi_binsize \n",
    "    img_center = logical_width / 2.0 + 0.5   \n",
    "    pixel_scale = src_binsize / multi_binsize \n",
    "    \n",
    "    new_xpos = img_center + (srcfit_off_x * pixel_scale)\n",
    "    new_ypos = img_center + (srcfit_off_y * pixel_scale)\n",
    "    scaled_src_fwhm = src_fwhm * pixel_scale\n",
    "    scaled_src_ampl = src_ampl / (pixel_scale**2)\n",
    "    \n",
    "    pixel_scale_guess = 1.0 / multi_binsize \n",
    "    scaled_cnt_ampl = cnt / (pixel_scale_guess**2)\n",
    "    scaled_default_fwhm = 1.0 * pixel_scale_guess \n",
    "\n",
    "    # This is your desired component setup\n",
    "    n_components = n_components_multi \n",
    "    positions = [(new_xpos, new_ypos)] + [(img_center, img_center)] * (n_components - 1)\n",
    "    amplitudes = [scaled_src_ampl] + [scaled_cnt_ampl] * (n_components - 1)\n",
    "    fwhms = [scaled_src_fwhm] + [scaled_default_fwhm] * (n_components - 1)\n",
    "\n",
    "    # This is your desired fit call\n",
    "    multi_fit_summary, multi_fit_fig, multi_corner_fig, multi_walker_fig = gaussian_image_fit(\n",
    "        obsid, n_components, positions, amplitudes, fwhms,\n",
    "        prefix=\"g\", background=0.1, pos_max=(logical_width, logical_width),\n",
    "        pos_min=(0, 0), exptime=exptime, confirm=False, \n",
    "        use_mcmc=run_mcmc_multi, \n",
    "        mcmc_iter=mcmc_iter_multi,\n",
    "        n_walkers=mcmc_n_walkers,  \n",
    "        ball_size=mcmc_ball_size,\n",
    "        progress_chunks=progress_chunks \n",
    "    )\n",
    "\n",
    "    if multi_fit_summary is None:\n",
    "        print(f\"Multi-component fit for {obsid} canceled. Skipping.\")\n",
    "        clean()\n",
    "        return (obsid, header_text, centroid_fit_summary, src_fit_summary, \"\", \"\", pdf_out_files, [])\n",
    "\n",
    "    # Save fit plot to temp PNG\n",
    "    temp_multi_fit_png = f\"2Dfits/temp_{obsid}_multi_fit.png\"\n",
    "    multi_fit_fig.savefig(temp_multi_fit_png)\n",
    "    plt.close(multi_fit_fig)\n",
    "    pdf_out_files.append(temp_multi_fit_png)\n",
    "    multi_pdf_out_files.append(temp_multi_fit_png)\n",
    "\n",
    "    # Save Walker Map (New Visualization)\n",
    "    if multi_walker_fig is not None:\n",
    "        temp_walker_png = f\"2Dfits/temp_{obsid}_walker_map.png\"\n",
    "        multi_walker_fig.savefig(temp_walker_png)\n",
    "        plt.close(multi_walker_fig)\n",
    "        pdf_out_files.append(temp_walker_png)\n",
    "\n",
    "    # Save Corner Plot\n",
    "    if multi_corner_fig is not None:\n",
    "        temp_corner_png = f\"2Dfits/temp_{obsid}_corner.png\"\n",
    "        multi_corner_fig.savefig(temp_corner_png)\n",
    "        plt.close(multi_corner_fig)\n",
    "        pdf_out_files.append(temp_corner_png)\n",
    "\n",
    "    # Create the text for the multi results file\n",
    "    multi_results_text = (\n",
    "        f\"Observation: {obsid}\\n\"\n",
    "        f\"Infile: {infile}\\n\"\n",
    "        f\"Date: {date}, Exptime: {exptime}\\n\"\n",
    "        f\"{multi_fit_summary}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Clean up the local Sherpa session\n",
    "    clean()\n",
    "    progress_queue.put(1) \n",
    "\n",
    "    # Return all results for aggregation\n",
    "    return (\n",
    "        obsid,\n",
    "        header_text,\n",
    "        centroid_fit_summary,\n",
    "        src_fit_summary,\n",
    "        multi_fit_summary,\n",
    "        multi_results_text,\n",
    "        pdf_out_files,\n",
    "        multi_pdf_out_files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6a9ef-571f-4bb3-94b7-7abcc06c28a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This guard is essential for multiprocessing in notebooks\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Fix DOS Warning for large plots\n",
    "    from PIL import Image\n",
    "    Image.MAX_IMAGE_PIXELS = None\n",
    "    \n",
    "    # Run Configuration\n",
    "    \n",
    "    # Set the number of components for the multi component fit\n",
    "    multi_n_components = 3\n",
    "    \n",
    "    # Set to True to run MCMC, or False for a fast test run\n",
    "    run_mcmc = True\n",
    "\n",
    "    # Set to True to force the pipeline to delete old chains and run fresh.\n",
    "    # Set to False to resume or load existing chains.\n",
    "    recalculate_chains = True\n",
    "    \n",
    "    # MCMC CONTROL PARAMETERS\n",
    "    mcmc_iterations = 20000\n",
    "    mcmc_n_walkers = 4 * (multi_n_components * 3 + 2)       \n",
    "    mcmc_ball_size = 1e-4\n",
    "    # New: Number of progress bar updates during MCMC (e.g. 50 = every 2%)\n",
    "    mcmc_updates_per_obs = 400\n",
    "\n",
    "    # Static Parameters\n",
    "    \n",
    "    # Set the main working directory\n",
    "    os.chdir('/Users/leodrake/Documents/MIT/ss433/HRC_2024/')\n",
    "\n",
    "    # Define WCS coordinates for each observation\n",
    "    obsid_coords = {\n",
    "        \"26568\": (\"287.9565362\", \"4.9826061\"),\n",
    "        \"26569\": (\"287.9563218\", \"4.9827745\"),\n",
    "        \"26570\": (\"287.9563754\", \"4.9825322\"),\n",
    "        \"26571\": (\"287.9561693\", \"4.9827006\"),\n",
    "        \"26572\": (\"287.9565032\", \"4.9826636\"),\n",
    "        \"26573\": (\"287.9565444\", \"4.9826390\"),\n",
    "        \"26574\": (\"287.9562518\", \"4.9825651\"),\n",
    "        \"26575\": (\"287.9566969\", \"4.9828114\"),\n",
    "        \"26576\": (\"287.9566351\", \"4.9826718\"),\n",
    "        \"26577\": (\"287.9565238\", \"4.9826020\"),\n",
    "        \"26578\": (\"287.9566021\", \"4.9826800\"),\n",
    "        \"26579\": (\"287.9565733\", \"4.9825774\")\n",
    "    }\n",
    "\n",
    "    # Define the empirical PSF file to be used\n",
    "    emp_psf_file = \"/Users/leodrake/Documents/MIT/ss433/HRC_2024/empPSF_iARLac_v2025_2017-2025.fits\" \n",
    "\n",
    "    # Find all event files to be processed\n",
    "    event_files = sorted(glob.glob('*/repro/*splinecorr.fits'))[:]\n",
    "    \n",
    "    # Define output file names\n",
    "    pdf_out_filename = '2Dfits/0fit-plots.pdf'\n",
    "    multi_pdf_out_filename = '2Dfits/0multi-comp-plots.pdf'\n",
    "    results_filename = '2Dfits/0fit-results.txt'\n",
    "    multi_results_filename = '2Dfits/0multi-comp-fit-results.txt'\n",
    "    \n",
    "    # Define total number of steps for the progress bar\n",
    "    # 3 standard checkpoints + MCMC updates per file\n",
    "    ticks_per_file = 3 + mcmc_updates_per_obs if run_mcmc else 3\n",
    "    total_steps = len(event_files) * ticks_per_file\n",
    "\n",
    "    # Run in Parallel\n",
    "    \n",
    "    # Create a Manager and a Queue for progress updates\n",
    "    manager = Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    \n",
    "    # Create the partial function, pre loading it with the args\n",
    "    worker_func = partial(process_observation, \n",
    "                          progress_queue=progress_queue,\n",
    "                          obsid_coords=obsid_coords, \n",
    "                          mcmc_scale_factors={}, \n",
    "                          emp_psf_file=emp_psf_file,\n",
    "                          n_components_multi=multi_n_components,\n",
    "                          run_mcmc_multi=run_mcmc,\n",
    "                          mcmc_iter_multi=mcmc_iterations,\n",
    "                          mcmc_n_walkers=mcmc_n_walkers,  \n",
    "                          mcmc_ball_size=mcmc_ball_size,\n",
    "                          progress_chunks=mcmc_updates_per_obs,\n",
    "                          recalc=recalculate_chains\n",
    "                         )\n",
    "\n",
    "    # Set number of cores to use\n",
    "    num_processes = os.cpu_count()\n",
    "    print(f\"--- Starting parallel processing on {num_processes} cores ---\")\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    # Start the pool and run the jobs\n",
    "    with tqdm(total=total_steps, desc=\"Processing Observations\", bar_format=\"{l_bar}{r_bar}\") as pbar:\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            # Use map_async to run jobs without blocking the main thread\n",
    "            async_result = pool.map_async(worker_func, event_files)\n",
    "            \n",
    "            # Monitor the queue and update the progress bar\n",
    "            while not async_result.ready():\n",
    "                while not progress_queue.empty():\n",
    "                    pbar.update(progress_queue.get())\n",
    "                time.sleep(0.1) \n",
    "            \n",
    "            # Update the bar with any remaining items in the queue\n",
    "            while not progress_queue.empty():\n",
    "                pbar.update(progress_queue.get())\n",
    "                \n",
    "            # Get the final results from all workers\n",
    "            results = async_result.get()\n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\n--- Parallel processing complete in {(end_total_time - start_total_time) / 60.0:.2f} minutes ---\")\n",
    "    print(\"--- Aggregating all results... ---\")\n",
    "\n",
    "    # Aggregate Results\n",
    "    \n",
    "    # Sort results by obsid first item in tuple to ensure correct order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Initialize master lists for file paths\n",
    "    all_pdf_out_files = []\n",
    "    all_multi_pdf_out_files = []\n",
    "\n",
    "    # Open text files and write all results in order\n",
    "    with open(results_filename, 'w') as results_file, open(multi_results_filename, 'w') as multi_results_file:\n",
    "        for res in results:\n",
    "            # Unpack the tuple from the worker\n",
    "            (obsid, header_text, centroid_fit_summary, src_fit_summary, \n",
    "             multi_fit_summary, multi_results_text, \n",
    "             pdf_out_files_worker, multi_pdf_out_files_worker) = res\n",
    "            \n",
    "            # Write to 0fit-results.txt\n",
    "            results_file.write(header_text)\n",
    "            results_file.write(\"\\nCENTROID FIT SUMMARY:\\n\\n\")\n",
    "            results_file.write(centroid_fit_summary)\n",
    "            results_file.write(\"SOURCE FIT SUMMARY:\\n\\n\")\n",
    "            results_file.write(src_fit_summary)\n",
    "            results_file.write(\"MULTI-COMPONENT FIT SUMMARY:\\n\\n\")\n",
    "            results_file.write(multi_fit_summary)\n",
    "            \n",
    "            # Write to 0multi-comp-fit-results.txt\n",
    "            multi_results_file.write(multi_results_text)\n",
    "            \n",
    "            # Add this workers PNG files to the master lists\n",
    "            all_pdf_out_files.extend(pdf_out_files_worker)\n",
    "            all_multi_pdf_out_files.extend(multi_pdf_out_files_worker)\n",
    "\n",
    "    print(\"Text files written.\")\n",
    "\n",
    "    # PDF Compilation and Cleanup\n",
    "    print('Compiling PDFs...\\n')\n",
    "\n",
    "    # Helper function to compile PNGs into a single PDF\n",
    "    def compile_pngs_to_pdf(pbar, png_files, pdf_filename):\n",
    "        if not png_files:\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(png_files[0]):\n",
    "            print(f\"ERROR: Cannot find file {png_files[0]} to start PDF.\")\n",
    "            return\n",
    "\n",
    "        images = []\n",
    "        img1 = Image.open(png_files[0]).convert('RGB')\n",
    "        pbar.update(1) \n",
    "        \n",
    "        for png_file in png_files[1:]:\n",
    "            if os.path.exists(png_file):\n",
    "                images.append(Image.open(png_file).convert('RGB'))\n",
    "            else:\n",
    "                print(f\"Warning: Missing file {png_file}, skipping.\")\n",
    "            pbar.update(1) \n",
    "        \n",
    "        img1.save(pdf_filename, \"PDF\", resolution=100.0, save_all=True, append_images=images)\n",
    "\n",
    "    # Create one single progress bar for all PDF compilation\n",
    "    total_plots_to_compile = len(all_pdf_out_files) + len(all_multi_pdf_out_files)\n",
    "    \n",
    "    with tqdm(total=total_plots_to_compile, desc=\"Compiling PDF Plots\", bar_format=\"{l_bar}{r_bar}\") as pbar:\n",
    "        try:\n",
    "            # Compile the main PDF all plots\n",
    "            compile_pngs_to_pdf(pbar, all_pdf_out_files, pdf_out_filename)\n",
    "            print(f\"Successfully compiled {pdf_out_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not compile {pdf_out_filename}: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Compile the multi component only PDF\n",
    "            compile_pngs_to_pdf(pbar, all_multi_pdf_out_files, multi_pdf_out_filename)\n",
    "            print(f\"Successfully compiled {multi_pdf_out_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not compile {multi_pdf_out_filename}: {e}\")\n",
    "\n",
    "    # Final cleanup\n",
    "    print(\"Cleaning up temporary PNG files...\")\n",
    "    temp_files_to_clean = glob.glob(\"2Dfits/temp_*.png\")\n",
    "    for f in temp_files_to_clean:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not remove {f}: {e}\")\n",
    "\n",
    "    print('Process Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8f165-374d-4a72-a070-1d7f57bc2309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CIAO-4.17)",
   "language": "python",
   "name": "ciao-4.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
