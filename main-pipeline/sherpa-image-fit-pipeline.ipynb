{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ed5217-8efe-4d40-9871-5d5f9337e37f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from astropy.io import fits\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sherpa.astro.ui import *\n",
    "from sherpa.astro.utils import *\n",
    "from ciao_contrib.runtool import *\n",
    "\n",
    "import datetime\n",
    "from scipy.ndimage import rotate\n",
    "from coords.format import ra2deg, dec2deg\n",
    "\n",
    "# high-DPI plots, larger fonts, origin at lower-left\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 400,\n",
    "    'font.size': 18,\n",
    "    'image.origin': 'lower',\n",
    "})\n",
    "\n",
    "# suppress Sherpa info messages\n",
    "logger = logging.getLogger(\"sherpa\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43f1ed7-4825-4ea7-a983-721da25a296b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quickpos(x, y, x0, y0, iterations=1, size_list=None, binsize_list=None, doplot=False):\n",
    "    \"\"\"\n",
    "    Iteratively refines the centroid position using histogram fitting.\n",
    "    (Streamlined: `step_plot` is now vectorized)\n",
    "    \"\"\"\n",
    "\n",
    "    def step_plot(x, y, binwidth):\n",
    "        xsteps = np.ravel(np.column_stack((x - binwidth/2, x + binwidth/2)))\n",
    "        ysteps = np.repeat(y, 2)\n",
    "        return xsteps, ysteps\n",
    "\n",
    "    # Set default lists if none are provided\n",
    "    if size_list is None:\n",
    "        size_list = [np.max(x) - np.min(x)] * iterations\n",
    "    if binsize_list is None:\n",
    "        binsize_list = [1.0] * iterations\n",
    "\n",
    "    fig_list = []\n",
    "    current_x0, current_y0 = x0, y0\n",
    "    cnt = None\n",
    "    best_x0, best_y0 = current_x0, current_y0 \n",
    "\n",
    "    for i in range(iterations):\n",
    "        size = size_list[i]\n",
    "        binsize = binsize_list[i]\n",
    "\n",
    "        if doplot:\n",
    "            fig, ax = plt.subplots(3, 1, figsize=(6, 10))\n",
    "            ax[0].scatter(x, y, s=0.5, c='k')\n",
    "            ax[0].set_xlim(current_x0 - size, current_x0 + size)\n",
    "            ax[0].set_ylim(current_y0 - size, current_y0 + size)\n",
    "            ax[0].set_title(f'Centroid Plot (Iteration {i+1})')\n",
    "        else:\n",
    "            fig = None\n",
    "            ax = [None, None, None]\n",
    "\n",
    "        ob = np.where((np.abs(x - current_x0) < size) & (np.abs(y - current_y0) < size))\n",
    "        \n",
    "        if len(ob[0]) == 0:\n",
    "            print(f\"Warning: No points found in window for iteration {i+1}. Using previous values.\")\n",
    "            if doplot:\n",
    "                 fig_list.append(fig)\n",
    "                 plt.show()\n",
    "            continue \n",
    "\n",
    "        xbins = np.arange(current_x0 - size, current_x0 + size + binsize, binsize)\n",
    "        ybins = np.arange(current_y0 - size, current_y0 + size + binsize, binsize)\n",
    "\n",
    "        xhist, xedges = np.histogram(x[ob], bins=xbins)\n",
    "        yhist, yedges = np.histogram(y[ob], bins=ybins)\n",
    "\n",
    "        xval = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "        yval = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "\n",
    "        def gaussian(x, a, mu, sigma, offset):\n",
    "            return a * np.exp(-((x - mu)**2) / (2 * sigma**2)) + offset\n",
    "\n",
    "        # Fit Gaussian to x histogram\n",
    "        try:\n",
    "            xmax = np.max(xhist)\n",
    "            x0_new = xval[np.argmax(xhist)]\n",
    "            xestpar = [xmax, x0_new, 2 * binsize, 0]\n",
    "            xpar, _ = curve_fit(gaussian, xval, xhist, p0=xestpar)\n",
    "            xf = gaussian(xval, *xpar)\n",
    "            best_x0 = xpar[1] \n",
    "            xcnt = xpar[0] * xpar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: X-fit failed for iteration {i+1}: {e}. Using previous X value.\")\n",
    "            xcnt = 0\n",
    "            xf = np.zeros_like(xval)\n",
    "\n",
    "        if doplot:\n",
    "            xval_s, xhist_s = step_plot(xval, xhist, binsize)\n",
    "            ax[1].plot(xval_s, xhist_s, c='b', alpha=0.3, label='x histogram')\n",
    "            ax[1].plot(xval, xf, '--', c='red', linewidth=0.75, label='Gaussian fit')\n",
    "            ax[1].legend()\n",
    "\n",
    "        # Fit Gaussian to y histogram\n",
    "        try:\n",
    "            ymax = np.max(yhist)\n",
    "            y0_new = yval[np.argmax(yhist)]\n",
    "            yestpar = [ymax, y0_new, 2 * binsize, 0]\n",
    "            ypar, _ = curve_fit(gaussian, yval, yhist, p0=yestpar)\n",
    "            yf = gaussian(yval, *ypar)\n",
    "            best_y0 = ypar[1] \n",
    "            ycnt = ypar[0] * ypar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Y-fit failed for iteration {i+1}: {e}. Using previous Y value.\")\n",
    "            ycnt = 0\n",
    "            yf = np.zeros_like(yval)\n",
    "\n",
    "\n",
    "        if doplot:\n",
    "            yval_s, yhist_s = step_plot(yval, yhist, binsize)\n",
    "            ax[2].plot(yval_s, yhist_s, c='b', alpha=0.3, label='y histogram')\n",
    "            ax[2].plot(yval, yf, '--', c='red', linewidth=0.75, label='Gaussian fit')\n",
    "            ax[2].legend()\n",
    "\n",
    "        # Estimate counts and update centroid\n",
    "        cnt = 0.5 * (xcnt + ycnt)\n",
    "        \n",
    "        current_x0 = best_x0\n",
    "        current_y0 = best_y0\n",
    "\n",
    "        if doplot:\n",
    "            fig_list.append(fig)\n",
    "            plt.show()\n",
    "\n",
    "    return best_x0, best_y0, cnt, fig_list\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def data_extract_quickpos_iter(infile, iters=3, sizes=[10, 5, 1.5], binsizes=[0.1, 0.1, 0.05]):\n",
    "    with fits.open(infile) as obs:\n",
    "        hdr = obs[1].header\n",
    "        data = obs[1].data\n",
    "        data.shape\n",
    "        \n",
    "        #extracting scale and reference coordinate   \n",
    "        scale = hdr['tcdlt20']\n",
    "        xc = hdr['tcrpx20']\n",
    "        exptime = hdr['exposure']\n",
    "            \n",
    "        #form modified julian date for this obs\n",
    "        mjd_start = hdr['mjd-obs']\n",
    "        half_expos = 0.5 * (hdr['tstop']-hdr['tstart'])\n",
    "        date = mjd_start + half_expos / 86400\n",
    "        \n",
    "        #converting event positions to arcsec\n",
    "        x = (data['x'] - xc) * scale * 3600\n",
    "        y = (data['y'] - xc) * scale * 3600\n",
    "        \n",
    "        rr = np.sqrt(x**2 + y**2)\n",
    "        ok = np.where(rr < 20)\n",
    "        \n",
    "        #starting estimate of centroid    \n",
    "        x0_est = np.average(x[ok])\n",
    "        y0_est = np.average(y[ok])\n",
    "\n",
    "    iterations = iters\n",
    "    size_list = sizes\n",
    "    binsize_list = binsizes\n",
    "    \n",
    "    x0_best, y0_best, cnt, qp_figs = quickpos(x[ok], y[ok], x0_est, y0_est, iterations, size_list, binsize_list)\n",
    "    \n",
    "    pixel_x0_best = x0_best / (scale * 3600) + xc\n",
    "    pixel_y0_best = y0_best / (scale * 3600) + xc\n",
    "\n",
    "    return date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def rotate_psf_array(psf_file, match_file, outfile):\n",
    "    \"\"\"\n",
    "    Rotates a PSF image array based on a match file's ROLL_NOM\n",
    "    and saves it with the *original* PSF's header.\n",
    "    (Streamlined: Uses 'with' statements)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Open the match file to get ROLL_NOM ---\n",
    "    try:\n",
    "        with fits.open(match_file) as hdu_match:\n",
    "            # Check the primary header (HDU 0)\n",
    "            if 'ROLL_NOM' in hdu_match[0].header:\n",
    "                roll_nom = hdu_match[0].header['ROLL_NOM']\n",
    "            # If not, check the EVENTS header (HDU 1)\n",
    "            elif hdu_match[1].header and 'ROLL_NOM' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_NOM']\n",
    "            # Fallback to ROLL_PNT in HDU 1\n",
    "            elif hdu_match[1].header and 'ROLL_PNT' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_PNT']\n",
    "                print(\"  Note: Using 'ROLL_PNT' as 'ROLL_NOM' was not found.\")\n",
    "            else:\n",
    "                print(\"  ERROR: Could not find 'ROLL_NOM' or 'ROLL_PNT' in HDU 0 or 1 of match file.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Match file not found: {match_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read match file header: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Calculate the rotation angle ---\n",
    "    # scipy.ndimage.rotate rotates counter-clockwise\n",
    "    angle_to_rotate = roll_nom - 45.0\n",
    "\n",
    "    # --- 3. Open the PSF file to get its data and header ---\n",
    "    try:\n",
    "        with fits.open(psf_file) as hdu_psf:\n",
    "            if hdu_psf[0].data is None:\n",
    "                # Handle cases where data is in HDU 1\n",
    "                psf_data = hdu_psf[1].data\n",
    "                psf_header = hdu_psf[1].header\n",
    "            else:\n",
    "                psf_data = hdu_psf[0].data\n",
    "                psf_header = hdu_psf[0].header\n",
    "                \n",
    "            if psf_data is None:\n",
    "                print(f\"ERROR: No image data found in HDU 0 or 1 of {psf_file}.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: PSF file not found: {psf_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read PSF file data/header: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Rotate the PSF data array ---\n",
    "    rotated_psf_data = rotate(\n",
    "        psf_data,\n",
    "        angle_to_rotate,\n",
    "        reshape=False,       # Keep the same array shape\n",
    "        cval=0.0,            # Fill new pixels with 0\n",
    "        order=3              # Cubic interpolation\n",
    "    )\n",
    "\n",
    "    # --- 5. Save the new, rotated data ---\n",
    "    \n",
    "    # Add a HISTORY card to document what we did.\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    psf_header.add_history(f\"Rotated by {angle_to_rotate:.4f} deg (ROLL_NOM={roll_nom:.4f} - 45.0)\")\n",
    "    psf_header.add_history(f\"Rotation applied by script on {timestamp}\")\n",
    "\n",
    "    hdu_out = fits.PrimaryHDU(data=rotated_psf_data, header=psf_header)\n",
    "\n",
    "    try:\n",
    "        hdu_out.writeto(outfile, overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not write output file: {e}\")\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def write_pixelscale(file: str, nx: int, ny: int, ra: str, dec: str, hrc_pscale_arcsec: float = 0.13175):\n",
    "    \"\"\"\n",
    "    Adds a WCS header to a FITS file.\n",
    "    (Streamlined: Repetitive 'dmhedit' calls are now looped)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. CRPIX Calculation ---\n",
    "    x_pix_ctr = (nx / 2.0) + 0.5\n",
    "    y_pix_ctr = (ny / 2.0) + 0.5\n",
    "\n",
    "    # --- 2. CDELT Calculation ---\n",
    "    hrc_pscale_deg = hrc_pscale_arcsec / 3600.\n",
    "    \n",
    "    # This is the 1/4 pixel scale\n",
    "    x_platescale = -abs(hrc_pscale_deg / 4.)\n",
    "    y_platescale = abs(hrc_pscale_deg / 4.)\n",
    "    \n",
    "    # --- 3. CRVAL Calculation ---\n",
    "    ra_deg = ra2deg(ra)\n",
    "    dec_deg = dec2deg(dec)\n",
    "\n",
    "    # --- 4. Apply Header Keywords ---\n",
    "    wcs_params = [\n",
    "        # (key, value, datatype, unit)\n",
    "        (\"WCSAXES\", 2, \"short\", None),\n",
    "        (\"CRPIX1\", x_pix_ctr, \"float\", None),\n",
    "        (\"CRPIX2\", y_pix_ctr, \"float\", None),\n",
    "        (\"CDELT1\", x_platescale, \"float\", \"deg\"),\n",
    "        (\"CDELT2\", y_platescale, \"float\", \"deg\"),\n",
    "        (\"CUNIT1\", \"deg\", \"string\", None),\n",
    "        (\"CUNIT2\", \"deg\", \"string\", None),\n",
    "        (\"CTYPE1\", \"RA---TAN\", \"string\", None),\n",
    "        (\"CTYPE2\", \"DEC--TAN\", \"string\", None),\n",
    "        (\"CRVAL1\", ra_deg, \"float\", \"deg\"),\n",
    "        (\"CRVAL2\", dec_deg, \"float\", \"deg\"),\n",
    "        (\"LONPOLE\", 180.0, \"float\", \"deg\"),\n",
    "        (\"LATPOLE\", 0, \"float\", \"deg\"),\n",
    "        (\"RADESYS\", \"ICRS\", \"string\", None),\n",
    "    ]\n",
    "    try:\n",
    "        for key, value, dtype, unit in wcs_params:\n",
    "            dmhedit(infile=file, op=\"add\", key=key, value=value, datatype=dtype, unit=unit)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: dmhedit failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1cc560-3f3b-4889-b921-cfbb5bf2d322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def src_psf_images(obsid, infile, x0, y0, diameter, wcs_ra, wcs_dec, binsize=0.25, shape='square', psfimg=True, showimg=False, empirical_psf=None):\n",
    "    \"\"\"\n",
    "    Creates and loads source and (optionally) PSF images into Sherpa.\n",
    "    (Streamlined: Consolidated tool setup)\n",
    "    \"\"\"\n",
    "\n",
    "    if shape.lower() == 'circle':\n",
    "        region_str = f\"circle({x0},{y0},{diameter/2})\"\n",
    "    elif shape.lower() == 'square':\n",
    "        region_str = f\"box({x0},{y0},{diameter},{diameter},0)\"\n",
    "        # This is the cutout region for the 512x512 PSF in LOGICAL pixels\n",
    "        img_region_str = f\"box(256.5,256.5,{diameter/binsize},{diameter/binsize},0)\"\n",
    "    else:\n",
    "        print('Shape is not circle or square, using user-defined region...')\n",
    "        region_str = shape.lower()\n",
    "\n",
    "    obsid = os.path.dirname(os.path.dirname(infile))\n",
    "    \n",
    "    # Define all output filenames\n",
    "    logical_width = diameter/binsize\n",
    "    \n",
    "    imagefile=f'{obsid}/src_image_{shape}_{int(logical_width)}pixel.fits'\n",
    "    psf_imagefile = f'{obsid}/psf_image_{shape}_raytrace_{int(logical_width)}pixel.fits' # Raytraced PSF\n",
    "    psf_rotated = f'{obsid}/psf_rotated.fits'        # Rotated empirical PSF\n",
    "    psf_rotated_cut = f'{obsid}/psf_rotated_cut.fits'  # Cut, rotated PSF\n",
    "    emp_psf_imagefile = f'{obsid}/psf_image_{shape}_empirical_{int(logical_width)}pixel.fits' # Final reprojected PSF\n",
    "    \n",
    "    dmcopy.punlearn()\n",
    "    dmcopy.clobber = 'yes'\n",
    "    reproject_image.punlearn()\n",
    "    reproject_image.clobber = 'yes'\n",
    "\n",
    "    # --- 1. Process and load the source image ---\n",
    "    print(f\"Creating source image: {imagefile}\")\n",
    "    dmcopy.infile = f'{infile}[sky={region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "    dmcopy.outfile = imagefile\n",
    "    dmcopy()\n",
    "    \n",
    "    load_data(imagefile)\n",
    "    if showimg:\n",
    "        image_close()\n",
    "        image_data()\n",
    "    \n",
    "    # --- 2. Process the PSF image ---\n",
    "    \n",
    "    if empirical_psf is not None:\n",
    "        # Rotate the PSF array\n",
    "        rotate_psf_array(psf_file=empirical_psf, match_file=infile, outfile=psf_rotated)\n",
    "        \n",
    "        # Give the rotated PSF a WCS\n",
    "        try:\n",
    "            with fits.open(psf_rotated) as hdu_rot:\n",
    "                nx = hdu_rot[0].header['NAXIS1']\n",
    "                ny = hdu_rot[0].header['NAXIS2']\n",
    "\n",
    "            write_pixelscale(file=psf_rotated, nx=nx, ny=ny, ra=str(wcs_ra), dec=str(wcs_dec))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR during WCS stamping: {e}\")\n",
    "\n",
    "        # Cut out the rotated, WCS-stamped PSF\n",
    "        dmcopy.infile = f'{psf_rotated}[{img_region_str}][bin x=::{binsize*4},y=::{binsize*4}]'\n",
    "        dmcopy.outfile = psf_rotated_cut\n",
    "        dmcopy()\n",
    "\n",
    "        # Reproject the cut PSF to match the data\n",
    "        reproject_image.infile = psf_rotated_cut\n",
    "        reproject_image.matchfile = imagefile      # Use matchfile parameter\n",
    "        reproject_image.outfile = emp_psf_imagefile # Use your final filename\n",
    "        reproject_image.method = 'sum'           # Conserve flux\n",
    "        reproject_image()\n",
    "\n",
    "        # Load the final PSF\n",
    "        load_psf(f'centr_psf{obsid}', emp_psf_imagefile) # Use your final filename\n",
    "        set_psf(f'centr_psf{obsid}')\n",
    "        print(f\"Loaded empirical PSF: {emp_psf_imagefile}\\n\") # Use your final filename\n",
    "        \n",
    "        if showimg:\n",
    "            image_close()\n",
    "            image_psf()\n",
    "\n",
    "    elif psfimg:\n",
    "        psf_infile = f'{obsid}/raytrace_projrays.fits'\n",
    "        \n",
    "        dmcopy.infile = f'{psf_infile}[{region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "        dmcopy.outfile = psf_imagefile\n",
    "        dmcopy()\n",
    "\n",
    "        load_psf(f'centr_psf{obsid}', psf_imagefile)\n",
    "        set_psf(f'centr_psf{obsid}')\n",
    "\n",
    "        if showimg:\n",
    "            image_close()\n",
    "            image_psf()\n",
    "    else:\n",
    "        print(\"No PSF loaded.\\n\")\n",
    "\n",
    "    return binsize\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def gaussian_image_fit(observation, n_components, position, ampl, fwhm,\n",
    "                       background=0, pos_min=(0, 0), pos_max=None, exptime=None, central_component=None, lock_fwhm=False,\n",
    "                       freeze_components=None, confidence=0, stat='cstat', method='moncar', prefix=\"g\", \n",
    "                       confirm=False, imgfit=False, plot=False, plot_options=None, results=False):\n",
    "\n",
    "    # Helper Functions\n",
    "    def process_numeric_param(param, name):\n",
    "        \"\"\"If param is a single number, expand it into a list for all components;\n",
    "        otherwise verify that the list is of correct length.\"\"\"\n",
    "        if isinstance(param, (int, float)):\n",
    "            return [param] * n_components\n",
    "        elif isinstance(param, list):\n",
    "            if len(param) != n_components:\n",
    "                raise ValueError(f\"The list of {name} values must have length equal to n_components.\")\n",
    "            return param\n",
    "        else:\n",
    "            raise ValueError(f\"{name} must be either a number or a list of numbers.\")\n",
    "\n",
    "    def process_tuple_param(param, name):\n",
    "        \"\"\"If param is a single (x, y) tuple, expand it for all components;\n",
    "        otherwise verify that the list of tuples is of correct length.\"\"\"\n",
    "        if isinstance(param, (tuple, list)) and len(param) == 2 and all(isinstance(x, (int, float)) for x in param):\n",
    "            return [param] * n_components\n",
    "        elif isinstance(param, list):\n",
    "            if len(param) != n_components:\n",
    "                raise ValueError(f\"The list of {name} values must have length equal to n_components.\")\n",
    "            return param\n",
    "        else:\n",
    "            raise ValueError(f\"{name} must be either a tuple (x, y) or a list of such tuples.\")\n",
    "\n",
    "    # Process Input Parameters\n",
    "    positions = process_tuple_param(position, \"position\")\n",
    "    ampls = process_numeric_param(ampl, \"ampl\")\n",
    "    fwhms = process_numeric_param(fwhm, \"fwhm\")\n",
    "    pos_mins = process_tuple_param(pos_min, \"pos_min\")\n",
    "    if pos_max is None:\n",
    "        pos_maxs = [None] * n_components\n",
    "    else:\n",
    "        pos_maxs = process_tuple_param(pos_max, \"pos_max\")\n",
    "\n",
    "    # Build the Model Expression and Create Components\n",
    "    comp_names = []\n",
    "    gaussian_components = []\n",
    "    model_components = []\n",
    "\n",
    "    for i in range(1, n_components + 1):\n",
    "        comp_name = f\"{prefix}{i}\"\n",
    "        comp_names.append(comp_name)\n",
    "        comp = gauss2d(comp_name)\n",
    "        gaussian_components.append(comp)\n",
    "        model_components.append(comp)\n",
    "\n",
    "    # Add background component if needed.\n",
    "    bkg_comp = None\n",
    "    if background > 0:\n",
    "        bkg_comp = const2d(\"c1\")\n",
    "        model_components.append(bkg_comp)\n",
    "\n",
    "    if model_components:\n",
    "        set_source(sum(model_components))\n",
    "    else:\n",
    "        raise ValueError(\"Model expression is empty. Cannot set source.\")\n",
    "\n",
    "    # Assign Parameters and Constraints for Each Gaussian Component\n",
    "    freeze_list = (freeze_components if isinstance(freeze_components, list)\n",
    "                   else ([freeze_components] if freeze_components is not None else []))\n",
    "    for i, comp in enumerate(gaussian_components):\n",
    "        comp_number = i + 1\n",
    "\n",
    "        # positions & amplitude\n",
    "        comp.xpos = positions[i][0]\n",
    "        comp.ypos = positions[i][1]\n",
    "        comp.ampl = ampls[i]\n",
    "        comp.fwhm = fwhms[i]\n",
    "\n",
    "        # constraints\n",
    "        if hasattr(comp.xpos, 'min'): comp.xpos.min = pos_mins[i][0]\n",
    "        if hasattr(comp.ypos, 'min'): comp.ypos.min = pos_mins[i][1]\n",
    "        if pos_maxs[i] is not None:\n",
    "            if hasattr(comp.xpos, 'max'): comp.xpos.max = pos_maxs[i][0]\n",
    "            if hasattr(comp.ypos, 'max'): comp.ypos.max = pos_maxs[i][1]\n",
    "        if hasattr(comp.ampl, 'min'): comp.ampl.min = 0\n",
    "\n",
    "        # any full-component freezes\n",
    "        if comp_number in freeze_list:\n",
    "            freeze(comp)\n",
    "            print(f\"Froze entire component {comp_number} ({comp.name}) as requested.\")\n",
    "\n",
    "    # Link FWHMs\n",
    "    if central_component is not None and lock_fwhm:\n",
    "        master = gaussian_components[central_component-1].fwhm\n",
    "        for idx, comp in enumerate(gaussian_components):\n",
    "            if idx != (central_component-1):\n",
    "                link(comp.fwhm, master)\n",
    "\n",
    "    # Background Component Setup\n",
    "    if bkg_comp is not None:\n",
    "        bkg_comp.c0 = background\n",
    "        if hasattr(bkg_comp.c0, 'min'):\n",
    "            bkg_comp.c0.min = 0\n",
    "\n",
    "    # Confirm Model (Optional)\n",
    "    if confirm:\n",
    "        show_model()\n",
    "        proceed = input(\"Proceed with fit? (y/n): \")\n",
    "        if proceed.lower() != \"y\":\n",
    "            print(\"Fit canceled.\")\n",
    "            return None\n",
    "\n",
    "    # Set Fitting Options and Run the Fit\n",
    "    set_stat(stat)\n",
    "    set_method(method)\n",
    "    if method == 'moncar':\n",
    "        set_method_opt('numcores', 12)\n",
    "        set_method_opt('population_size', 10 * 16 * (n_components * 3 + 1))\n",
    "        set_method_opt('xprob', 0.5)\n",
    "        set_method_opt('weighting_factor', 0.5)\n",
    "\n",
    "    fit()\n",
    "    fit_results = get_fit_results()\n",
    "\n",
    "    # Confidence Interval Calculation\n",
    "    conf_results = None # Ensure conf_results exists\n",
    "    if confidence > 0:\n",
    "        print(f\"-->Moncar fit statistic: {fit_results.statval:.2f}\\n\")\n",
    "        for comp in gaussian_components:\n",
    "            thaw(comp.xpos, comp.ypos, comp.ampl)\n",
    "        print(f\"Calculating {confidence}-sigma confidence intervals using simplex...\")\n",
    "        set_method('simplex')\n",
    "        fit()\n",
    "        fit_results = get_fit_results()\n",
    "        print(f\"-->Simplex fit statistic: {fit_results.statval:.2f}\\n\")\n",
    "        set_conf_opt('numcores', 12)\n",
    "        set_conf_opt('sigma', confidence)\n",
    "        conf()\n",
    "        conf_results = get_conf_results()\n",
    "\n",
    "    if imgfit:\n",
    "        image_close()\n",
    "        image_fit()\n",
    "\n",
    "    # Build Fit Result Summary (Optional)\n",
    "    if results:\n",
    "        fit_summary = (\n",
    "            f\"Method = {fit_results.methodname}\\n\"\n",
    "            f\"Statistic = {fit_results.statname}\\n\"\n",
    "            f\"Initial fit statistic = {fit_results.istatval:.2f}\\n\"\n",
    "            f\"Final fit statistic = {fit_results.statval:.2f} at function evaluation {fit_results.nfev}\\n\"\n",
    "            f\"Data points = {fit_results.numpoints}\\n\"\n",
    "            f\"Degrees of freedom = {fit_results.dof}\\n\"\n",
    "            f\"Probability [Q-value] = {fit_results.qval}\\n\"\n",
    "            f\"Reduced statistic = {fit_results.rstat:.5f}\\n\"\n",
    "            f\"Change in statistic = {fit_results.dstatval:.2f}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        def fmt_val(val, width=10, prec=3):\n",
    "            if val is None:\n",
    "                return \"-------\".rjust(width)\n",
    "            return f\"{val:>{width}.{prec}f}\"\n",
    "\n",
    "        if confidence and conf_results is not None:\n",
    "            param_table_rows = [\n",
    "                f\"confidence {confidence}-sigma bounds:\",\n",
    "                f\"{'Param':<10}\\t{'Best-Fit':>10}\\t{'Lower':>10}\\t{'Upper':>10}\",\n",
    "                f\"{'-'*5:<10}\\t{'-'*8:>10}\\t{'-'*5:>10}\\t{'-'*5:>10}\"\n",
    "            ]\n",
    "            for name, best, low, high in zip(conf_results.parnames, \n",
    "                                             conf_results.parvals, \n",
    "                                             conf_results.parmins, \n",
    "                                             conf_results.parmaxes):\n",
    "                param_table_rows.append(\n",
    "                    f\"{name:<10}\\t{fmt_val(best)}\\t{fmt_val(low)}\\t{fmt_val(high)}\"\n",
    "                )\n",
    "            param_table = \"\\n\".join(param_table_rows)\n",
    "        else:\n",
    "            param_table_rows = [\n",
    "                \"Best-Fit Parameter Values:\",\n",
    "                f\"{'Param':<10}\\t{'Best-Fit':>10}\",\n",
    "                f\"{'-'*5:<10}\\t{'-'*8:>10}\"\n",
    "            ]\n",
    "            for name, best in zip(fit_results.parnames, fit_results.parvals):\n",
    "                param_table_rows.append(f\"{name:<10}\\t{fmt_val(best)}\")\n",
    "            param_table = \"\\n\".join(param_table_rows)\n",
    "                \n",
    "        summary_output = fit_summary + param_table + '\\n'\n",
    "\n",
    "        # Count rate block (with amplitude & FWHM error propagation)\n",
    "        if exptime and confidence > 0 and conf_results is not None:\n",
    "            rate_block_rows = [\"Component count rates (counts/s):\"]\n",
    "            for comp in gaussian_components:\n",
    "                comp_img   = get_model_component_image(comp.name)\n",
    "                total_cts  = comp_img.y.sum()\n",
    "                rate       = total_cts / exptime\n",
    "                short      = comp.name.split('.')[-1]\n",
    "                amp_name   = f\"{short}.ampl\"\n",
    "                fwhm_name  = f\"{short}.fwhm\"\n",
    "    \n",
    "                if amp_name in conf_results.parnames:\n",
    "                    a_idx      = conf_results.parnames.index(amp_name)\n",
    "                    A_best     = conf_results.parvals[a_idx]\n",
    "                    \n",
    "                    dA_minus_val = conf_results.parmins[a_idx]\n",
    "                    dA_plus_val  = conf_results.parmaxes[a_idx]\n",
    "                    \n",
    "                    dA_minus = abs(dA_minus_val) if dA_minus_val is not None else 0\n",
    "                    dA_plus  = abs(dA_plus_val)  if dA_plus_val  is not None else 0\n",
    "                else:\n",
    "                    A_best = 1; dA_minus = 0; dA_plus = 0\n",
    "    \n",
    "                if fwhm_name in conf_results.parnames:\n",
    "                    f_idx      = conf_results.parnames.index(fwhm_name)\n",
    "                    F_best     = conf_results.parvals[f_idx]\n",
    "                    \n",
    "                    dF_minus_val = conf_results.parmins[f_idx]\n",
    "                    dF_plus_val  = conf_results.parmaxes[f_idx]\n",
    "                    \n",
    "                    dF_minus = abs(dF_minus_val) if dF_minus_val is not None else 0\n",
    "                    dF_plus  = abs(dF_plus_val)  if dF_plus_val  is not None else 0\n",
    "                else:\n",
    "                    F_best = 1; dF_minus = 0; dF_plus = 0\n",
    "    \n",
    "                frac_minus = np.sqrt((dA_minus/A_best)**2 + (2*dF_minus/F_best)**2) if A_best > 0 and F_best > 0 else 0\n",
    "                frac_plus  = np.sqrt((dA_plus /A_best)**2 + (2*dF_plus /F_best)**2) if A_best > 0 and F_best > 0 else 0\n",
    "    \n",
    "                dR_minus = (total_cts * frac_minus) / exptime\n",
    "                dR_plus  = (total_cts * frac_plus)  / exptime\n",
    "    \n",
    "                rate_block_rows.append(\n",
    "                    f\"  {short:<6}: {rate:7.4f}  \"\n",
    "                    f\"(-{dR_minus:6.4f}/+{dR_plus:6.4f})\"\n",
    "                )\n",
    "            summary_output += \"\\n\" + \"\\n\".join(rate_block_rows) + \"\\n\"\n",
    "        else:\n",
    "            summary_output = fit_summary + param_table + '\\n\\n\\n\\n'\n",
    "\n",
    "    # Retrieve Data/Model Images and Compute Residuals\n",
    "    data_img = get_data_image()\n",
    "    data_vals = data_img.y\n",
    "    min_positive_val = np.min(data_vals[data_vals > 0]) if np.any(data_vals > 0) else 1e-9\n",
    "    display_floor = min_positive_val / 10.0\n",
    "    \n",
    "    data_masked = np.maximum(data_vals, display_floor) \n",
    "\n",
    "    model_img = get_model_image()\n",
    "    model_vals = model_img.y\n",
    "    model_masked = np.maximum(model_vals, display_floor)\n",
    "    \n",
    "    # Use the original, unmasked arrays for logic\n",
    "    d_vals = data_vals\n",
    "    m_vals = model_vals\n",
    "\n",
    "    # Start with the standard formula, using masked arrays to avoid log(0)\n",
    "    # This is Case 3 (d > 0, m > 0). We will overwrite the other cases.\n",
    "    D = 2.0 * (data_masked * np.log(data_masked / model_masked) - (data_masked - model_masked))\n",
    "    \n",
    "    # Apply Case 1 (m <= 0), dev = 2*d\n",
    "    D = np.where(m_vals <= 0, 2.0 * d_vals, D)\n",
    "    \n",
    "    # Apply Case 2 (m > 0 AND d <= 0), dev = 2*m\n",
    "    D = np.where((m_vals > 0) & (d_vals <= 0), 2.0 * m_vals, D)\n",
    "\n",
    "    resid_dev = np.sign(data_vals - model_vals) * np.sqrt(np.abs(D))\n",
    "\n",
    "    # Determine and Prepare Plot Options\n",
    "    if plot_options is None:\n",
    "        plot_options = [\"data_fit\", \"model\", \"deviance\"]\n",
    "    elif isinstance(plot_options, str):\n",
    "        plot_options = [plot_options]\n",
    "    elif not isinstance(plot_options, list):\n",
    "        print(f\"Warning: plot_options was of type {type(plot_options)}. Using default plots.\")\n",
    "        plot_options = [\"data_fit\", \"model\", \"deviance\"]\n",
    "\n",
    "    n_plots = len(plot_options)\n",
    "    if n_plots == 0:\n",
    "        print(\"No plots requested.\")\n",
    "        fig, axs = plt.subplots(1, 1); plt.close(fig) # Create and close empty fig\n",
    "        return (summary_output, fig) if results else fig\n",
    "\n",
    "    fig, axs = plt.subplots(1, n_plots, figsize=(10 * n_plots, 5 * n_plots))\n",
    "    if n_plots == 1:\n",
    "        axs = [axs]\n",
    "    plot_idx = 0\n",
    "\n",
    "    vmax_display = np.max(data_vals)\n",
    "    log_norm = mcolors.LogNorm(\n",
    "        vmin=display_floor, \n",
    "        vmax=vmax_display if vmax_display > display_floor else display_floor + 1\n",
    "    )\n",
    "\n",
    "    # Plot \"data_fit\"\n",
    "    if \"data_fit\" in plot_options:\n",
    "        ax = axs[plot_idx]\n",
    "        im = ax.imshow(data_masked, origin='lower', cmap='gnuplot2', norm=log_norm,\n",
    "                       interpolation='nearest')\n",
    "\n",
    "        legend_elements = []\n",
    "        base_colors = ['white', 'cyan', 'lime', 'xkcd:light lavender']\n",
    "        linestyles = ['--', ':', '-.']\n",
    "\n",
    "        for i, comp_name in enumerate(comp_names):\n",
    "            comp_vals = get_model_component_image(comp_name).y\n",
    "            if not np.any(comp_vals > 0): continue\n",
    "\n",
    "            color = base_colors[i % len(base_colors)]\n",
    "            linestyle = '--' if i < len(base_colors) else linestyles[(i // len(base_colors)) % len(linestyles)]\n",
    "\n",
    "            if n_components > 1:\n",
    "                level = 0.2 * np.max(comp_vals)\n",
    "                if level <= 0: level = 1e-9 * np.max(comp_vals)\n",
    "                if level > 0:\n",
    "                    ax.contour(comp_vals, levels=[level], colors=[color],\n",
    "                               linestyles=linestyle, linewidths=2)\n",
    "            else:\n",
    "                levels = np.linspace(np.min(comp_vals), np.max(comp_vals), 6)\n",
    "                if len(np.unique(levels)) > 1:\n",
    "                    ax.contour(comp_vals, levels=levels[1:], colors=[color],\n",
    "                               linestyles=linestyle, linewidths=2)\n",
    "\n",
    "            legend_elements.append(Line2D([0], [0], lw=2, linestyle=linestyle,\n",
    "                                          color=color, label=f\"{comp_name}\"))\n",
    "        if legend_elements:\n",
    "            ax.legend(handles=legend_elements, loc='upper right')\n",
    "        ax.set_title(f\"{observation} Data + Fit Overlay\")\n",
    "        ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "        fig.colorbar(im, ax=ax, label=\"Counts\", shrink=0.53)\n",
    "        plot_idx += 1\n",
    "\n",
    "    # Plot \"model\"\n",
    "    if \"model\" in plot_options:\n",
    "        if plot_idx >= n_plots: return (summary_output, fig) if results else fig\n",
    "        ax = axs[plot_idx]\n",
    "        im = ax.imshow(model_masked, origin='lower', cmap='gnuplot2', norm=log_norm,\n",
    "                       interpolation='nearest')\n",
    "        ax.set_title(\"Model\")\n",
    "        ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "        fig.colorbar(im, ax=ax, label=\"Model Counts\", shrink=0.53)\n",
    "        plot_idx += 1\n",
    "\n",
    "    # Plot \"deviance\"\n",
    "    if \"deviance\" in plot_options:\n",
    "        if plot_idx >= n_plots: return (summary_output, fig) if results else fig\n",
    "        ax = axs[plot_idx]\n",
    "        im = ax.imshow(np.abs(resid_dev), origin='lower', cmap='gnuplot2',\n",
    "                       norm=mcolors.Normalize(vmin=0, vmax=5),\n",
    "                       interpolation='nearest')\n",
    "        ax.set_title(\"Poisson Deviance Residuals\")\n",
    "        ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "        fig.colorbar(im, ax=ax, label=\"|Residuals|\", shrink=0.53)\n",
    "        plot_idx += 1\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Return Results\n",
    "    return (summary_output, fig) if results else fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d6a9ef-571f-4bb3-94b7-7abcc06c28a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 26568\n",
      "\n",
      "Creating source image: 26568/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26568/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26568/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26568/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26568/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4181.17\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4181.17\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26569\n",
      "\n",
      "Creating source image: 26569/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26569/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26569/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26569/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26569/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4131.82\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4131.82\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26570\n",
      "\n",
      "Creating source image: 26570/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26570/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26570/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26570/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26570/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4151.07\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4151.07\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26571\n",
      "\n",
      "Creating source image: 26571/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26571/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26571/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26571/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26571/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 3851.48\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 3851.48\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26572\n",
      "\n",
      "Creating source image: 26572/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26572/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26572/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26572/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26572/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 3880.98\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 3880.98\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26573\n",
      "\n",
      "Creating source image: 26573/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26573/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26573/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26573/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26573/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 3819.23\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 3819.23\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26574\n",
      "\n",
      "Creating source image: 26574/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26574/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26574/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26574/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26574/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4126.10\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4126.10\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26575\n",
      "\n",
      "Creating source image: 26575/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26575/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26575/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26575/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26575/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4061.52\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4061.52\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26576\n",
      "\n",
      "Creating source image: 26576/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26576/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26576/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26576/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26576/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4197.63\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4197.63\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26577\n",
      "\n",
      "Creating source image: 26577/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26577/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26577/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26577/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26577/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4107.54\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4107.54\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26578\n",
      "\n",
      "Creating source image: 26578/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26578/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26578/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26578/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26578/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4311.93\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4311.93\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "\n",
      "Processing 26579\n",
      "\n",
      "Creating source image: 26579/src_image_square_40pixel.fits\n",
      "No PSF loaded.\n",
      "\n",
      "Centroiding...\n",
      "\n",
      "Creating source image: 26579/src_image_square_40pixel.fits\n",
      "Loaded empirical PSF: 26579/psf_image_square_empirical_40pixel.fits\n",
      "\n",
      "Fitting Source...\n",
      "\n",
      "Creating source image: 26579/src_image_square_80pixel.fits\n",
      "Loaded empirical PSF: 26579/psf_image_square_empirical_80pixel.fits\n",
      "\n",
      "Fitting multi-component Gaussian...\n",
      "-->Moncar fit statistic: 4164.65\n",
      "\n",
      "Calculating 1-sigma confidence intervals using simplex...\n",
      "-->Simplex fit statistic: 4164.65\n",
      "\n",
      "\n",
      "Plots saving to PDF...\n",
      "\n",
      "Sherpa Session Cleaned\n",
      "\n",
      "\n",
      "Tidying Up...\n",
      "\n",
      "Process Complete\n"
     ]
    }
   ],
   "source": [
    "# Change to your working directory.\n",
    "os.chdir('/Users/leodrake/Documents/MIT/ss433/HRC_2024/')\n",
    "\n",
    "# We define the specific RA/Dec for each ObsID here.\n",
    "# Keys are strings to match the 'obsid' variable.\n",
    "# Values are (ra, dec) tuples of strings.\n",
    "obsid_coords = {\n",
    "    \"26568\": (\"287.9565362\", \"4.9826061\"),\n",
    "    \"26569\": (\"287.9563218\", \"4.9827745\"),\n",
    "    \"26570\": (\"287.9563754\", \"4.9825322\"),\n",
    "    \"26571\": (\"287.9561693\", \"4.9827006\"),\n",
    "    \"26572\": (\"287.9565032\", \"4.9826636\"),\n",
    "    \"26573\": (\"287.9565444\", \"4.9826390\"),\n",
    "    \"26574\": (\"287.9562518\", \"4.9825651\"),\n",
    "    \"26575\": (\"287.9566969\", \"4.9828114\"),\n",
    "    \"26576\": (\"287.9566351\", \"4.9826718\"),\n",
    "    \"26577\": (\"287.9565238\", \"4.9826020\"),\n",
    "    \"26578\": (\"287.9566021\", \"4.9826800\"),\n",
    "    \"26579\": (\"287.9565733\", \"4.9825774\")\n",
    "}\n",
    "\n",
    "# Define the empirical PSF file to be used.\n",
    "emp_psf_file = \"/Users/leodrake/Documents/MIT/ss433/HRC_2024/empPSF_iARLac_v2025_2017-2025.fits\" \n",
    "\n",
    "# Find all evt2.fits files in subdirectories.\n",
    "event_files = sorted(glob.glob('*/repro/*splinecorr.fits'))\n",
    "\n",
    "# Open PdfPages objects to save one page per observation.\n",
    "pdf_out = PdfPages('2Dfits/0fit-plots.pdf')\n",
    "multi_pdf_out = PdfPages('2Dfits/0multi-comp-plots.pdf')\n",
    "\n",
    "# Open the main results file and the multi-component-only results file.\n",
    "results_filename = '2Dfits/0fit-results.txt'\n",
    "multi_results_filename = '2Dfits/0multi-comp-fit-results.txt'\n",
    "with open(results_filename, 'w') as results_file, open(multi_results_filename, 'w') as multi_results_file:\n",
    "\n",
    "    # Loop over each event file.\n",
    "    for infile in event_files[:]:\n",
    "        # Extract observation directory/name.\n",
    "        obsid = os.path.dirname(os.path.dirname(infile))\n",
    "        print(f'\\nProcessing {obsid}\\n')\n",
    "\n",
    "        # Get coordinates for this ObsID\n",
    "        if obsid not in obsid_coords:\n",
    "            print(f\"!!! WARNING: ObsID {obsid} not in coordinate lookup table. Skipping.\")\n",
    "            continue\n",
    "        current_ra, current_dec = obsid_coords[obsid]\n",
    "        \n",
    "        # Data extraction and initial quickpos\n",
    "        date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs = data_extract_quickpos_iter(infile)\n",
    "        \n",
    "        # Aggregate text block for writing\n",
    "        header_text = (\n",
    "            f\"Observation: {obsid}\\n\"\n",
    "            f\"Infile: {infile}\\n\"\n",
    "            f\"Date: {date}, Exptime: {exptime}\\n\"\n",
    "        )\n",
    "        results_file.write(header_text)\n",
    "\n",
    "        # PSF image and centroid fit\n",
    "        img_width = 40  # physical pixels\n",
    "        \n",
    "        cent_binsize = 1.0 # Define binsize for this step\n",
    "        src_psf_images(\n",
    "            obsid, infile, pixel_x0_best, pixel_y0_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec, # Pass WCS info\n",
    "            binsize=cent_binsize, \n",
    "            psfimg=False, \n",
    "            empirical_psf=None\n",
    "        )\n",
    "        \n",
    "        logical_width = img_width / cent_binsize\n",
    "        img_center = logical_width / 2.0 + 0.5\n",
    "\n",
    "        print('Centroiding...\\n')\n",
    "        \n",
    "        centroid_fit_summary, centroid_fit_fig = gaussian_image_fit(\n",
    "            obsid, 1, (img_center, img_center), cnt, (1.0 / cent_binsize), # Scale FWHM guess\n",
    "            prefix=\"centrg\",\n",
    "            background=0.1, \n",
    "            pos_max=(logical_width, logical_width), # Use logical width\n",
    "            results=True\n",
    "        )\n",
    "        pdf_out.savefig(centroid_fit_fig)\n",
    "        plt.close(centroid_fit_fig)\n",
    "\n",
    "        # Save centroid fit parameters\n",
    "        results_file.write(\"\\nCENTROID FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(centroid_fit_summary)\n",
    "\n",
    "        # Retrieve centroid fit physical coordinates\n",
    "        d = get_data()\n",
    "        crval_x, crval_y = d.sky.crval\n",
    "        crpix_x, crpix_y = d.sky.crpix\n",
    "        cdelt_x, cdelt_y = d.sky.cdelt\n",
    "        xphys_best = crval_x + (centrg1.xpos.val - crpix_x) * cdelt_x\n",
    "        yphys_best = crval_y + (centrg1.ypos.val - crpix_y) * cdelt_y\n",
    "\n",
    "        # Source fit in physical coordinates\n",
    "        img_width = 10  # physical pixels\n",
    "        \n",
    "        # Using EMPIRICAL PSF at binsize=0.25\n",
    "        src_binsize = 0.25 # This is forced by using empirical_psf\n",
    "        src_psf_images(\n",
    "            obsid, infile, xphys_best, yphys_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec, # Pass WCS info\n",
    "            binsize=src_binsize, \n",
    "            psfimg=True, \n",
    "            empirical_psf=emp_psf_file\n",
    "        )\n",
    "\n",
    "        logical_width = img_width / src_binsize # 10 / 0.25 = 40\n",
    "        img_center = logical_width / 2.0 + 0.5 # 40 / 2 + 0.5 = 20.5\n",
    "        \n",
    "        # Scale 'cnt' and 'fwhm' guess to this 0.25-binned image\n",
    "        pixel_scale_guess = 1.0 / src_binsize # 4.0\n",
    "        scaled_cnt_guess = cnt / (pixel_scale_guess**2)\n",
    "        scaled_fwhm_guess = 1.0 * pixel_scale_guess # fwhm=1.0 at binsize=1.0 -> fwhm=4.0 at binsize=0.25\n",
    "\n",
    "        print('Fitting Source...\\n')\n",
    "        \n",
    "        src_fit_summary, src_fit_fig = gaussian_image_fit(\n",
    "            obsid, 1, (img_center, img_center), scaled_cnt_guess, scaled_fwhm_guess,\n",
    "            prefix=\"srcg\",\n",
    "            pos_max=(logical_width, logical_width),\n",
    "            results=True\n",
    "        )\n",
    "        pdf_out.savefig(src_fit_fig)\n",
    "        plt.close(src_fit_fig)\n",
    "\n",
    "        # Save source fit parameters\n",
    "        results_file.write(\"SOURCE FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(src_fit_summary)\n",
    "\n",
    "        # Multi-component fit\n",
    "        \n",
    "        # Dynamic scaling logic\n",
    "        srcfit_off_x = srcg1.xpos.val - img_center # img_center is 20.5\n",
    "        srcfit_off_y = srcg1.ypos.val - img_center # img_center is 20.5\n",
    "        src_ampl = srcg1.ampl.val\n",
    "        src_fwhm = srcg1.fwhm.val\n",
    "        \n",
    "        # Set up for multi-component fit\n",
    "        img_width = 40 # physical pixels\n",
    "        multi_binsize = 0.5\n",
    "        \n",
    "        src_psf_images(\n",
    "            obsid, infile, xphys_best, yphys_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec, # Pass WCS info\n",
    "            binsize=multi_binsize, \n",
    "            empirical_psf=emp_psf_file\n",
    "        )\n",
    "        \n",
    "        logical_width = img_width / multi_binsize # 160.0\n",
    "        img_center = logical_width / 2.0 + 0.5   # 80.5\n",
    "        \n",
    "        # Calculate scale factor\n",
    "        pixel_scale = src_binsize / multi_binsize # 0.25 / 0.25 = 1.0\n",
    "        \n",
    "        # Scale parameters\n",
    "        new_xpos = img_center + (srcfit_off_x * pixel_scale)\n",
    "        new_ypos = img_center + (srcfit_off_y * pixel_scale)\n",
    "        scaled_src_fwhm = src_fwhm * pixel_scale\n",
    "        scaled_src_ampl = src_ampl / (pixel_scale**2)\n",
    "        \n",
    "        # Re-scale the 'cnt' guess for this new image\n",
    "        pixel_scale_guess = 1.0 / multi_binsize # 4.0\n",
    "        scaled_cnt_ampl = cnt / (pixel_scale_guess**2)\n",
    "        scaled_default_fwhm = 1.0 * pixel_scale_guess # 4.0\n",
    "\n",
    "        n_components = 3  # total number of components\n",
    "        positions = [(new_xpos, new_ypos)] + [(img_center, img_center)] * (n_components - 1)\n",
    "        amplitudes = [scaled_src_ampl] + [scaled_cnt_ampl] * (n_components - 1)\n",
    "        fwhms = [scaled_src_fwhm] + [scaled_default_fwhm] * (n_components - 1)\n",
    "\n",
    "        print('Fitting multi-component Gaussian...')\n",
    "\n",
    "        multi_fit_summary, multi_fit_fig = gaussian_image_fit(\n",
    "            obsid, n_components, positions, amplitudes, fwhms,\n",
    "            prefix=\"g\",\n",
    "            background=0.1,\n",
    "            pos_max=(logical_width, logical_width),\n",
    "            pos_min=(0, 0),\n",
    "            exptime=exptime,\n",
    "            central_component=1,\n",
    "            lock_fwhm=True,\n",
    "            confidence=1,\n",
    "            results=True\n",
    "        )\n",
    "\n",
    "        print('\\nPlots saving to PDF...\\n')\n",
    "        pdf_out.savefig(multi_fit_fig)\n",
    "        multi_pdf_out.savefig(multi_fit_fig)\n",
    "        plt.close(multi_fit_fig)\n",
    "\n",
    "        # Save multi-component fit parameters\n",
    "        results_file.write(\"MULTI-COMPONENT FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(multi_fit_summary)\n",
    "\n",
    "        # Aggregate text block for writing\n",
    "        multi_results_text = (\n",
    "            f\"Observation: {obsid}\\n\"\n",
    "            f\"Infile: {infile}\\n\"\n",
    "            f\"Date: {date}, Exptime: {exptime}\\n\"\n",
    "            f\"{multi_fit_summary}\\n\\n\"\n",
    "        )\n",
    "        multi_results_file.write(multi_results_text)\n",
    "\n",
    "        # Clear the current Sherpa session to prepare for the next observation\n",
    "        clean()\n",
    "        print('Sherpa Session Cleaned\\n\\n')\n",
    "\n",
    "# Close the PDF files after processing all files.\n",
    "print('Tidying Up...\\n')\n",
    "pdf_out.close()\n",
    "multi_pdf_out.close()\n",
    "\n",
    "print('Process Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8f165-374d-4a72-a070-1d7f57bc2309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CIAO-4.17)",
   "language": "python",
   "name": "ciao-4.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
