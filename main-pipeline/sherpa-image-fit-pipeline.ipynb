{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed5217-8efe-4d40-9871-5d5f9337e37f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Core scientific and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D\n",
    "from astropy.io import fits\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Sherpa and CIAO imports\n",
    "from sherpa.astro.ui import *\n",
    "from sherpa.astro.utils import *\n",
    "from ciao_contrib.runtool import *\n",
    "from coords.format import ra2deg, dec2deg\n",
    "\n",
    "# MCMC and PDF compilation imports\n",
    "import corner\n",
    "from PIL import Image\n",
    "\n",
    "# high-DPI plots, larger fonts, origin at lower-left\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 400,\n",
    "    'font.size': 18,\n",
    "    'image.origin': 'lower',\n",
    "})\n",
    "\n",
    "# suppress Sherpa info messages\n",
    "logger = logging.getLogger(\"sherpa\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f1ed7-4825-4ea7-a983-721da25a296b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quickpos(x, y, x0, y0, iterations=1, size_list=None, binsize_list=None, doplot=False):\n",
    "    \"\"\"\n",
    "    Iteratively refines the centroid position using 1D histogram fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to create step-plot coordinates from histograms\n",
    "    def step_plot(x, y, binwidth):\n",
    "        xsteps = np.ravel(np.column_stack((x - binwidth/2, x + binwidth/2)))\n",
    "        ysteps = np.repeat(y, 2)\n",
    "        return xsteps, ysteps\n",
    "\n",
    "    # Set default window/bin sizes if none are provided\n",
    "    if size_list is None:\n",
    "        size_list = [np.max(x) - np.min(x)] * iterations\n",
    "    if binsize_list is None:\n",
    "        binsize_list = [1.0] * iterations\n",
    "\n",
    "    # Initialize variables\n",
    "    fig_list = []\n",
    "    current_x0, current_y0 = x0, y0\n",
    "    cnt = None\n",
    "    best_x0, best_y0 = current_x0, current_y0 \n",
    "\n",
    "    # Loop for each refinement iteration\n",
    "    for i in range(iterations):\n",
    "        size = size_list[i]\n",
    "        binsize = binsize_list[i]\n",
    "\n",
    "        # Create debug plots if requested\n",
    "        if doplot:\n",
    "            fig, ax = plt.subplots(3, 1, figsize=(6, 10))\n",
    "            ax[0].scatter(x, y, s=0.5, c='k')\n",
    "            ax[0].set_xlim(current_x0 - size, current_x0 + size)\n",
    "            ax[0].set_ylim(current_y0 - size, current_y0 + size)\n",
    "            ax[0].set_title(f'Centroid Plot (Iteration {i+1})')\n",
    "        else:\n",
    "            fig = None\n",
    "            ax = [None, None, None]\n",
    "\n",
    "        # Get all points within the current window\n",
    "        ob = np.where((np.abs(x - current_x0) < size) & (np.abs(y - current_y0) < size))\n",
    "        \n",
    "        # Skip if no points are found\n",
    "        if len(ob[0]) == 0:\n",
    "            print(f\"Warning: No points found in window for iteration {i+1}. Using previous values.\")\n",
    "            if doplot:\n",
    "                 fig_list.append(fig)\n",
    "                 plt.show()\n",
    "            continue \n",
    "\n",
    "        # Create 1D histograms for X and Y\n",
    "        xbins = np.arange(current_x0 - size, current_x0 + size + binsize, binsize)\n",
    "        ybins = np.arange(current_y0 - size, current_y0 + size + binsize, binsize)\n",
    "\n",
    "        xhist, xedges = np.histogram(x[ob], bins=xbins)\n",
    "        yhist, yedges = np.histogram(y[ob], bins=ybins)\n",
    "\n",
    "        # Get bin centers\n",
    "        xval = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "        yval = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "\n",
    "        # Define 1D Gaussian model for fitting\n",
    "        def gaussian(x, a, mu, sigma, offset):\n",
    "            return a * np.exp(-((x - mu)**2) / (2 * sigma**2)) + offset\n",
    "\n",
    "        # Fit Gaussian to x histogram\n",
    "        try:\n",
    "            xmax = np.max(xhist)\n",
    "            x0_new = xval[np.argmax(xhist)]\n",
    "            xestpar = [xmax, x0_new, 2 * binsize, 0]\n",
    "            xpar, _ = curve_fit(gaussian, xval, xhist, p0=xestpar)\n",
    "            xf = gaussian(xval, *xpar)\n",
    "            best_x0 = xpar[1] \n",
    "            xcnt = xpar[0] * xpar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: X-fit failed for iteration {i+1}: {e}. Using previous X value.\")\n",
    "            xcnt = 0\n",
    "            xf = np.zeros_like(xval)\n",
    "\n",
    "        if doplot:\n",
    "            xval_s, xhist_s = step_plot(xval, xhist, binsize)\n",
    "            ax[1].plot(xval_s, xhist_s, c='b', alpha=0.3, label='x histogram')\n",
    "            ax[1].plot(xval, xf, '--', c='red', linewidth=0.75, label='Gaussian fit')\n",
    "            ax[1].legend()\n",
    "\n",
    "        # Fit Gaussian to y histogram\n",
    "        try:\n",
    "            ymax = np.max(yhist)\n",
    "            y0_new = yval[np.argmax(yhist)]\n",
    "            yestpar = [ymax, y0_new, 2 * binsize, 0]\n",
    "            ypar, _ = curve_fit(gaussian, yval, yhist, p0=yestpar)\n",
    "            yf = gaussian(yval, *ypar)\n",
    "            best_y0 = ypar[1] \n",
    "            ycnt = ypar[0] * ypar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Y-fit failed for iteration {i+1}: {e}. Using previous Y value.\")\n",
    "            ycnt = 0\n",
    "            yf = np.zeros_like(yval)\n",
    "\n",
    "\n",
    "        if doplot:\n",
    "            yval_s, yhist_s = step_plot(yval, yhist, binsize)\n",
    "            ax[2].plot(yval_s, yhist_s, c='b', alpha=0.3, label='y histogram')\n",
    "            ax[2].plot(yval, yf, '--', c='red', linewidth=0.75, label='Gaussian fit')\n",
    "            ax[2].legend()\n",
    "\n",
    "        # Estimate counts and update centroid for next iteration\n",
    "        cnt = 0.5 * (xcnt + ycnt)\n",
    "        \n",
    "        current_x0 = best_x0\n",
    "        current_y0 = best_y0\n",
    "\n",
    "        if doplot:\n",
    "            fig_list.append(fig)\n",
    "            plt.show()\n",
    "\n",
    "    return best_x0, best_y0, cnt, fig_list\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def data_extract_quickpos_iter(infile, iters=3, sizes=[10, 5, 1.5], binsizes=[0.1, 0.1, 0.05]):\n",
    "    \"\"\"\n",
    "    Extracts data from a FITS file and runs quickpos to get an initial centroid.\n",
    "    \"\"\"\n",
    "    # Open the FITS file\n",
    "    with fits.open(infile) as obs:\n",
    "        hdr = obs[1].header\n",
    "        data = obs[1].data\n",
    "        \n",
    "        # Extract essential header info\n",
    "        scale = hdr['tcdlt20']\n",
    "        xc = hdr['tcrpx20']\n",
    "        exptime = hdr['exposure']\n",
    "            \n",
    "        # Form modified julian date for this obs\n",
    "        mjd_start = hdr['mjd-obs']\n",
    "        half_expos = 0.5 * (hdr['tstop']-hdr['tstart'])\n",
    "        date = mjd_start + half_expos / 86400\n",
    "        \n",
    "        # Convert event positions to arcsec\n",
    "        x = (data['x'] - xc) * scale * 3600\n",
    "        y = (data['y'] - xc) * scale * 3600\n",
    "        \n",
    "        # Filter a 20 arcsec radius region\n",
    "        rr = np.sqrt(x**2 + y**2)\n",
    "        ok = np.where(rr < 20)\n",
    "        \n",
    "        # Get a rough starting estimate of the centroid    \n",
    "        x0_est = np.average(x[ok])\n",
    "        y0_est = np.average(y[ok])\n",
    "\n",
    "    # Set iteration parameters\n",
    "    iterations = iters\n",
    "    size_list = sizes\n",
    "    binsize_list = binsizes\n",
    "    \n",
    "    # Run the iterative centroid refinement\n",
    "    x0_best, y0_best, cnt, qp_figs = quickpos(x[ok], y[ok], x0_est, y0_est, iterations, size_list, binsize_list)\n",
    "    \n",
    "    # Convert best-fit arcsec position back to pixels\n",
    "    pixel_x0_best = x0_best / (scale * 3600) + xc\n",
    "    pixel_y0_best = y0_best / (scale * 3600) + xc\n",
    "\n",
    "    return date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def rotate_psf_array(psf_file, match_file, outfile):\n",
    "    \"\"\"\n",
    "    Rotates a PSF image array based on a match file's ROLL_NOM\n",
    "    and saves it with the *original* PSF's header.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the roll angle from the observation's event file\n",
    "    try:\n",
    "        with fits.open(match_file) as hdu_match:\n",
    "            # Check the primary header (HDU 0)\n",
    "            if 'ROLL_NOM' in hdu_match[0].header:\n",
    "                roll_nom = hdu_match[0].header['ROLL_NOM']\n",
    "            # If not, check the EVENTS header (HDU 1)\n",
    "            elif hdu_match[1].header and 'ROLL_NOM' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_NOM']\n",
    "            # Fallback to ROLL_PNT in HDU 1\n",
    "            elif hdu_match[1].header and 'ROLL_PNT' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_PNT']\n",
    "                print(\"  Note: Using 'ROLL_PNT' as 'ROLL_NOM' was not found.\")\n",
    "            else:\n",
    "                print(\"  ERROR: Could not find 'ROLL_NOM' or 'ROLL_PNT' in HDU 0 or 1 of match file.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Match file not found: {match_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read match file header: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the rotation angle (scipy rotates counter-clockwise)\n",
    "    angle_to_rotate = roll_nom - 45.0\n",
    "\n",
    "    # Open the empirical PSF file\n",
    "    try:\n",
    "        with fits.open(psf_file) as hdu_psf:\n",
    "            # Get the main image data and header\n",
    "            if hdu_psf[0].data is None:\n",
    "                # Handle cases where data is in HDU 1\n",
    "                psf_data = hdu_psf[1].data\n",
    "                psf_header = hdu_psf[1].header\n",
    "            else:\n",
    "                psf_data = hdu_psf[0].data\n",
    "                psf_header = hdu_psf[0].header\n",
    "                \n",
    "            if psf_data is None:\n",
    "                print(f\"ERROR: No image data found in HDU 0 or 1 of {psf_file}.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: PSF file not found: {psf_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read PSF file data/header: {e}\")\n",
    "        return\n",
    "\n",
    "    # Rotate the PSF data array\n",
    "    rotated_psf_data = rotate(\n",
    "        psf_data,\n",
    "        angle_to_rotate,\n",
    "        reshape=False,       # Keep the same array shape\n",
    "        cval=0.0,            # Fill new pixels with 0\n",
    "        order=3              # Cubic interpolation\n",
    "    )\n",
    "\n",
    "    # Save the new, rotated data\n",
    "    \n",
    "    # Add a HISTORY card to document the rotation\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    psf_header.add_history(f\"Rotated by {angle_to_rotate:.4f} deg (ROLL_NOM={roll_nom:.4f} - 45.0)\")\n",
    "    psf_header.add_history(f\"Rotation applied by script on {timestamp}\")\n",
    "\n",
    "    # Write the new FITS file\n",
    "    hdu_out = fits.PrimaryHDU(data=rotated_psf_data, header=psf_header)\n",
    "    try:\n",
    "        hdu_out.writeto(outfile, overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not write output file: {e}\")\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def write_pixelscale(file: str, nx: int, ny: int, ra: str, dec: str, hrc_pscale_arcsec: float = 0.13175):\n",
    "    \"\"\"\n",
    "    Adds a WCS header to a FITS file using dmhedit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate center pixel (CRPIX)\n",
    "    x_pix_ctr = (nx / 2.0) + 0.5\n",
    "    y_pix_ctr = (ny / 2.0) + 0.5\n",
    "\n",
    "    # Calculate plate scale in degrees (CDELT)\n",
    "    hrc_pscale_deg = hrc_pscale_arcsec / 3600.\n",
    "    \n",
    "    # This is the 1/4 pixel scale of the empirical PSF\n",
    "    x_platescale = -abs(hrc_pscale_deg / 4.)\n",
    "    y_platescale = abs(hrc_pscale_deg / 4.)\n",
    "    \n",
    "    # Convert RA/Dec to degrees (CRVAL)\n",
    "    ra_deg = ra2deg(ra)\n",
    "    dec_deg = dec2deg(dec)\n",
    "\n",
    "    # Define all WCS keywords to be added\n",
    "    wcs_params = [\n",
    "        # (key, value, datatype, unit)\n",
    "        (\"WCSAXES\", 2, \"short\", None),\n",
    "        (\"CRPIX1\", x_pix_ctr, \"float\", None),\n",
    "        (\"CRPIX2\", y_pix_ctr, \"float\", None),\n",
    "        (\"CDELT1\", x_platescale, \"float\", \"deg\"),\n",
    "        (\"CDELT2\", y_platescale, \"float\", \"deg\"),\n",
    "        (\"CUNIT1\", \"deg\", \"string\", None),\n",
    "        (\"CUNIT2\", \"deg\", \"string\", None),\n",
    "        (\"CTYPE1\", \"RA---TAN\", \"string\", None),\n",
    "        (\"CTYPE2\", \"DEC--TAN\", \"string\", None),\n",
    "        (\"CRVAL1\", ra_deg, \"float\", \"deg\"),\n",
    "        (\"CRVAL2\", dec_deg, \"float\", \"deg\"),\n",
    "        (\"LONPOLE\", 180.0, \"float\", \"deg\"),\n",
    "        (\"LATPOLE\", 0, \"float\", \"deg\"),\n",
    "        (\"RADESYS\", \"ICRS\", \"string\", None),\n",
    "    ]\n",
    "    \n",
    "    # Loop and apply all keywords using dmhedit\n",
    "    try:\n",
    "        for key, value, dtype, unit in wcs_params:\n",
    "            dmhedit(infile=file, op=\"add\", key=key, value=value, datatype=dtype, unit=unit)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: dmhedit failed: {e}\")\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def process_mcmc_results(covar_results, chains, burn_in_frac=0.2, sigma=1):\n",
    "    \"\"\"\n",
    "    Processes raw MCMC chains to calculate median and error bounds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transpose the chains array to (niter, nparams)\n",
    "    chains = chains.T\n",
    "    \n",
    "    # Set percentile for the requested sigma level\n",
    "    if sigma == 1:\n",
    "        # 1-sigma (68.27%)\n",
    "        q_low = 15.865\n",
    "        q_high = 84.135\n",
    "    elif sigma == 2:\n",
    "        # 2-sigma (95.45%)\n",
    "        q_low = 2.275\n",
    "        q_high = 97.725\n",
    "    elif sigma == 3:\n",
    "        # 3-sigma (99.73%)\n",
    "        q_low = 0.135\n",
    "        q_high = 99.865\n",
    "    else:\n",
    "        raise ValueError(\"Only sigma=1, 2, or 3 is supported.\")\n",
    "\n",
    "    # Calculate number of burn-in steps\n",
    "    n_iter, n_params = chains.shape\n",
    "    burn_in = int(n_iter * burn_in_frac)\n",
    "    \n",
    "    # Handle case where burn_in is too high\n",
    "    if burn_in >= n_iter:\n",
    "        print(f\"Warning: burn_in_frac ({burn_in_frac}) is too high, resulting in 0 valid chains.\")\n",
    "        print(f\"         Resetting burn_in to 0 for this run.\")\n",
    "        burn_in = 0\n",
    "        \n",
    "    # Get the valid chains after discarding burn-in\n",
    "    valid_chains = chains[burn_in:, :]\n",
    "    \n",
    "    print(f\"  Processing {valid_chains.shape[0]} valid chain iterations (after {burn_in} burn-in).\")\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    mcmc_results = {\n",
    "        'parnames': [],\n",
    "        'parvals': [],  # Median (50th percentile)\n",
    "        'parmins': [],  # Lower error bound (e.g., 16th percentile)\n",
    "        'parmaxes': []  # Upper error bound (e.g., 84th percentile)\n",
    "    }\n",
    "\n",
    "    # Get thawed parameter names from the covariance results\n",
    "    thawed_parnames = covar_results.parnames\n",
    "\n",
    "    # Check for parameter/chain dimension mismatch\n",
    "    if len(thawed_parnames) != n_params:\n",
    "        print(f\"Warning: Number of thawed params ({len(thawed_parnames)}) does not match chain dims ({n_params}).\")\n",
    "        thawed_parnames = covar_results.parnames[:n_params]\n",
    "\n",
    "    # Iterate over each parameter's chain\n",
    "    for i, parname in enumerate(thawed_parnames):\n",
    "        chain_col = valid_chains[:, i]\n",
    "        \n",
    "        # Calculate quantiles (median, 1-sigma lower, 1-sigma upper)\n",
    "        p_low, p_mid, p_high = np.percentile(chain_col, [q_low, 50, q_high])\n",
    "        \n",
    "        # Store results\n",
    "        mcmc_results['parnames'].append(parname)\n",
    "        mcmc_results['parvals'].append(p_mid)\n",
    "        mcmc_results['parmins'].append(p_low)\n",
    "        mcmc_results['parmaxes'].append(p_high)\n",
    "\n",
    "    # Return the summary dict, valid chains, and param names\n",
    "    return mcmc_results, valid_chains, thawed_parnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cc560-3f3b-4889-b921-cfbb5bf2d322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def src_psf_images(obsid, infile, x0, y0, diameter, wcs_ra, wcs_dec, binsize=0.25, shape='square', psfimg=True, showimg=False, empirical_psf=None):\n",
    "    \"\"\"\n",
    "    Creates and loads source and (optionally) PSF images into Sherpa.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the region string based on shape\n",
    "    if shape.lower() == 'circle':\n",
    "        region_str = f\"circle({x0},{y0},{diameter/2})\"\n",
    "    elif shape.lower() == 'square':\n",
    "        region_str = f\"box({x0},{y0},{diameter},{diameter},0)\"\n",
    "        # Define the cutout region for the 512x512 logical PSF\n",
    "        img_region_str = f\"box(256.5,256.5,{diameter/binsize},{diameter/binsize},0)\"\n",
    "    else:\n",
    "        print('Shape is not circle or square, using user-defined region...')\n",
    "        region_str = shape.lower()\n",
    "\n",
    "    # Get the obsid directory name\n",
    "    obsid = os.path.dirname(os.path.dirname(infile))\n",
    "    \n",
    "    # Define all output filenames\n",
    "    logical_width = diameter/binsize\n",
    "    \n",
    "    imagefile=f'{obsid}/src_image_{shape}_{int(logical_width)}pixel.fits'\n",
    "    psf_imagefile = f'{obsid}/psf_image_{shape}_raytrace_{int(logical_width)}pixel.fits' # Raytraced PSF\n",
    "    psf_rotated = f'{obsid}/psf_rotated.fits'        # Rotated empirical PSF\n",
    "    psf_rotated_cut = f'{obsid}/psf_rotated_cut.fits'  # Cut, rotated PSF\n",
    "    emp_psf_imagefile = f'{obsid}/psf_image_{shape}_empirical_{int(logical_width)}pixel.fits' # Final reprojected PSF\n",
    "    \n",
    "    # Unlearn CIAO tools for a clean run\n",
    "    dmcopy.punlearn()\n",
    "    dmcopy.clobber = 'yes'\n",
    "    reproject_image.punlearn()\n",
    "    reproject_image.clobber = 'yes'\n",
    "\n",
    "    # Process and load the source image\n",
    "    print(f\"Creating source image: {imagefile}\")\n",
    "    dmcopy.infile = f'{infile}[sky={region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "    dmcopy.outfile = imagefile\n",
    "    dmcopy()\n",
    "    \n",
    "    # Load the data image into Sherpa\n",
    "    load_data(imagefile)\n",
    "    if showimg:\n",
    "        image_close()\n",
    "        image_data()\n",
    "    \n",
    "    # Process the PSF image\n",
    "    \n",
    "    # This is the main path: using an empirical PSF\n",
    "    if empirical_psf is not None:\n",
    "        # Rotate the empirical PSF to match the observation's roll angle\n",
    "        rotate_psf_array(psf_file=empirical_psf, match_file=infile, outfile=psf_rotated)\n",
    "        \n",
    "        # Give the newly rotated PSF a WCS header\n",
    "        try:\n",
    "            with fits.open(psf_rotated) as hdu_rot:\n",
    "                nx = hdu_rot[0].header['NAXIS1']\n",
    "                ny = hdu_rot[0].header['NAXIS2']\n",
    "\n",
    "            write_pixelscale(file=psf_rotated, nx=nx, ny=ny, ra=str(wcs_ra), dec=str(wcs_dec))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR during WCS stamping: {e}\")\n",
    "\n",
    "        # Cut out the rotated, WCS-stamped PSF\n",
    "        # Note: it's binned by binsize*4 because the empirical PSF is 4x oversampled\n",
    "        dmcopy.infile = f'{psf_rotated}[{img_region_str}][bin x=::{binsize*4},y=::{binsize*4}]'\n",
    "        dmcopy.outfile = psf_rotated_cut\n",
    "        dmcopy()\n",
    "\n",
    "        # Reproject the cut PSF to match the data image's grid\n",
    "        reproject_image.infile = psf_rotated_cut\n",
    "        reproject_image.matchfile = imagefile      # Match the data image\n",
    "        reproject_image.outfile = emp_psf_imagefile\n",
    "        reproject_image.method = 'sum'           # Conserve flux\n",
    "        reproject_image()\n",
    "\n",
    "        # Load the final reprojected PSF into Sherpa\n",
    "        load_psf(f'centr_psf{obsid}', emp_psf_imagefile)\n",
    "        set_psf(f'centr_psf{obsid}')\n",
    "        print(f\"Loaded empirical PSF: {emp_psf_imagefile}\\n\")\n",
    "        \n",
    "        if showimg:\n",
    "            image_close()\n",
    "            image_psf()\n",
    "\n",
    "    # This path is for a raytraced PSF\n",
    "    elif psfimg:\n",
    "        psf_infile = f'{obsid}/raytrace_projrays.fits'\n",
    "        \n",
    "        dmcopy.infile = f'{psf_infile}[{region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "        dmcopy.outfile = psf_imagefile\n",
    "        dmcopy()\n",
    "\n",
    "        load_psf(f'centr_psf{obsid}', psf_imagefile)\n",
    "        set_psf(f'centr_psf{obsid}')\n",
    "\n",
    "        if showimg:\n",
    "            image_close()\n",
    "            image_psf()\n",
    "    \n",
    "    # This path is for no PSF\n",
    "    else:\n",
    "        print(\"No PSF loaded.\\n\")\n",
    "\n",
    "    return binsize\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def gaussian_image_fit(observation, n_components, position, ampl, fwhm,\n",
    "                       background=0, pos_min=(0, 0), pos_max=None, exptime=None, lock_fwhm=True,\n",
    "                       freeze_components=None, use_mcmc=True, mcmc_iter=5000, mcmc_burn_in_frac=0.2,\n",
    "                       mcmc_scale=1.0, \n",
    "                       prefix=\"g\", confirm=True, imgfit=False):\n",
    "    \"\"\"\n",
    "    Fits multi-component 2D Gaussian models to image data, runs MCMC, \n",
    "    and generates summary plots and text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to expand single-value inputs\n",
    "    def process_numeric_param(param, name):\n",
    "        \"\"\"Expand a single number to a list for all components.\"\"\"\n",
    "        if isinstance(param, (int, float)):\n",
    "            return [param] * n_components\n",
    "        elif isinstance(param, list):\n",
    "            if len(param) != n_components:\n",
    "                raise ValueError(f\"The list of {name} values must have length equal to n_components.\")\n",
    "            return param\n",
    "        else:\n",
    "            raise ValueError(f\"{name} must be either a number or a list of numbers.\")\n",
    "\n",
    "    # Helper function to expand single-tuple inputs\n",
    "    def process_tuple_param(param, name):\n",
    "        \"\"\"Expand a single (x, y) tuple to a list for all components.\"\"\"\n",
    "        if isinstance(param, (tuple, list)) and len(param) == 2 and all(isinstance(x, (int, float)) for x in param):\n",
    "            return [param] * n_components\n",
    "        elif isinstance(param, list):\n",
    "            if len(param) != n_components:\n",
    "                raise ValueError(f\"The list of {name} values must have length equal to n_components.\")\n",
    "            return param\n",
    "        else:\n",
    "            raise ValueError(f\"{name} must be either a tuple (x, y) or a list of such tuples.\")\n",
    "\n",
    "    # Process all input parameters\n",
    "    positions = process_tuple_param(position, \"position\")\n",
    "    ampls = process_numeric_param(ampl, \"ampl\")\n",
    "    fwhms = process_numeric_param(fwhm, \"fwhm\")\n",
    "    pos_mins = process_tuple_param(pos_min, \"pos_min\")\n",
    "    if pos_max is None:\n",
    "        pos_maxs = [None] * n_components\n",
    "    else:\n",
    "        pos_maxs = process_tuple_param(pos_max, \"pos_max\")\n",
    "\n",
    "    # Build the model expression\n",
    "    comp_names = []\n",
    "    gaussian_components = []\n",
    "    model_components = []\n",
    "    for i in range(1, n_components + 1):\n",
    "        comp_name = f\"{prefix}{i}\"\n",
    "        comp_names.append(comp_name)\n",
    "        comp = gauss2d(comp_name)\n",
    "        gaussian_components.append(comp)\n",
    "        model_components.append(comp)\n",
    "    \n",
    "    # Add background component if requested\n",
    "    bkg_comp = None\n",
    "    if background > 0:\n",
    "        bkg_comp = const2d(\"c1\")\n",
    "        model_components.append(bkg_comp)\n",
    "    \n",
    "    # Set the final model in Sherpa\n",
    "    if model_components:\n",
    "        set_source(sum(model_components))\n",
    "    else:\n",
    "        raise ValueError(\"Model expression is empty. Cannot set source.\")\n",
    "\n",
    "    # Assign parameters and constraints for each component\n",
    "    freeze_list = (freeze_components if isinstance(freeze_components, list)\n",
    "                   else ([freeze_components] if freeze_components is not None else []))\n",
    "    for i, comp in enumerate(gaussian_components):\n",
    "        comp_number = i + 1\n",
    "        comp.xpos = positions[i][0]\n",
    "        comp.ypos = positions[i][1]\n",
    "        comp.ampl = ampls[i]\n",
    "        comp.fwhm = fwhms[i]\n",
    "        \n",
    "        # Set parameter limits\n",
    "        if hasattr(comp.xpos, 'min'): comp.xpos.min = pos_mins[i][0]\n",
    "        if hasattr(comp.ypos, 'min'): comp.ypos.min = pos_mins[i][1]\n",
    "        if pos_maxs[i] is not None:\n",
    "            if hasattr(comp.xpos, 'max'): comp.xpos.max = pos_maxs[i][0]\n",
    "            if hasattr(comp.ypos, 'max'): comp.ypos.max = pos_maxs[i][1]\n",
    "        if hasattr(comp.ampl, 'min'): comp.ampl.min = 0\n",
    "        \n",
    "        # Freeze components if requested\n",
    "        if comp_number in freeze_list:\n",
    "            freeze(comp)\n",
    "            print(f\"Froze entire component {comp_number} ({comp.name}) as requested.\")\n",
    "\n",
    "    # Link FWHMs of all components to the first one\n",
    "    central_component = 1\n",
    "    if lock_fwhm:\n",
    "        master = gaussian_components[central_component-1].fwhm\n",
    "        for idx, comp in enumerate(gaussian_components):\n",
    "            if idx != (central_component-1):\n",
    "                link(comp.fwhm, master)\n",
    "\n",
    "    # Set up background component\n",
    "    if bkg_comp is not None:\n",
    "        bkg_comp.c0 = background\n",
    "        if hasattr(bkg_comp.c0, 'min'):\n",
    "            bkg_comp.c0.min = 0\n",
    "\n",
    "    # Confirm model with user\n",
    "    if confirm:\n",
    "        show_model()\n",
    "        proceed = input(\"Proceed with fit? (y/n): \")\n",
    "        if proceed.lower() != \"y\":\n",
    "            print(\"Fit canceled.\")\n",
    "            return None, None, None # Return None for summary, fig, and corner_fig\n",
    "\n",
    "    # Run global 'moncar' fit\n",
    "    print(\"  Running global fit (moncar)...\")\n",
    "    set_stat('cstat')\n",
    "    set_method('moncar')\n",
    "    set_method_opt('numcores', 12)\n",
    "    set_method_opt('population_size', 10 * 16 * (n_components * 3 + 1))\n",
    "    set_method_opt('xprob', 0.5)\n",
    "    set_method_opt('weighting_factor', 0.5)\n",
    "    fit()\n",
    "    \n",
    "    # Run local 'simplex' fit to polish the result\n",
    "    print(\"  Polishing fit (simplex)...\")\n",
    "    set_method('simplex')\n",
    "    fit()\n",
    "    \n",
    "    fit_results = get_fit_results()\n",
    "\n",
    "    # MCMC Error Estimation\n",
    "    mcmc_results = None\n",
    "    corner_fig = None \n",
    "    mcmc_duration_str = \"\"\n",
    "    if use_mcmc:\n",
    "        # Start timer\n",
    "        mcmc_start_time = time.time()\n",
    "        print(f\"-->Running MCMC for error estimation with {mcmc_iter} iterations...\")\n",
    "        \n",
    "        # Thaw parameters for MCMC run\n",
    "        for i, comp in enumerate(gaussian_components):\n",
    "            comp_number = i + 1\n",
    "            if comp_number not in freeze_list:\n",
    "                thaw(comp.ampl)\n",
    "                if not (lock_fwhm and comp_number != central_component):\n",
    "                     thaw(comp.fwhm)\n",
    "                thaw(comp.xpos, comp.ypos)\n",
    "        \n",
    "        # Set the sampler to Metropolis-Hastings\n",
    "        set_sampler('metropolismh')\n",
    "        set_sampler_opt('scale', mcmc_scale)\n",
    "        \n",
    "        # Calculate covariance matrix (required by sampler)\n",
    "        print(\"  Calculating covariance matrix for MCMC proposal...\")\n",
    "        covar()\n",
    "        \n",
    "        # Get covariance results (for parameter names)\n",
    "        covar_results = get_covar_results()\n",
    "        \n",
    "        # Run the MCMC chain\n",
    "        stats, accept, chains = get_draws(niter=mcmc_iter)\n",
    "        print(f\"  MCMC complete. Overall acceptance rate: {np.mean(accept):.3f}\")\n",
    "\n",
    "        # Process the raw chains into a results dictionary\n",
    "        mcmc_results, valid_chains, thawed_parnames = process_mcmc_results(\n",
    "            covar_results,\n",
    "            chains,\n",
    "            burn_in_frac=mcmc_burn_in_frac,\n",
    "            sigma=1\n",
    "        )\n",
    "        \n",
    "        # Generate corner plot\n",
    "        print(f\"  Generating corner plot for prefix '{prefix}'...\")\n",
    "        corner_fig = corner.corner(\n",
    "            valid_chains,\n",
    "            labels=thawed_parnames,\n",
    "            quantiles=[0.16, 0.5, 0.84],\n",
    "            show_titles=True,\n",
    "            title_fmt=\".3f\"\n",
    "        )\n",
    "        corner_fig.suptitle(f\"MCMC Corner Plot - ObsID {observation} ({prefix})\", y=1.02)\n",
    "\n",
    "        # Stop timer and format duration string\n",
    "        mcmc_end_time = time.time()\n",
    "        mcmc_duration = mcmc_end_time - mcmc_start_time\n",
    "        mcmc_duration_min = mcmc_duration / 60.0\n",
    "        print(f\"  MCMC block execution time: {mcmc_duration_min:.2f} minutes\")\n",
    "        mcmc_duration_str = f\"MCMC block execution time = {mcmc_duration_min:.2f} minutes\\n\\n\"\n",
    "\n",
    "    # Optional: Display imgfit in ds9\n",
    "    if imgfit:\n",
    "        print(\"\\nDisplaying image_fit() in ds9...\")\n",
    "        image_fit()\n",
    "        input(\"  `imgfit` is active. Press Enter in this terminal to continue...\")\n",
    "\n",
    "    # Build Fit Result Summary\n",
    "    fit_summary = (\n",
    "        f\"Method = {fit_results.methodname}\\n\"\\\n",
    "        f\"Statistic = {fit_results.statname}\\n\"\\\n",
    "        f\"Initial fit statistic = {fit_results.istatval:.2f}\\n\"\\\n",
    "        f\"Final fit statistic = {fit_results.statval:.2f} at function evaluation {fit_results.nfev}\\n\"\\\n",
    "        f\"Data points = {fit_results.numpoints}\\n\"\\\n",
    "        f\"Degrees of freedom = {fit_results.dof}\\n\"\\\n",
    "        f\"Probability [Q-value] = {fit_results.qval}\\n\"\\\n",
    "        f\"Reduced statistic = {fit_results.rstat:.5f}\\n\"\\\n",
    "        f\"Change in statistic = {fit_results.dstatval:.2f}\\n\\n\"\\\n",
    "    )\n",
    "    \n",
    "    # Helper for formatting parameter values\n",
    "    def fmt_val(val, width=10, prec=3):\n",
    "        if val is None:\n",
    "            return \"------\".rjust(width)\n",
    "        return f\"{val:>{width}.{prec}f}\"\n",
    "        \n",
    "    # Build the parameter table (MCMC results)\n",
    "    if mcmc_results is not None:\n",
    "        param_table_rows = [\n",
    "            f\"MCMC ({mcmc_burn_in_frac*100:.0f}% burn-in) 1-sigma bounds:\",\n",
    "            f\"{'Param':<12} {'Median':>10} {'Lower':>10} {'Upper':>10}\",\n",
    "            f\"{'-'*5:<12} {'-'*8:>10} {'-'*5:>10} {'-'*5:>10}\"\n",
    "        ]\n",
    "        for name, best, low, high in zip(mcmc_results['parnames'], \n",
    "                                         mcmc_results['parvals'], \n",
    "                                         mcmc_results['parmins'], \n",
    "                                         mcmc_results['parmaxes']):\n",
    "            param_table_rows.append(\n",
    "                f\"{name:<12} {fmt_val(best)} {fmt_val(low)} {fmt_val(high)}\"\n",
    "            )\n",
    "        param_table = \"\\n\".join(param_table_rows)\n",
    "    # Build the parameter table (no MCMC)\n",
    "    else:\n",
    "        param_table_rows = [\n",
    "            \"Best-Fit Parameter Values (No MCMC):\",\n",
    "            f\"{'Param':<12} {'Best-Fit':>10}\",\n",
    "            f\"{'-'*5:<12} {'-'*8:>10}\"\n",
    "        ]\n",
    "        for name, best in zip(fit_results.parnames, fit_results.parvals):\n",
    "            param_table_rows.append(f\"{name:<12} {fmt_val(best)}\")\n",
    "        param_table = \"\\n\".join(param_table_rows)\n",
    "            \n",
    "    # Combine all text components\n",
    "    summary_output = fit_summary + mcmc_duration_str + param_table + '\\n'\n",
    "\n",
    "    # Build component count rate block\n",
    "    if exptime and use_mcmc and mcmc_results is not None:\n",
    "        rate_block_rows = [\"Component count rates (counts/s):\"]\n",
    "        parnames = mcmc_results['parnames']\n",
    "        parvals  = mcmc_results['parvals']\n",
    "        parmins  = mcmc_results['parmins']\n",
    "        parmaxes = mcmc_results['parmaxes']\n",
    "        \n",
    "        # Loop over each Gaussian component\n",
    "        for comp in gaussian_components:\n",
    "            comp_img   = get_model_component_image(comp.name)\n",
    "            total_cts  = comp_img.y.sum()\n",
    "            rate       = total_cts / exptime\n",
    "            short      = comp.name.split('.')[-1]\n",
    "            amp_name   = f\"{short}.ampl\"\n",
    "            fwhm_name  = f\"{short}.fwhm\"\n",
    "            \n",
    "            # Get amplitude param values\n",
    "            if amp_name in parnames:\n",
    "                a_idx      = parnames.index(amp_name)\n",
    "                A_best     = parvals[a_idx]\n",
    "                dA_minus_val = parmins[a_idx]\n",
    "                dA_plus_val  = parmaxes[a_idx]\n",
    "                dA_minus = abs(A_best - dA_minus_val) if dA_minus_val is not None else 0\n",
    "                dA_plus  = abs(dA_plus_val - A_best)  if dA_plus_val  is not None else 0\n",
    "            else:\n",
    "                A_best = 1; dA_minus = 0; dA_plus = 0\n",
    "                \n",
    "            # Get FWHM param values\n",
    "            if fwhm_name in parnames:\n",
    "                f_idx      = parnames.index(fwhm_name)\n",
    "                F_best     = parvals[f_idx]\n",
    "                dF_minus_val = parmins[f_idx]\n",
    "                dF_plus_val  = parmaxes[f_idx]\n",
    "                dF_minus = abs(F_best - dF_minus_val) if dF_minus_val is not None else 0\n",
    "                dF_plus  = abs(dF_plus_val - F_best)  if dF_plus_val  is not None else 0\n",
    "            else:\n",
    "                F_best = 1; dF_minus = 0; dF_plus = 0\n",
    "                \n",
    "            # Propagate errors for count rate (CR ~ A * FWHM^2)\n",
    "            frac_minus = np.sqrt((dA_minus/A_best)**2 + (2*dF_minus/F_best)**2) if A_best > 0 and F_best > 0 else 0\n",
    "            frac_plus  = np.sqrt((dA_plus /A_best)**2 + (2*dF_plus /F_best)**2) if A_best > 0 and F_best > 0 else 0\n",
    "            dR_minus = (total_cts * frac_minus) / exptime\n",
    "            dR_plus  = (total_cts * frac_plus)  / exptime\n",
    "            \n",
    "            # Append formatted string\n",
    "            rate_block_rows.append(\n",
    "                f\"  {short:<6}: {rate:7.4f}  \"\\\n",
    "                f\"(-{dR_minus:6.4f}/+{dR_plus:6.4f})\"\\\n",
    "            )\n",
    "        summary_output += \"\\n\" + \"\\n\".join(rate_block_rows) + \"\\n\"\n",
    "    else:\n",
    "        summary_output = fit_summary + param_table + '\\n\\n\\n\\n'\n",
    "\n",
    "    # Get images for plotting\n",
    "    plot_options = [\"data_fit\", \"model\", \"deviance\"]\n",
    "    n_plots = len(plot_options)\n",
    "    fig, axs = plt.subplots(1, n_plots, figsize=(10 * n_plots, 5 * n_plots))\n",
    "    if n_plots == 1:\n",
    "        axs = [axs]\n",
    "    plot_idx = 0\n",
    "    data_img = get_data_image()\n",
    "    data_vals = data_img.y\n",
    "    \n",
    "    # Set a display floor to avoid log(0)\n",
    "    min_positive_val = np.min(data_vals[data_vals > 0]) if np.any(data_vals > 0) else 1e-9\n",
    "    display_floor = min_positive_val / 10.0\n",
    "    data_masked = np.maximum(data_vals, display_floor) \n",
    "    \n",
    "    model_img = get_model_image()\n",
    "    model_vals = model_img.y\n",
    "    model_masked = np.maximum(model_vals, display_floor)\n",
    "    \n",
    "    # Calculate C-stat deviance residuals\n",
    "    d_vals = data_vals\n",
    "    m_vals = model_vals\n",
    "    D = 2.0 * (data_masked * np.log(data_masked / model_masked) - (data_masked - model_masked))\n",
    "    D = np.where(m_vals <= 0, 2.0 * d_vals, D)\n",
    "    D = np.where((m_vals > 0) & (d_vals <= 0), 2.0 * m_vals, D)\n",
    "    resid_dev = np.sign(data_vals - model_vals) * np.sqrt(np.abs(D))\n",
    "    \n",
    "    # Set log normalization for plots\n",
    "    vmax_display = np.max(data_vals)\n",
    "    log_norm = mcolors.LogNorm(\n",
    "        vmin=display_floor, \n",
    "        vmax=vmax_display if vmax_display > display_floor else display_floor + 1\n",
    "    )\n",
    "    \n",
    "    # Plot data and fit contours\n",
    "    if \"data_fit\" in plot_options:\n",
    "        ax = axs[plot_idx]\n",
    "        im = ax.imshow(data_masked, origin='lower', cmap='gnuplot2', norm=log_norm,\n",
    "                       interpolation='nearest')\n",
    "\n",
    "        legend_elements = []\n",
    "        # Use visible colors for contours\n",
    "        base_colors = ['yellow', 'cyan', 'lime', 'xkcd:light lavender']\n",
    "        linestyles = ['--', ':', '-.']\n",
    "        \n",
    "        for i, comp_name in enumerate(comp_names):\n",
    "            comp_vals = get_model_component_image(comp_name).y\n",
    "            \n",
    "            # Skip any all-zero (frozen) components\n",
    "            if not np.any(comp_vals > 0): continue\n",
    "            \n",
    "            color = base_colors[i % len(base_colors)]\n",
    "            linestyle = '--' if i < len(base_colors) else linestyles[(i // len(base_colors)) % len(linestyles)]\n",
    "\n",
    "            # Plot multi-component contours\n",
    "            if n_components > 1:\n",
    "                level = 0.2 * np.max(comp_vals)\n",
    "                ax.contour(comp_vals, levels=[level], colors=[color],\n",
    "                           linestyles=linestyle, linewidths=2)\n",
    "            # Plot single-component contours\n",
    "            else:\n",
    "                levels = np.linspace(np.min(comp_vals), np.max(comp_vals), 6)\n",
    "                if len(np.unique(levels)) > 1:\n",
    "                    ax.contour(comp_vals, levels=levels[1:], colors=[color],\n",
    "                               linestyles=linestyle, linewidths=2)\n",
    "\n",
    "            legend_elements.append(Line2D([0], [0], lw=2, linestyle=linestyle,\n",
    "                                          color=color, label=f\"{comp_name}\"))\n",
    "        if legend_elements:\n",
    "            ax.legend(handles=legend_elements, loc='upper right')\n",
    "        ax.set_title(f\"{observation} Data + Fit Overlay\")\n",
    "        ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "        fig.colorbar(im, ax=ax, label=\"Counts\", shrink=0.53)\n",
    "        plot_idx += 1\n",
    "\n",
    "    # Plot model\n",
    "    if \"model\" in plot_options:\n",
    "        ax = axs[plot_idx]\n",
    "        im = ax.imshow(model_masked, origin='lower', cmap='gnuplot2', norm=log_norm,\n",
    "                       interpolation='nearest')\n",
    "        ax.set_title(\"Model\")\n",
    "        ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "        fig.colorbar(im, ax=ax, label=\"Model Counts\", shrink=0.53)\n",
    "        plot_idx += 1\n",
    "\n",
    "    # Plot deviance\n",
    "    if \"deviance\" in plot_options:\n",
    "        ax = axs[plot_idx]\n",
    "        im = ax.imshow(np.abs(resid_dev), origin='lower', cmap='gnuplot2',\n",
    "                       norm=mcolors.Normalize(vmin=0, vmax=5),\n",
    "                       interpolation='nearest')\n",
    "        ax.set_title(\"Poisson Deviance Residuals\")\n",
    "        ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "        fig.colorbar(im, ax=ax, label=\"|Residuals|\", shrink=0.53)\n",
    "        plot_idx += 1\n",
    "        \n",
    "    # Clean up and close plot\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Return all results\n",
    "    return summary_output, fig, corner_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6a9ef-571f-4bb3-94b7-7abcc06c28a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change to your working directory.\n",
    "os.chdir('/Users/leodrake/Documents/MIT/ss433/HRC_2024/')\n",
    "\n",
    "# We define the specific RA/Dec for each ObsID here.\n",
    "obsid_coords = {\n",
    "    \"26568\": (\"287.9565362\", \"4.9826061\"),\n",
    "    \"26569\": (\"287.9563218\", \"4.9827745\"),\n",
    "    \"26570\": (\"287.9563754\", \"4.9825322\"),\n",
    "    \"26571\": (\"287.9561693\", \"4.9827006\"),\n",
    "    \"26572\": (\"287.9565032\", \"4.9826636\"),# Set the main working directory\n",
    "os.chdir('/Users/leodrake/Documents/MIT/ss433/HRC_2024/')\n",
    "\n",
    "# Define WCS coordinates for each observation\n",
    "obsid_coords = {\n",
    "    \"26568\": (\"287.9565362\", \"4.9826061\"),\n",
    "    \"26569\": (\"287.9563218\", \"4.9827745\"),\n",
    "    \"26570\": (\"287.9563754\", \"4.9825322\"),\n",
    "    \"26571\": (\"287.9561693\", \"4.9827006\"),\n",
    "    \"26572\": (\"287.9565032\", \"4.9826636\"),\n",
    "    \"26573\": (\"287.9565444\", \"4.9826390\"),\n",
    "    \"26574\": (\"287.9562518\", \"4.9825651\"),\n",
    "    \"26575\": (\"287.9566969\", \"4.9828114\"),\n",
    "    \"26576\": (\"287.9566351\", \"4.9826718\"),\n",
    "    \"26577\": (\"287.9565238\", \"4.9826020\"),\n",
    "    \"26578\": (\"287.9566021\", \"4.9826800\"),\n",
    "    \"26579\": (\"287.9565733\", \"4.9825774\")\n",
    "}\n",
    "\n",
    "# Define tuned MCMC scale factors for each observation\n",
    "mcmc_scale_factors = {\n",
    "    \"26568\": 0.4,   # Rate 21.2%\n",
    "    \"26569\": 0.03,  # Rate was 12.6% @ 0.05 trying 0.03\n",
    "    \"26570\": 0.25,  # Rate 21.8%\n",
    "    \"26571\": 0.03,  # Rate was 12.6% @ 0.05 trying 0.03\n",
    "    \"26572\": 0.1,   # Rate 21.1%\n",
    "    \"26573\": 0.25,  # Rate 23.0%\n",
    "    \"26574\": 0.5,   # Rate 22.3%\n",
    "    \"26575\": 0.2,   # Rate 20.2%\n",
    "    \"26576\": 0.3,   # Rate 20.6%\n",
    "    \"26577\": 0.5,   # Rate 25.2%\n",
    "    \"26578\": 0.6,   # Rate 26.4%\n",
    "    \"26579\": 0.4,   # Rate 20.1%\n",
    "}\n",
    "\n",
    "# Define the empirical PSF file to be used\n",
    "emp_psf_file = \"/Users/leodrake/Documents/MIT/ss433/HRC_2024/empPSF_iARLac_v2025_2017-2025.fits\" \n",
    "\n",
    "# Find all event files to be processed\n",
    "event_files = sorted(glob.glob('*/repro/*splinecorr.fits'))\n",
    "\n",
    "# Create lists to hold temporary plot filenames\n",
    "pdf_out_files = []\n",
    "multi_pdf_out_files = []\n",
    "\n",
    "# Define output PDF and text file names\n",
    "pdf_out_filename = '2Dfits/0fit-plots.pdf'\n",
    "multi_pdf_out_filename = '2Dfits/0multi-comp-plots.pdf'\n",
    "results_filename = '2Dfits/0fit-results.txt'\n",
    "multi_results_filename = '2Dfits/0multi-comp-fit-results.txt'\n",
    "\n",
    "# Open the main results file and the multi-component-only results file\n",
    "with open(results_filename, 'w') as results_file, open(multi_results_filename, 'w') as multi_results_file:\n",
    "\n",
    "    # Start main loop over each event file\n",
    "    for infile in event_files[:]:\n",
    "        \n",
    "        # Get ObsID from file path\n",
    "        obsid = os.path.dirname(os.path.dirname(infile))\n",
    "        print(f'\\nProcessing {obsid}\\n')\n",
    "\n",
    "        # Get ObsID-specific coordinates\n",
    "        if obsid not in obsid_coords:\n",
    "            print(f\"!!! WARNING: ObsID {obsid} not in coordinate lookup table. Skipping.\")\n",
    "            continue\n",
    "        current_ra, current_dec = obsid_coords[obsid]\n",
    "        \n",
    "        # Get ObsID-specific MCMC scale factor\n",
    "        current_mcmc_scale = mcmc_scale_factors.get(obsid, 1.0)\n",
    "        print(f\"  Using MCMC scale factor: {current_mcmc_scale}\")\n",
    "        \n",
    "        # Run initial data extraction and centroiding\n",
    "        date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs = data_extract_quickpos_iter(infile)\n",
    "        \n",
    "        # Write the header block to the main results file\n",
    "        header_text = (\n",
    "            f\"Observation: {obsid}\\n\"\\\n",
    "            f\"Infile: {infile}\\n\"\\\n",
    "            f\"Date: {date}, Exptime: {exptime}\\n\"\\\n",
    "        )\n",
    "        results_file.write(header_text)\n",
    "\n",
    "        # Stage 1: Centroid Fit\n",
    "        img_width = 40  # physical pixels\n",
    "        cent_binsize = 1.0 # 1.0 pixel bins\n",
    "        \n",
    "        # Load image for centroiding (no PSF)\n",
    "        src_psf_images(\n",
    "            obsid, infile, pixel_x0_best, pixel_y0_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "            binsize=cent_binsize, \n",
    "            psfimg=False, \n",
    "            empirical_psf=None\n",
    "        )\n",
    "        \n",
    "        # Set logical image dimensions\n",
    "        logical_width = img_width / cent_binsize\n",
    "        img_center = logical_width / 2.0 + 0.5\n",
    "\n",
    "        print('Centroiding...\\n')\n",
    "        \n",
    "        # Call the fit function (1 component, no MCMC)\n",
    "        centroid_fit_summary, centroid_fit_fig, cent_corner_fig = gaussian_image_fit(\n",
    "            obsid, 1, (img_center, img_center), cnt, (1.0 / cent_binsize),\n",
    "            prefix=\"centrg\",\n",
    "            background=0.1, \n",
    "            pos_max=(logical_width, logical_width),\n",
    "            use_mcmc=False,\n",
    "            confirm=False\n",
    "        )\n",
    "        \n",
    "        # Check if user canceled the fit\n",
    "        if centroid_fit_summary is None:\n",
    "            print(f\"Centroid fit for {obsid} canceled. Skipping observation.\")\n",
    "            clean()\n",
    "            continue\n",
    "            \n",
    "        # Save plot to a temporary PNG\n",
    "        temp_cent_fit_png = f\"2Dfits/temp_{obsid}_cent_fit.png\"\n",
    "        centroid_fit_fig.savefig(temp_cent_fit_png)\n",
    "        plt.close(centroid_fit_fig)\n",
    "        pdf_out_files.append(temp_cent_fit_png)\n",
    "\n",
    "        # Write summary to text file\n",
    "        results_file.write(\"\\nCENTROID FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(centroid_fit_summary)\n",
    "\n",
    "        # Get the best-fit physical coordinates from the centroid fit\n",
    "        d = get_data()\n",
    "        crval_x, crval_y = d.sky.crval\n",
    "        crpix_x, crpix_y = d.sky.crpix\n",
    "        cdelt_x, cdelt_y = d.sky.cdelt\n",
    "        xphys_best = crval_x + (centrg1.xpos.val - crpix_x) * cdelt_x\n",
    "        yphys_best = crval_y + (centrg1.ypos.val - crpix_y) * cdelt_y\n",
    "\n",
    "        # Stage 2: Single-Component Source Fit\n",
    "        img_width = 10  # physical pixels\n",
    "        src_binsize = 0.25 # 1/4 pixel bins\n",
    "        \n",
    "        # Load image and empirical PSF\n",
    "        src_psf_images(\n",
    "            obsid, infile, xphys_best, yphys_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "            binsize=src_binsize, \n",
    "            psfimg=True, \n",
    "            empirical_psf=emp_psf_file\n",
    "        )\n",
    "\n",
    "        # Set logical image dimensions\n",
    "        logical_width = img_width / src_binsize \n",
    "        img_center = logical_width / 2.0 + 0.5 \n",
    "        \n",
    "        # Scale initial guesses to the new binning\n",
    "        pixel_scale_guess = 1.0 / src_binsize \n",
    "        scaled_cnt_guess = cnt / (pixel_scale_guess**2)\n",
    "        scaled_fwhm_guess = 1.0 * pixel_scale_guess \n",
    "\n",
    "        print('Fitting Source...\\n')\n",
    "        \n",
    "        # Call the fit function (1 component, no MCMC)\n",
    "        src_fit_summary, src_fit_fig, src_corner_fig = gaussian_image_fit(\n",
    "            obsid, 1, (img_center, img_center), scaled_cnt_guess, scaled_fwhm_guess,\n",
    "            prefix=\"srcg\",\n",
    "            pos_max=(logical_width, logical_width),\n",
    "            use_mcmc=False,\n",
    "            confirm=False\n",
    "        )\n",
    "        \n",
    "        # Check if user canceled the fit\n",
    "        if src_fit_summary is None:\n",
    "            print(f\"Source fit for {obsid} canceled. Skipping observation.\")\n",
    "            clean()\n",
    "            continue\n",
    "\n",
    "        # Save plot to temporary PNG\n",
    "        temp_src_fit_png = f\"2Dfits/temp_{obsid}_src_fit.png\"\n",
    "        src_fit_fig.savefig(temp_src_fit_png)\n",
    "        plt.close(src_fit_fig)\n",
    "        pdf_out_files.append(temp_src_fit_png)\n",
    "\n",
    "        # Write summary to text file\n",
    "        results_file.write(\"SOURCE FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(src_fit_summary)\n",
    "\n",
    "        # Stage 3: Multi-Component Fit\n",
    "        \n",
    "        # Get scaling info from the single-component fit\n",
    "        srcfit_off_x = srcg1.xpos.val - img_center \n",
    "        srcfit_off_y = srcg1.ypos.val - img_center \n",
    "        src_ampl = srcg1.ampl.val\n",
    "        src_fwhm = srcg1.fwhm.val\n",
    "        \n",
    "        # Set up image properties\n",
    "        img_width = 40 \n",
    "        multi_binsize = 0.25\n",
    "        \n",
    "        # Load image and empirical PSF\n",
    "        src_psf_images(\n",
    "            obsid, infile, xphys_best, yphys_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "            binsize=multi_binsize, \n",
    "            empirical_psf=emp_psf_file\n",
    "        )\n",
    "        \n",
    "        # Set logical image dimensions\n",
    "        logical_width = img_width / multi_binsize \n",
    "        img_center = logical_width / 2.0 + 0.5   \n",
    "        \n",
    "        # Calculate pixel scale factor\n",
    "        pixel_scale = src_binsize / multi_binsize \n",
    "        \n",
    "        # Scale parameters for g1 (from src_fit)\n",
    "        new_xpos = img_center + (srcfit_off_x * pixel_scale)\n",
    "        new_ypos = img_center + (srcfit_off_y * pixel_scale)\n",
    "        scaled_src_fwhm = src_fwhm * pixel_scale\n",
    "        scaled_src_ampl = src_ampl / (pixel_scale**2)\n",
    "        \n",
    "        # Scale parameters for g2, g3... (from quickpos cnt)\n",
    "        pixel_scale_guess = 1.0 / multi_binsize \n",
    "        scaled_cnt_ampl = cnt / (pixel_scale_guess**2)\n",
    "        scaled_default_fwhm = 1.0 * pixel_scale_guess \n",
    "\n",
    "        # Set component lists for the fit\n",
    "        n_components = 3  # total number of components (change as needed)\n",
    "        positions = [(new_xpos, new_ypos)] + [(img_center, img_center)] * (n_components - 1)\n",
    "        amplitudes = [scaled_src_ampl] + [scaled_cnt_ampl] * (n_components - 1)\n",
    "        fwhms = [scaled_src_fwhm] + [scaled_default_fwhm] * (n_components - 1)\n",
    "\n",
    "        print('Fitting multi-component Gaussian...')\n",
    "\n",
    "        # Call the fit function (N components, with MCMC)\n",
    "        multi_fit_summary, multi_fit_fig, multi_corner_fig = gaussian_image_fit(\n",
    "            obsid, n_components, positions, amplitudes, fwhms,\n",
    "            prefix=\"g\",\n",
    "            background=0.1,\n",
    "            pos_max=(logical_width, logical_width),\n",
    "            pos_min=(0, 0),\n",
    "            exptime=exptime,\n",
    "            confirm=False,\n",
    "            use_mcmc=True,\n",
    "            mcmc_iter=100000,\n",
    "            mcmc_scale=current_mcmc_scale\n",
    "        )\n",
    "\n",
    "        # Check if user canceled the fit\n",
    "        if multi_fit_summary is None:\n",
    "            print(f\"Multi-component fit for {obsid} canceled. Skipping observation.\")\n",
    "            clean()\n",
    "            continue\n",
    "\n",
    "        print('\\nPlots saving to PDF...\\n')\n",
    "        \n",
    "        # Save fit plot to temp PNG\n",
    "        temp_multi_fit_png = f\"2Dfits/temp_{obsid}_multi_fit.png\"\n",
    "        multi_fit_fig.savefig(temp_multi_fit_png)\n",
    "        plt.close(multi_fit_fig)\n",
    "        pdf_out_files.append(temp_multi_fit_png)\n",
    "        multi_pdf_out_files.append(temp_multi_fit_png)\n",
    "\n",
    "        # Save corner plot to temp PNG\n",
    "        if multi_corner_fig is not None:\n",
    "            temp_multi_corner_png = f\"2Dfits/temp_{obsid}_multi_corner.png\"\n",
    "            multi_corner_fig.savefig(temp_multi_corner_png)\n",
    "            plt.close(multi_corner_fig)\n",
    "            pdf_out_files.append(temp_multi_corner_png)\n",
    "            multi_pdf_out_files.append(temp_multi_corner_png)\n",
    "\n",
    "        # Save multi-component fit parameters\n",
    "        results_file.write(\"MULTI-COMPONENT FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(multi_fit_summary)\n",
    "\n",
    "        # Aggregate text block for writing to separate file\n",
    "        multi_results_text = (\n",
    "            f\"Observation: {obsid}\\n\"\\\n",
    "            f\"Infile: {infile}\\n\"\\\n",
    "            f\"Date: {date}, Exptime: {exptime}\\n\"\\\n",
    "            f\"{multi_fit_summary}\\n\\n\"\\\n",
    "        )\n",
    "        multi_results_file.write(multi_results_text)\n",
    "\n",
    "        # Clear the current Sherpa session to prepare for the next observation\n",
    "        clean()\n",
    "        print('Sherpa Session Cleaned\\n\\n')\n",
    "\n",
    "# Final compilation and cleanup block\n",
    "print('Tidying Up and Compiling PDFs...\\n')\n",
    "\n",
    "# Helper function to compile PNGs into a single PDF\n",
    "def compile_pngs_to_pdf(png_files, pdf_filename):\n",
    "    if not png_files:\n",
    "        print(f\"No images to compile for {pdf_filename}.\")\n",
    "        return\n",
    "    \n",
    "    # Check if the first file exists\n",
    "    if not os.path.exists(png_files[0]):\n",
    "        print(f\"ERROR: Cannot find file {png_files[0]} to start PDF.\")\n",
    "        return\n",
    "\n",
    "    images = []\n",
    "    # Open the first image\n",
    "    img1 = Image.open(png_files[0]).convert('RGB')\n",
    "    \n",
    "    # Open and append all subsequent images\n",
    "    for png_file in png_files[1:]:\n",
    "        if os.path.exists(png_file):\n",
    "            images.append(Image.open(png_file).convert('RGB'))\n",
    "        else:\n",
    "            print(f\"Warning: Missing file {png_file}, skipping.\")\n",
    "    \n",
    "    # Save the final PDF\n",
    "    img1.save(pdf_filename, \"PDF\", resolution=100.0, save_all=True, append_images=images)\n",
    "\n",
    "# Compile the main PDF (all plots)\n",
    "try:\n",
    "    compile_pngs_to_pdf(pdf_out_files, pdf_out_filename)\n",
    "    print(f\"Successfully compiled {pdf_out_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not compile {pdf_out_filename}: {e}\")\n",
    "\n",
    "# Compile the multi-component-only PDF\n",
    "try:\n",
    "    compile_pngs_to_pdf(multi_pdf_out_files, multi_pdf_out_filename)\n",
    "    print(f\"Successfully compiled {multi_pdf_out_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not compile {multi_pdf_out_filename}: {e}\")\n",
    "\n",
    "# Final, separate cleanup step\n",
    "print(\"Cleaning up temporary PNG files...\")\n",
    "temp_files_to_clean = glob.glob(\"2Dfits/temp_*.png\")\n",
    "for f in temp_files_to_clean:\n",
    "    try:\n",
    "        os.remove(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not remove {f}: {e}\")\n",
    "\n",
    "print('Process Complete')\n",
    "    \"26573\": (\"287.9565444\", \"4.9826390\"),\n",
    "    \"26574\": (\"287.9562518\", \"4.9825651\"),\n",
    "    \"26575\": (\"287.9566969\", \"4.9828114\"),\n",
    "    \"26576\": (\"287.9566351\", \"4.9826718\"),\n",
    "    \"26577\": (\"287.9565238\", \"4.9826020\"),\n",
    "    \"26578\": (\"287.9566021\", \"4.9826800\"),\n",
    "    \"26579\": (\"287.9565733\", \"4.9825774\")\n",
    "}\n",
    "\n",
    "# [MODIFIED] Final tuning pass for scale factors\n",
    "mcmc_scale_factors = {\n",
    "    # Perfect (20-27%)\n",
    "    \"26568\": 0.4,   # Rate 21.2%\n",
    "    \"26569\": 0.03,  # Was 12.6% @ 0.05. Try 0.03\n",
    "    \"26570\": 0.25,  # Rate 21.8%\n",
    "    \"26571\": 0.03,  # Was 12.6% @ 0.05. Try 0.03\n",
    "    \"26572\": 0.1,   # Rate 21.1%\n",
    "    \"26573\": 0.25,  # Rate 23.0%\n",
    "    \"26574\": 0.5,   # Rate 22.3%\n",
    "    \"26575\": 0.2,   # Rate 20.2%\n",
    "    \"26576\": 0.3,   # Rate 20.6%\n",
    "    \"26577\": 0.5,   # Rate 25.2%\n",
    "    \"26578\": 0.6,   # Rate 26.4%\n",
    "    \"26579\": 0.4,   # Rate 20.1%\n",
    "}\n",
    "\n",
    "\n",
    "# Define the empirical PSF file to be used.\n",
    "emp_psf_file = \"/Users/leodrake/Documents/MIT/ss433/HRC_2024/empPSF_iARLac_v2025_2017-2025.fits\" \n",
    "\n",
    "# Find all evt2.fits files in subdirectories.\n",
    "event_files = sorted(glob.glob('*/repro/*splinecorr.fits'))\n",
    "\n",
    "# Create lists to hold temporary plot FILENAMES\n",
    "pdf_out_files = []\n",
    "multi_pdf_out_files = []\n",
    "\n",
    "# Define output PDF names\n",
    "pdf_out_filename = '2Dfits/0fit-plots.pdf'\n",
    "multi_pdf_out_filename = '2Dfits/0multi-comp-plots.pdf'\n",
    "\n",
    "# Open the main results file and the multi-component-only results file.\n",
    "results_filename = '2Dfits/0fit-results.txt'\n",
    "multi_results_filename = '2Dfits/0multi-comp-fit-results.txt'\n",
    "with open(results_filename, 'w') as results_file, open(multi_results_filename, 'w') as multi_results_file:\n",
    "\n",
    "    # Loop over each event file.\n",
    "    for infile in event_files[:]:\n",
    "        # Extract observation directory/name.\n",
    "        obsid = os.path.dirname(os.path.dirname(infile))\n",
    "        print(f'\\nProcessing {obsid}\\n')\n",
    "\n",
    "        # Get coordinates for this ObsID\n",
    "        if obsid not in obsid_coords:\n",
    "            print(f\"!!! WARNING: ObsID {obsid} not in coordinate lookup table. Skipping.\")\n",
    "            continue\n",
    "        current_ra, current_dec = obsid_coords[obsid]\n",
    "        \n",
    "        # Get MCMC scale factor for this ObsID (default to 1.0)\n",
    "        current_mcmc_scale = mcmc_scale_factors.get(obsid, 1.0)\n",
    "        print(f\"  Using MCMC scale factor: {current_mcmc_scale}\")\n",
    "        \n",
    "        # Data extraction and initial quickpos\n",
    "        date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs = data_extract_quickpos_iter(infile)\n",
    "        \n",
    "        # Aggregate text block for writing\n",
    "        header_text = (\n",
    "            f\"Observation: {obsid}\\n\"\\\n",
    "            f\"Infile: {infile}\\n\"\\\n",
    "            f\"Date: {date}, Exptime: {exptime}\\n\"\\\n",
    "        )\n",
    "        results_file.write(header_text)\n",
    "\n",
    "        # PSF image and centroid fit\n",
    "        img_width = 40  # physical pixels\n",
    "        \n",
    "        cent_binsize = 1.0 # Define binsize for this step\n",
    "        src_psf_images(\n",
    "            obsid, infile, pixel_x0_best, pixel_y0_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec, # Pass WCS info\n",
    "            binsize=cent_binsize, \n",
    "            psfimg=False, \n",
    "            empirical_psf=None\n",
    "        )\n",
    "        \n",
    "        logical_width = img_width / cent_binsize\n",
    "        img_center = logical_width / 2.0 + 0.5\n",
    "\n",
    "        print('Centroiding...\\n')\n",
    "        \n",
    "        centroid_fit_summary, centroid_fit_fig, cent_corner_fig = gaussian_image_fit(\n",
    "            obsid, 1, (img_center, img_center), cnt, (1.0 / cent_binsize),\n",
    "            prefix=\"centrg\",\n",
    "            background=0.1, \n",
    "            pos_max=(logical_width, logical_width),\n",
    "            use_mcmc=False,\n",
    "            confirm=False\n",
    "        )\n",
    "        \n",
    "        if centroid_fit_summary is None:\n",
    "            print(f\"Centroid fit for {obsid} canceled. Skipping observation.\")\n",
    "            clean()\n",
    "            continue\n",
    "            \n",
    "        temp_cent_fit_png = f\"2Dfits/temp_{obsid}_cent_fit.png\"\n",
    "        centroid_fit_fig.savefig(temp_cent_fit_png)\n",
    "        plt.close(centroid_fit_fig)\n",
    "        pdf_out_files.append(temp_cent_fit_png)\n",
    "\n",
    "        # Save centroid fit parameters\n",
    "        results_file.write(\"\\nCENTROID FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(centroid_fit_summary)\n",
    "\n",
    "        # Retrieve centroid fit physical coordinates\n",
    "        d = get_data()\n",
    "        crval_x, crval_y = d.sky.crval\n",
    "        crpix_x, crpix_y = d.sky.crpix\n",
    "        cdelt_x, cdelt_y = d.sky.cdelt\n",
    "        xphys_best = crval_x + (centrg1.xpos.val - crpix_x) * cdelt_x\n",
    "        yphys_best = crval_y + (centrg1.ypos.val - crpix_y) * cdelt_y\n",
    "\n",
    "        # Source fit in physical coordinates\n",
    "        img_width = 10  # physical pixels\n",
    "        \n",
    "        src_binsize = 0.25 \n",
    "        src_psf_images(\n",
    "            obsid, infile, xphys_best, yphys_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "            binsize=src_binsize, \n",
    "            psfimg=True, \n",
    "            empirical_psf=emp_psf_file\n",
    "        )\n",
    "\n",
    "        logical_width = img_width / src_binsize \n",
    "        img_center = logical_width / 2.0 + 0.5 \n",
    "        \n",
    "        pixel_scale_guess = 1.0 / src_binsize \n",
    "        scaled_cnt_guess = cnt / (pixel_scale_guess**2)\n",
    "        scaled_fwhm_guess = 1.0 * pixel_scale_guess \n",
    "\n",
    "        print('Fitting Source...\\n')\n",
    "        \n",
    "        src_fit_summary, src_fit_fig, src_corner_fig = gaussian_image_fit(\n",
    "            obsid, 1, (img_center, img_center), scaled_cnt_guess, scaled_fwhm_guess,\n",
    "            prefix=\"srcg\",\n",
    "            pos_max=(logical_width, logical_width),\n",
    "            use_mcmc=False,\n",
    "            confirm=False\n",
    "        )\n",
    "        \n",
    "        if src_fit_summary is None:\n",
    "            print(f\"Source fit for {obsid} canceled. Skipping observation.\")\n",
    "            clean()\n",
    "            continue\n",
    "\n",
    "        temp_src_fit_png = f\"2Dfits/temp_{obsid}_src_fit.png\"\n",
    "        src_fit_fig.savefig(temp_src_fit_png)\n",
    "        plt.close(src_fit_fig)\n",
    "        pdf_out_files.append(temp_src_fit_png)\n",
    "\n",
    "        # Save source fit parameters\n",
    "        results_file.write(\"SOURCE FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(src_fit_summary)\n",
    "\n",
    "        # Multi-component fit\n",
    "        \n",
    "        srcfit_off_x = srcg1.xpos.val - img_center \n",
    "        srcfit_off_y = srcg1.ypos.val - img_center \n",
    "        src_ampl = srcg1.ampl.val\n",
    "        src_fwhm = srcg1.fwhm.val\n",
    "        \n",
    "        img_width = 40 \n",
    "        multi_binsize = 0.25\n",
    "        \n",
    "        src_psf_images(\n",
    "            obsid, infile, xphys_best, yphys_best, img_width,\n",
    "            wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "            binsize=multi_binsize, \n",
    "            empirical_psf=emp_psf_file\n",
    "        )\n",
    "        \n",
    "        logical_width = img_width / multi_binsize \n",
    "        img_center = logical_width / 2.0 + 0.5   \n",
    "        \n",
    "        pixel_scale = src_binsize / multi_binsize \n",
    "        \n",
    "        new_xpos = img_center + (srcfit_off_x * pixel_scale)\n",
    "        new_ypos = img_center + (srcfit_off_y * pixel_scale)\n",
    "        scaled_src_fwhm = src_fwhm * pixel_scale\n",
    "        scaled_src_ampl = src_ampl / (pixel_scale**2)\n",
    "        \n",
    "        pixel_scale_guess = 1.0 / multi_binsize \n",
    "        scaled_cnt_ampl = cnt / (pixel_scale_guess**2)\n",
    "        scaled_default_fwhm = 1.0 * pixel_scale_guess \n",
    "\n",
    "        n_components = 3  # total number of components (change as needed)\n",
    "        positions = [(new_xpos, new_ypos)] + [(img_center, img_center)] * (n_components - 1)\n",
    "        amplitudes = [scaled_src_ampl] + [scaled_cnt_ampl] * (n_components - 1)\n",
    "        fwhms = [scaled_src_fwhm] + [scaled_default_fwhm] * (n_components - 1)\n",
    "\n",
    "        print('Fitting multi-component Gaussian...')\n",
    "\n",
    "        multi_fit_summary, multi_fit_fig, multi_corner_fig = gaussian_image_fit(\n",
    "            obsid, n_components, positions, amplitudes, fwhms,\n",
    "            prefix=\"g\",\n",
    "            background=0.1,\n",
    "            pos_max=(logical_width, logical_width),\n",
    "            pos_min=(0, 0),\n",
    "            exptime=exptime,\n",
    "            confirm=False,\n",
    "            use_mcmc=True,\n",
    "            mcmc_iter=100000,\n",
    "            mcmc_scale=current_mcmc_scale\n",
    "        )\n",
    "\n",
    "        if multi_fit_summary is None:\n",
    "            print(f\"Multi-component fit for {obsid} canceled. Skipping observation.\")\n",
    "            clean()\n",
    "            continue\n",
    "\n",
    "        print('\\nPlots saving to PDF...\\n')\n",
    "        \n",
    "        temp_multi_fit_png = f\"2Dfits/temp_{obsid}_multi_fit.png\"\n",
    "        multi_fit_fig.savefig(temp_multi_fit_png)\n",
    "        plt.close(multi_fit_fig)\n",
    "        pdf_out_files.append(temp_multi_fit_png)\n",
    "        multi_pdf_out_files.append(temp_multi_fit_png)\n",
    "\n",
    "\n",
    "        if multi_corner_fig is not None:\n",
    "            temp_multi_corner_png = f\"2Dfits/temp_{obsid}_multi_corner.png\"\n",
    "            multi_corner_fig.savefig(temp_multi_corner_png)\n",
    "            plt.close(multi_corner_fig)\n",
    "            pdf_out_files.append(temp_multi_corner_png)\n",
    "            multi_pdf_out_files.append(temp_multi_corner_png)\n",
    "\n",
    "        # Save multi-component fit parameters\n",
    "        results_file.write(\"MULTI-COMPONENT FIT SUMMARY:\\n\\n\")\n",
    "        results_file.write(multi_fit_summary)\n",
    "\n",
    "        # Aggregate text block for writing\n",
    "        multi_results_text = (\n",
    "            f\"Observation: {obsid}\\n\"\\\n",
    "            f\"Infile: {infile}\\n\"\\\n",
    "            f\"Date: {date}, Exptime: {exptime}\\n\"\\\n",
    "            f\"{multi_fit_summary}\\n\\n\"\\\n",
    "        )\n",
    "        multi_results_file.write(multi_results_text)\n",
    "\n",
    "        # Clear the current Sherpa session to prepare for the next observation\n",
    "        clean()\n",
    "        print('Sherpa Session Cleaned\\n\\n')\n",
    "\n",
    "# [MODIFIED] New PDF compilation and cleanup block\n",
    "print('Tidying Up and Compiling PDFs...\\n')\n",
    "\n",
    "# Helper function to compile PNGs into a PDF\n",
    "def compile_pngs_to_pdf(png_files, pdf_filename):\n",
    "    if not png_files:\n",
    "        print(f\"No images to compile for {pdf_filename}.\")\n",
    "        return\n",
    "    \n",
    "    # Check if the first file exists before trying to open it\n",
    "    if not os.path.exists(png_files[0]):\n",
    "        print(f\"ERROR: Cannot find file {png_files[0]} to start PDF.\")\n",
    "        return\n",
    "\n",
    "    images = []\n",
    "    # Open the first image\n",
    "    img1 = Image.open(png_files[0]).convert('RGB')\n",
    "    \n",
    "    # Open subsequent images\n",
    "    for png_file in png_files[1:]:\n",
    "        if os.path.exists(png_file):\n",
    "            images.append(Image.open(png_file).convert('RGB'))\n",
    "        else:\n",
    "            print(f\"Warning: Missing file {png_file}, skipping.\")\n",
    "    \n",
    "    # Save as PDF\n",
    "    img1.save(pdf_filename, \"PDF\", resolution=100.0, save_all=True, append_images=images)\n",
    "\n",
    "# Compile the main PDF\n",
    "try:\n",
    "    compile_pngs_to_pdf(pdf_out_files, pdf_out_filename)\n",
    "    print(f\"Successfully compiled {pdf_out_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not compile {pdf_out_filename}: {e}\")\n",
    "\n",
    "# Compile the multi-component-only PDF\n",
    "try:\n",
    "    compile_pngs_to_pdf(multi_pdf_out_files, multi_pdf_out_filename)\n",
    "    print(f\"Successfully compiled {multi_pdf_out_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not compile {multi_pdf_out_filename}: {e}\")\n",
    "\n",
    "# [NEW] Final, separate cleanup step\n",
    "print(\"Cleaning up temporary PNG files...\")\n",
    "temp_files_to_clean = glob.glob(\"2Dfits/temp_*.png\")\n",
    "for f in temp_files_to_clean:\n",
    "    try:\n",
    "        os.remove(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not remove {f}: {e}\")\n",
    "\n",
    "print('Process Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8f165-374d-4a72-a070-1d7f57bc2309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CIAO-4.17)",
   "language": "python",
   "name": "ciao-4.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
