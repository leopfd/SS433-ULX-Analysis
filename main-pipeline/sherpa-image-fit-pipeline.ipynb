{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed5217-8efe-4d40-9871-5d5f9337e37f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "# Core scientific and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "from scipy.stats import chi2\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# MCMC, PDF compilation, and Parallelization\n",
    "import corner\n",
    "from PIL import Image\n",
    "from multiprocess import Pool, Manager\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set default Matplotlib styles\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 400,\n",
    "    'font.size': 18,\n",
    "    'image.origin': 'lower',\n",
    "})\n",
    "\n",
    "# Suppress Sherpa info messages\n",
    "logger = logging.getLogger(\"sherpa\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f1ed7-4825-4ea7-a983-721da25a296b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quickpos(x, y, x0, y0, iterations=1, size_list=None, binsize_list=None, doplot=False):\n",
    "    \"\"\"\n",
    "    Iteratively refines the centroid position using 1D histogram fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper function to create step-plot coordinates from histograms\n",
    "    def step_plot(x, y, binwidth):\n",
    "        xsteps = np.ravel(np.column_stack((x - binwidth/2, x + binwidth/2)))\n",
    "        ysteps = np.repeat(y, 2)\n",
    "        return xsteps, ysteps\n",
    "\n",
    "    # Set default window/bin sizes if none are provided\n",
    "    if size_list is None:\n",
    "        size_list = [np.max(x) - np.min(x)] * iterations\n",
    "    if binsize_list is None:\n",
    "        binsize_list = [1.0] * iterations\n",
    "\n",
    "    # Initialize variables\n",
    "    fig_list = []\n",
    "    current_x0, current_y0 = x0, y0\n",
    "    cnt = None\n",
    "    best_x0, best_y0 = current_x0, current_y0 \n",
    "\n",
    "    # Loop for each refinement iteration\n",
    "    for i in range(iterations):\n",
    "        size = size_list[i]\n",
    "        binsize = binsize_list[i]\n",
    "\n",
    "        # Get all points within the current window\n",
    "        ob = np.where((np.abs(x - current_x0) < size) & (np.abs(y - current_y0) < size))\n",
    "        \n",
    "        # Skip if no points are found\n",
    "        if len(ob[0]) == 0:\n",
    "            # print(f\"Warning: No points found in window for iteration {i+1}. Using previous values.\")\n",
    "            continue \n",
    "\n",
    "        # Create 1D histograms for X and Y\n",
    "        xbins = np.arange(current_x0 - size, current_x0 + size + binsize, binsize)\n",
    "        ybins = np.arange(current_y0 - size, current_y0 + size + binsize, binsize)\n",
    "\n",
    "        xhist, xedges = np.histogram(x[ob], bins=xbins)\n",
    "        yhist, yedges = np.histogram(y[ob], bins=ybins)\n",
    "\n",
    "        # Get bin centers\n",
    "        xval = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "        yval = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "\n",
    "        # Define 1D Gaussian model for fitting\n",
    "        def gaussian(x, a, mu, sigma, offset):\n",
    "            return a * np.exp(-((x - mu)**2) / (2 * sigma**2)) + offset\n",
    "\n",
    "        # Fit Gaussian to x histogram\n",
    "        try:\n",
    "            xmax = np.max(xhist)\n",
    "            x0_new = xval[np.argmax(xhist)]\n",
    "            xestpar = [xmax, x0_new, 2 * binsize, 0]\n",
    "            xpar, _ = curve_fit(gaussian, xval, xhist, p0=xestpar)\n",
    "            best_x0 = xpar[1] \n",
    "            xcnt = xpar[0] * xpar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: X-fit failed for iteration {i+1}: {e}. Using previous X value.\")\n",
    "            xcnt = 0\n",
    "\n",
    "        # Fit Gaussian to y histogram\n",
    "        try:\n",
    "            ymax = np.max(yhist)\n",
    "            y0_new = yval[np.argmax(yhist)]\n",
    "            yestpar = [ymax, y0_new, 2 * binsize, 0]\n",
    "            ypar, _ = curve_fit(gaussian, yval, yhist, p0=yestpar)\n",
    "            best_y0 = ypar[1] \n",
    "            ycnt = ypar[0] * ypar[2] * np.sqrt(2 * np.pi)\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: Y-fit failed for iteration {i+1}: {e}. Using previous Y value.\")\n",
    "            ycnt = 0\n",
    "\n",
    "        # Estimate counts and update centroid for next iteration\n",
    "        cnt = 0.5 * (xcnt + ycnt)\n",
    "        current_x0 = best_x0\n",
    "        current_y0 = best_y0\n",
    "\n",
    "    return best_x0, best_y0, cnt, fig_list\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def data_extract_quickpos_iter(infile, iters=3, sizes=[10, 5, 1.5], binsizes=[0.1, 0.1, 0.05]):\n",
    "    \"\"\"\n",
    "    Extracts data from a FITS file and runs quickpos to get an initial centroid.\n",
    "    \"\"\"\n",
    "    # Open the FITS file\n",
    "    with fits.open(infile) as obs:\n",
    "        hdr = obs[1].header\n",
    "        data = obs[1].data\n",
    "        \n",
    "        # Extract essential header info\n",
    "        scale = hdr['tcdlt20']\n",
    "        xc = hdr['tcrpx20']\n",
    "        exptime = hdr['exposure']\n",
    "            \n",
    "        # Form modified julian date for this obs\n",
    "        mjd_start = hdr['mjd-obs']\n",
    "        half_expos = 0.5 * (hdr['tstop']-hdr['tstart'])\n",
    "        date = mjd_start + half_expos / 86400\n",
    "        \n",
    "        # Convert event positions to arcsec\n",
    "        x = (data['x'] - xc) * scale * 3600\n",
    "        y = (data['y'] - xc) * scale * 3600\n",
    "        \n",
    "        # Filter a 20 arcsec radius region\n",
    "        rr = np.sqrt(x**2 + y**2)\n",
    "        ok = np.where(rr < 20)\n",
    "        \n",
    "        # Get a rough starting estimate of the centroid    \n",
    "        x0_est = np.average(x[ok])\n",
    "        y0_est = np.average(y[ok])\n",
    "\n",
    "    # Set iteration parameters\n",
    "    iterations = iters\n",
    "    size_list = sizes\n",
    "    binsize_list = binsizes\n",
    "    \n",
    "    # Run the iterative centroid refinement\n",
    "    x0_best, y0_best, cnt, qp_figs = quickpos(x[ok], y[ok], x0_est, y0_est, iterations, size_list, binsize_list)\n",
    "    \n",
    "    # Convert best-fit arcsec position back to pixels\n",
    "    pixel_x0_best = x0_best / (scale * 3600) + xc\n",
    "    pixel_y0_best = y0_best / (scale * 3600) + xc\n",
    "\n",
    "    return date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def rotate_psf_array(psf_file, match_file, outfile):\n",
    "    \"\"\"\n",
    "    Rotates a PSF image array based on a match file's ROLL_NOM\n",
    "    and saves it with the *original* PSF's header.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the roll angle from the observation's event file\n",
    "    try:\n",
    "        with fits.open(match_file) as hdu_match:\n",
    "            # Check the primary header (HDU 0)\n",
    "            if 'ROLL_NOM' in hdu_match[0].header:\n",
    "                roll_nom = hdu_match[0].header['ROLL_NOM']\n",
    "            # If not, check the EVENTS header (HDU 1)\n",
    "            elif hdu_match[1].header and 'ROLL_NOM' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_NOM']\n",
    "            # Fallback to ROLL_PNT in HDU 1\n",
    "            elif hdu_match[1].header and 'ROLL_PNT' in hdu_match[1].header:\n",
    "                roll_nom = hdu_match[1].header['ROLL_PNT']\n",
    "                # print(\"  Note: Using 'ROLL_PNT' as 'ROLL_NOM' was not found.\")\n",
    "            else:\n",
    "                print(f\"  ERROR: Could not find 'ROLL_NOM' or 'ROLL_PNT' in {match_file}.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Match file not found: {match_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read match file header: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate the rotation angle (scipy rotates counter-clockwise)\n",
    "    angle_to_rotate = roll_nom - 45.0\n",
    "\n",
    "    # Open the empirical PSF file\n",
    "    try:\n",
    "        with fits.open(psf_file) as hdu_psf:\n",
    "            # Get the main image data and header\n",
    "            if hdu_psf[0].data is None:\n",
    "                # Handle cases where data is in HDU 1\n",
    "                psf_data = hdu_psf[1].data\n",
    "                psf_header = hdu_psf[1].header\n",
    "            else:\n",
    "                psf_data = hdu_psf[0].data\n",
    "                psf_header = hdu_psf[0].header\n",
    "                \n",
    "            if psf_data is None:\n",
    "                print(f\"ERROR: No image data found in HDU 0 or 1 of {psf_file}.\")\n",
    "                return\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: PSF file not found: {psf_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not read PSF file data/header: {e}\")\n",
    "        return\n",
    "\n",
    "    # Rotate the PSF data array\n",
    "    rotated_psf_data = rotate(\n",
    "        psf_data,\n",
    "        angle_to_rotate,\n",
    "        reshape=False,       # Keep the same array shape\n",
    "        cval=0.0,            # Fill new pixels with 0\n",
    "        order=3              # Cubic interpolation\n",
    "    )\n",
    "\n",
    "    # Save the new, rotated data\n",
    "    \n",
    "    # Add a HISTORY card to document the rotation\n",
    "    timestamp = datetime.datetime.now().isoformat()\n",
    "    psf_header.add_history(f\"Rotated by {angle_to_rotate:.4f} deg (ROLL_NOM={roll_nom:.4f} - 45.0)\")\n",
    "    psf_header.add_history(f\"Rotation applied by script on {timestamp}\")\n",
    "\n",
    "    # Write the new FITS file\n",
    "    hdu_out = fits.PrimaryHDU(data=rotated_psf_data, header=psf_header)\n",
    "    try:\n",
    "        hdu_out.writeto(outfile, overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Could not write output file: {e}\")\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "def process_mcmc_results(covar_results, chains, burn_in_frac=0.2, sigma=1):\n",
    "    \"\"\"\n",
    "    Processes raw MCMC chains to calculate median and error bounds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transpose the chains array to (niter, nparams)\n",
    "    chains = chains.T\n",
    "    \n",
    "    # Set percentile for the requested sigma level\n",
    "    if sigma == 1:\n",
    "        # 1-sigma (68.27%)\n",
    "        q_low = 15.865\n",
    "        q_high = 84.135\n",
    "    elif sigma == 2:\n",
    "        # 2-sigma (95.45%)\n",
    "        q_low = 2.275\n",
    "        q_high = 97.725\n",
    "    elif sigma == 3:\n",
    "        # 3-sigma (99.73%)\n",
    "        q_low = 0.135\n",
    "        q_high = 99.865\n",
    "    else:\n",
    "        raise ValueError(\"Only sigma=1, 2, or 3 is supported.\")\n",
    "\n",
    "    # Calculate number of burn-in steps\n",
    "    n_iter, n_params = chains.shape\n",
    "    burn_in = int(n_iter * burn_in_frac)\n",
    "    \n",
    "    # Handle case where burn_in is too high\n",
    "    if burn_in >= n_iter:\n",
    "        # print(f\"Warning: burn_in_frac ({burn_in_frac}) is too high, resulting in 0 valid chains.\")\n",
    "        # print(f\"         Resetting burn_in to 0 for this run.\")\n",
    "        burn_in = 0\n",
    "        \n",
    "    # Get the valid chains after discarding burn-in\n",
    "    valid_chains = chains[burn_in:, :]\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    mcmc_results = {\n",
    "        'parnames': [],\n",
    "        'parvals': [],  # Median (50th percentile)\n",
    "        'parmins': [],  # Lower error bound (e.g., 16th percentile)\n",
    "        'parmaxes': []  # Upper error bound (e.g., 84th percentile)\n",
    "    }\n",
    "\n",
    "    # Get thawed parameter names from the covariance results\n",
    "    thawed_parnames = covar_results.parnames\n",
    "\n",
    "    # Check for parameter/chain dimension mismatch\n",
    "    if len(thawed_parnames) != n_params:\n",
    "        # print(f\"Warning: Number of thawed params ({len(thawed_parnames)}) does not match chain dims ({n_params}).\")\n",
    "        thawed_parnames = covar_results.parnames[:n_params]\n",
    "\n",
    "    # Iterate over each parameter's chain\n",
    "    for i, parname in enumerate(thawed_parnames):\n",
    "        chain_col = valid_chains[:, i]\n",
    "        \n",
    "        # Calculate quantiles (median, 1-sigma lower, 1-sigma upper)\n",
    "        p_low, p_mid, p_high = np.percentile(chain_col, [q_low, 50, q_high])\n",
    "        \n",
    "        # Store results\n",
    "        mcmc_results['parnames'].append(parname)\n",
    "        mcmc_results['parvals'].append(p_mid)\n",
    "        mcmc_results['parmins'].append(p_low)\n",
    "        mcmc_results['parmaxes'].append(p_high)\n",
    "\n",
    "    # Return the summary dict, valid chains, and param names\n",
    "    return mcmc_results, valid_chains, thawed_parnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cc560-3f3b-4889-b921-cfbb5bf2d322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_observation(infile, progress_queue, obsid_coords, mcmc_scale_factors, emp_psf_file,\n",
    "                        n_components_multi, run_mcmc_multi, mcmc_iter_multi):\n",
    "    \"\"\"\n",
    "    Worker function to process a single observation.\n",
    "    This function imports its own Sherpa/CIAO instances to be process-safe.\n",
    "    All print statements are silenced for clean parallel execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process-Local Imports\n",
    "    \n",
    "    # Explicitly import all required Sherpa/CIAO functions\n",
    "    from sherpa.astro.ui import (\n",
    "        load_data, image_close, image_data, load_psf, set_psf, image_psf,\n",
    "        gauss2d, const2d, set_source, freeze, link, show_model, set_stat,\n",
    "        set_method, set_method_opt, fit, get_fit_results, thaw, set_sampler,\n",
    "        set_sampler_opt, covar, get_covar_results, get_draws,\n",
    "        get_model_component_image, get_data_image, get_model_image, get_data,\n",
    "        clean\n",
    "    )\n",
    "    # Dynamically created model objects will be accessed via globals()\n",
    "    \n",
    "    from ciao_contrib.runtool import dmcopy, reproject_image, dmhedit\n",
    "    from coords.format import ra2deg, dec2deg\n",
    "    \n",
    "    # Import plotting and math libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib.lines import Line2D\n",
    "    import corner\n",
    "    import numpy as np\n",
    "    from astropy.io import fits\n",
    "    from scipy.stats import chi2\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy.ndimage import rotate\n",
    "    import datetime\n",
    "    import time\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    # Suppress info messages in this worker process\n",
    "    logging.getLogger(\"sherpa\").setLevel(logging.WARNING)\n",
    "    \n",
    "    # Process-Local Helper Functions\n",
    "    \n",
    "    def write_pixelscale(file: str, nx: int, ny: int, ra: str, dec: str, hrc_pscale_arcsec: float = 0.13175):\n",
    "        \"\"\"Adds a WCS header to a FITS file using dmhedit.\"\"\"\n",
    "        x_pix_ctr = (nx / 2.0) + 0.5\n",
    "        y_pix_ctr = (ny / 2.0) + 0.5\n",
    "        hrc_pscale_deg = hrc_pscale_arcsec / 3600.\n",
    "        x_platescale = -abs(hrc_pscale_deg / 4.)\n",
    "        y_platescale = abs(hrc_pscale_deg / 4.)\n",
    "        ra_deg = ra2deg(ra)\n",
    "        dec_deg = dec2deg(dec)\n",
    "        wcs_params = [\n",
    "            (\"WCSAXES\", 2, \"short\", None), (\"CRPIX1\", x_pix_ctr, \"float\", None),\n",
    "            (\"CRPIX2\", y_pix_ctr, \"float\", None), (\"CDELT1\", x_platescale, \"float\", \"deg\"),\n",
    "            (\"CDELT2\", y_platescale, \"float\", \"deg\"), (\"CUNIT1\", \"deg\", \"string\", None),\n",
    "            (\"CUNIT2\", \"deg\", \"string\", None), (\"CTYPE1\", \"RA---TAN\", \"string\", None),\n",
    "            (\"CTYPE2\", \"DEC--TAN\", \"string\", None), (\"CRVAL1\", ra_deg, \"float\", \"deg\"),\n",
    "            (\"CRVAL2\", dec_deg, \"float\", \"deg\"), (\"LONPOLE\", 180.0, \"float\", \"deg\"),\n",
    "            (\"LATPOLE\", 0, \"float\", \"deg\"), (\"RADESYS\", \"ICRS\", \"string\", None),\n",
    "        ]\n",
    "        try:\n",
    "            for key, value, dtype, unit in wcs_params:\n",
    "                dmhedit(infile=file, op=\"add\", key=key, value=value, datatype=dtype, unit=unit)\n",
    "        except Exception as e:\n",
    "            obsid = os.path.basename(os.path.dirname(file))\n",
    "            print(f\"  ERROR (ObsID {obsid}): dmhedit failed: {e}\") # Keep error prints\n",
    "\n",
    "    def src_psf_images(obsid, infile, x0, y0, diameter, wcs_ra, wcs_dec, binsize=0.25, shape='square', psfimg=True, showimg=False, empirical_psf=None):\n",
    "        \"\"\"Creates and loads source and (optionally) PSF images into Sherpa.\"\"\"\n",
    "        if shape.lower() == 'circle':\n",
    "            region_str = f\"circle({x0},{y0},{diameter/2})\"\n",
    "        elif shape.lower() == 'square':\n",
    "            region_str = f\"box({x0},{y0},{diameter},{diameter},0)\"\n",
    "            img_region_str = f\"box(256.5,256.5,{diameter*4},{diameter*4},0)\"\n",
    "        else:\n",
    "            region_str = shape.lower()\n",
    "            \n",
    "        logical_width = diameter/binsize\n",
    "        imagefile=f'{obsid}/src_image_{shape}_{int(logical_width)}pixel.fits'\n",
    "        psf_rotated = f'{obsid}/psf_rotated.fits'\n",
    "        psf_rotated_cut = f'{obsid}/psf_rotated_cut.fits'\n",
    "        emp_psf_imagefile = f'{obsid}/psf_image_{shape}_empirical_{int(logical_width)}pixel.fits'\n",
    "        \n",
    "        dmcopy.punlearn()\n",
    "        dmcopy.clobber = 'yes'\n",
    "        reproject_image.punlearn()\n",
    "        reproject_image.clobber = 'yes'\n",
    "\n",
    "        dmcopy.infile = f'{infile}[sky={region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "        dmcopy.outfile = imagefile\n",
    "        dmcopy()\n",
    "        load_data(imagefile)\n",
    "\n",
    "        if empirical_psf is not None:\n",
    "            # --- Start of in-lined rotate_psf_array ---\n",
    "            try:\n",
    "                with fits.open(infile) as hdu_match:\n",
    "                    if 'ROLL_NOM' in hdu_match[0].header:\n",
    "                        roll_nom = hdu_match[0].header['ROLL_NOM']\n",
    "                    elif hdu_match[1].header and 'ROLL_NOM' in hdu_match[1].header:\n",
    "                        roll_nom = hdu_match[1].header['ROLL_NOM']\n",
    "                    elif hdu_match[1].header and 'ROLL_PNT' in hdu_match[1].header:\n",
    "                        roll_nom = hdu_match[1].header['ROLL_PNT']\n",
    "                    else:\n",
    "                        print(f\"  ERROR: Could not find 'ROLL_NOM' or 'ROLL_PNT' in {infile}\")\n",
    "                        return\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  ERROR: Match file not found: {infile}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not read match file header: {e}\")\n",
    "                return\n",
    "            angle_to_rotate = roll_nom - 45.0\n",
    "            try:\n",
    "                with fits.open(empirical_psf) as hdu_psf:\n",
    "                    if hdu_psf[0].data is None:\n",
    "                        psf_data = hdu_psf[1].data\n",
    "                        psf_header = hdu_psf[1].header\n",
    "                    else:\n",
    "                        psf_data = hdu_psf[0].data\n",
    "                        psf_header = hdu_psf[0].header\n",
    "                    if psf_data is None:\n",
    "                        print(f\"ERROR: No image data found in {empirical_psf}.\")\n",
    "                        return\n",
    "            except FileNotFoundError:\n",
    "                print(f\"  ERROR: PSF file not found: {empirical_psf}\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not read PSF file data/header: {e}\")\n",
    "                return\n",
    "            rotated_psf_data = rotate(\n",
    "                psf_data, angle_to_rotate, reshape=False, cval=0.0, order=3\n",
    "            )\n",
    "            timestamp = datetime.datetime.now().isoformat()\n",
    "            psf_header.add_history(f\"Rotated by {angle_to_rotate:.4f} deg (ROLL_NOM={roll_nom:.4f} - 45.0)\")\n",
    "            psf_header.add_history(f\"Rotation applied by script on {timestamp}\")\n",
    "            hdu_out = fits.PrimaryHDU(data=rotated_psf_data, header=psf_header)\n",
    "            try:\n",
    "                hdu_out.writeto(psf_rotated, overwrite=True)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not write output file: {psf_rotated}\")\n",
    "            # --- End of in-lined rotate_psf_array ---\n",
    "            \n",
    "            try:\n",
    "                with fits.open(psf_rotated) as hdu_rot:\n",
    "                    nx = hdu_rot[0].header['NAXIS1']\n",
    "                    ny = hdu_rot[0].header['NAXIS2']\n",
    "                write_pixelscale(file=psf_rotated, nx=nx, ny=ny, ra=str(wcs_ra), dec=str(wcs_dec))\n",
    "            except Exception as e:\n",
    "                print(f\"!!! ERROR (ObsID {obsid}): WCS stamping failed: {e}\")\n",
    "\n",
    "            dmcopy.infile = f'{psf_rotated}[{img_region_str}][bin x=::{binsize*4},y=::{binsize*4}]'\n",
    "            dmcopy.outfile = psf_rotated_cut\n",
    "            dmcopy()\n",
    "            reproject_image.infile = psf_rotated_cut\n",
    "            reproject_image.matchfile = imagefile\n",
    "            reproject_image.outfile = emp_psf_imagefile\n",
    "            reproject_image.method = 'sum'\n",
    "            reproject_image()\n",
    "            load_psf(f'centr_psf{obsid}', emp_psf_imagefile)\n",
    "            set_psf(f'centr_psf{obsid}')\n",
    "        elif psfimg:\n",
    "            psf_infile = f'{obsid}/raytrace_projrays.fits'\n",
    "            psf_imagefile = f'{obsid}/psf_image_{shape}_raytrace_{int(logical_width)}pixel.fits'\n",
    "            dmcopy.infile = f'{psf_infile}[sky={region_str}][bin x=::{binsize},y=::{binsize}]'\n",
    "            dmcopy.outfile = psf_imagefile\n",
    "            dmcopy()\n",
    "            load_psf(f'centr_psf{obsid}', psf_imagefile)\n",
    "            set_psf(f'centr_psf{obsid}')\n",
    "\n",
    "        return binsize\n",
    "\n",
    "    def gaussian_image_fit(observation, n_components, position, ampl, fwhm,\n",
    "                           background=0, pos_min=(0, 0), pos_max=None, exptime=None, lock_fwhm=True,\n",
    "                           freeze_components=None, use_mcmc=True, mcmc_iter=5000, mcmc_burn_in_frac=0.2,\n",
    "                           mcmc_scale=1.0, \n",
    "                           prefix=\"g\", confirm=True, imgfit=False):\n",
    "        \"\"\"\n",
    "        Fits multi-component 2D Gaussian models, runs MCMC, and generates plots/text.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Helper to expand single-value inputs\n",
    "        def process_numeric_param(param, name):\n",
    "            if isinstance(param, (int, float)):\n",
    "                return [param] * n_components\n",
    "            elif isinstance(param, list):\n",
    "                if len(param) != n_components:\n",
    "                    raise ValueError(f\"List of {name} must have length {n_components}.\")\n",
    "                return param\n",
    "            else:\n",
    "                raise ValueError(f\"{name} must be a number or a list.\")\n",
    "\n",
    "        # Helper to expand single-tuple inputs\n",
    "        def process_tuple_param(param, name):\n",
    "            if isinstance(param, (tuple, list)) and len(param) == 2 and all(isinstance(x, (int, float)) for x in param):\n",
    "                return [param] * n_components\n",
    "            elif isinstance(param, list):\n",
    "                if len(param) != n_components:\n",
    "                    raise ValueError(f\"List of {name} must have length {n_components}.\")\n",
    "                return param\n",
    "            else:\n",
    "                raise ValueError(f\"{name} must be a tuple (x, y) or a list.\")\n",
    "\n",
    "        # Process all input parameters\n",
    "        positions = process_tuple_param(position, \"position\")\n",
    "        ampls = process_numeric_param(ampl, \"ampl\")\n",
    "        fwhms = process_numeric_param(fwhm, \"fwhm\")\n",
    "        pos_mins = process_tuple_param(pos_min, \"pos_min\")\n",
    "        pos_maxs = [None] * n_components if pos_max is None else process_tuple_param(pos_max, \"pos_max\")\n",
    "\n",
    "        # Build the model expression\n",
    "        comp_names = []\n",
    "        gaussian_components = []\n",
    "        model_components = []\n",
    "        for i in range(1, n_components + 1):\n",
    "            comp_name = f\"{prefix}{i}\"\n",
    "            comp_names.append(comp_name)\n",
    "            comp = gauss2d(comp_name)\n",
    "            gaussian_components.append(comp)\n",
    "            model_components.append(comp)\n",
    "        \n",
    "        # Add background component if requested\n",
    "        bkg_comp = None\n",
    "        if background > 0:\n",
    "            bkg_comp = const2d(\"c1\")\n",
    "            model_components.append(bkg_comp)\n",
    "        \n",
    "        # Set the final model in Sherpa\n",
    "        if model_components:\n",
    "            set_source(sum(model_components))\n",
    "        else:\n",
    "            raise ValueError(\"Model expression is empty.\")\n",
    "\n",
    "        # Assign parameters and constraints\n",
    "        freeze_list = (freeze_components if isinstance(freeze_components, list)\n",
    "                       else ([freeze_components] if freeze_components is not None else []))\n",
    "        for i, comp in enumerate(gaussian_components):\n",
    "            comp_number = i + 1\n",
    "            comp.xpos = positions[i][0]\n",
    "            comp.ypos = positions[i][1]\n",
    "            comp.ampl = ampls[i]\n",
    "            comp.fwhm = fwhms[i]\n",
    "            if hasattr(comp.xpos, 'min'): comp.xpos.min = pos_mins[i][0]\n",
    "            if hasattr(comp.ypos, 'min'): comp.ypos.min = pos_mins[i][1]\n",
    "            if pos_maxs[i] is not None:\n",
    "                if hasattr(comp.xpos, 'max'): comp.xpos.max = pos_maxs[i][0]\n",
    "                if hasattr(comp.ypos, 'max'): comp.ypos.max = pos_maxs[i][1]\n",
    "            if hasattr(comp.ampl, 'min'): comp.ampl.min = 0\n",
    "            if comp_number in freeze_list:\n",
    "                freeze(comp)\n",
    "\n",
    "        # Link FWHMs\n",
    "        central_component = 1\n",
    "        if lock_fwhm:\n",
    "            master = gaussian_components[central_component-1].fwhm\n",
    "            for idx, comp in enumerate(gaussian_components):\n",
    "                if idx != (central_component-1):\n",
    "                    link(comp.fwhm, master)\n",
    "\n",
    "        # Setup background\n",
    "        if bkg_comp is not None:\n",
    "            bkg_comp.c0 = background\n",
    "            if hasattr(bkg_comp.c0, 'min'):\n",
    "                bkg_comp.c0.min = 0\n",
    "\n",
    "        # Confirm model with user\n",
    "        if confirm:\n",
    "            show_model()\n",
    "            proceed = input(f\"  (ObsID {observation}) Proceed with fit? (y/n): \")\n",
    "            if proceed.lower() != \"y\":\n",
    "                # print(f\"  (ObsID {observation}) Fit canceled.\")\n",
    "                return None, None, None\n",
    "\n",
    "        # Run global 'moncar' fit\n",
    "        set_stat('cstat')\n",
    "        set_method('moncar')\n",
    "        set_method_opt('numcores', 1) # Must be 1 in parallel\n",
    "        set_method_opt('population_size', 10 * 16 * (n_components * 3 + 1))\n",
    "        set_method_opt('xprob', 0.5)\n",
    "        set_method_opt('weighting_factor', 0.5)\n",
    "        fit()\n",
    "        \n",
    "        # Run local 'simplex' fit to polish\n",
    "        set_method('simplex')\n",
    "        fit()\n",
    "        fit_results = get_fit_results()\n",
    "\n",
    "        # MCMC Error Estimation\n",
    "        mcmc_results = None\n",
    "        corner_fig = None \n",
    "        mcmc_duration_str = \"\"\n",
    "        if use_mcmc:\n",
    "            mcmc_start_time = time.time()\n",
    "            \n",
    "            # Thaw parameters for MCMC run\n",
    "            for i, comp in enumerate(gaussian_components):\n",
    "                comp_number = i + 1\n",
    "                if comp_number not in freeze_list:\n",
    "                    thaw(comp.ampl)\n",
    "                    if not (lock_fwhm and comp_number != central_component):\n",
    "                         thaw(comp.fwhm)\n",
    "                    thaw(comp.xpos, comp.ypos)\n",
    "            \n",
    "            # Set sampler and options\n",
    "            set_sampler('metropolismh')\n",
    "            set_sampler_opt('scale', mcmc_scale)\n",
    "            covar()\n",
    "            covar_results = get_covar_results()\n",
    "            \n",
    "            # Run MCMC with error handling to identify failing observations\n",
    "            try:\n",
    "                stats, accept, chains = get_draws(niter=mcmc_iter)\n",
    "                \n",
    "                # Process MCMC results\n",
    "                chains_T = chains.T\n",
    "                q_low = 15.865\n",
    "                q_high = 84.135\n",
    "                n_iter, n_params = chains_T.shape\n",
    "                bif = mcmc_burn_in_frac \n",
    "                burn_in = int(n_iter * bif)\n",
    "                if burn_in >= n_iter:\n",
    "                    burn_in = 0\n",
    "                valid_chains = chains_T[burn_in:, :]\n",
    "                mcmc_results_data = {'parnames': [], 'parvals': [], 'parmins': [], 'parmaxes': []}\n",
    "                thawed_parnames = covar_results.parnames\n",
    "                if len(thawed_parnames) != n_params:\n",
    "                    thawed_parnames = covar_results.parnames[:n_params]\n",
    "                for i, parname in enumerate(thawed_parnames):\n",
    "                    chain_col = valid_chains[:, i]\n",
    "                    p_low, p_mid, p_high = np.percentile(chain_col, [q_low, 50, q_high])\n",
    "                    mcmc_results_data['parnames'].append(parname)\n",
    "                    mcmc_results_data['parvals'].append(p_mid)\n",
    "                    mcmc_results_data['parmins'].append(p_low)\n",
    "                    mcmc_results_data['parmaxes'].append(p_high)\n",
    "                mcmc_results = mcmc_results_data\n",
    "                \n",
    "                # Generate corner plot\n",
    "                corner_fig = corner.corner(\n",
    "                    valid_chains, labels=thawed_parnames, quantiles=[0.16, 0.5, 0.84],\n",
    "                    show_titles=True, title_fmt=\".3f\"\n",
    "                )\n",
    "                corner_fig.suptitle(f\"MCMC Corner Plot - ObsID {observation} ({prefix})\", y=1.02)\n",
    "\n",
    "                # Record MCMC duration and acceptance rate\n",
    "                mcmc_end_time = time.time()\n",
    "                mcmc_duration = mcmc_end_time - mcmc_start_time\n",
    "                mcmc_duration_min = mcmc_duration / 60.0\n",
    "                mcmc_duration_str = (f\"MCMC block execution time = {mcmc_duration_min:.2f} minutes\\n\"\n",
    "                                     f\"MCMC acceptance rate = {np.mean(accept):.3f}\\n\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: MCMC failed for ObsID {observation}: {e}\")\n",
    "                mcmc_results = None\n",
    "                mcmc_duration_str = f\"MCMC FAILED: {e}\\n\\n\"\n",
    "\n",
    "        # Build Fit Result Summary\n",
    "        fit_summary = (\n",
    "            f\"Method = {fit_results.methodname}\\n\"\\\n",
    "            f\"Statistic = {fit_results.statname}\\n\"\\\n",
    "            f\"Initial fit statistic = {fit_results.istatval:.2f}\\n\"\\\n",
    "            f\"Final fit statistic = {fit_results.statval:.2f} at {fit_results.nfev} evals\\n\"\\\n",
    "            f\"Data points = {fit_results.numpoints}\\n\"\\\n",
    "            f\"Degrees of freedom = {fit_results.dof}\\n\"\\\n",
    "            f\"Probability [Q-value] = {fit_results.qval}\\n\"\\\n",
    "            f\"Reduced statistic = {fit_results.rstat:.5f}\\n\"\\\n",
    "            f\"Change in statistic = {fit_results.dstatval:.2f}\\n\\n\"\\\n",
    "        )\n",
    "        \n",
    "        # Helper for formatting parameter values\n",
    "        def fmt_val(val, width=10, prec=3):\n",
    "            if val is None: return \"------\".rjust(width)\n",
    "            return f\"{val:>{width}.{prec}f}\"\n",
    "            \n",
    "        # Build the parameter table\n",
    "        if mcmc_results is not None:\n",
    "            param_table_rows = [\n",
    "                f\"MCMC ({mcmc_burn_in_frac*100:.0f}% burn-in) 1-sigma bounds:\",\n",
    "                f\"{'Param':<12} {'Median':>10} {'Lower':>10} {'Upper':>10}\",\n",
    "                f\"{'-'*5:<12} {'-'*8:>10} {'-'*5:>10} {'-'*5:>10}\"\n",
    "            ]\n",
    "            for name, best, low, high in zip(mcmc_results['parnames'], \n",
    "                                             mcmc_results['parvals'], \n",
    "                                             mcmc_results['parmins'], \n",
    "                                             mcmc_results['parmaxes']):\n",
    "                param_table_rows.append(\n",
    "                    f\"{name:<12} {fmt_val(best)} {fmt_val(low)} {fmt_val(high)}\"\n",
    "                )\n",
    "            param_table = \"\\n\".join(param_table_rows)\n",
    "        else:\n",
    "            param_table_rows = [\n",
    "                \"Best-Fit Parameter Values (No MCMC):\",\n",
    "                f\"{'Param':<12} {'Best-Fit':>10}\",\n",
    "                f\"{'-'*5:<12} {'-'*8:>10}\"\n",
    "            ]\n",
    "            for name, best in zip(fit_results.parnames, fit_results.parvals):\n",
    "                param_table_rows.append(f\"{name:<12} {fmt_val(best)}\")\n",
    "            param_table = \"\\n\".join(param_table_rows)\n",
    "                \n",
    "        # Combine all text components\n",
    "        summary_output = fit_summary + mcmc_duration_str + param_table + '\\n'\n",
    "\n",
    "        # Build component count rate block\n",
    "        if exptime and use_mcmc and mcmc_results is not None:\n",
    "            rate_block_rows = [\"Component count rates (counts/s):\"]\n",
    "            parnames = mcmc_results['parnames']\n",
    "            parvals  = mcmc_results['parvals']\n",
    "            parmins  = mcmc_results['parmins']\n",
    "            parmaxes = mcmc_results['parmaxes']\n",
    "            \n",
    "            for comp in gaussian_components:\n",
    "                comp_img   = get_model_component_image(comp.name)\n",
    "                total_cts  = comp_img.y.sum()\n",
    "                rate       = total_cts / exptime\n",
    "                short      = comp.name.split('.')[-1]\n",
    "                amp_name   = f\"{short}.ampl\"\n",
    "                fwhm_name  = f\"{short}.fwhm\"\n",
    "                \n",
    "                if amp_name in parnames:\n",
    "                    a_idx      = parnames.index(amp_name)\n",
    "                    A_best     = parvals[a_idx]\n",
    "                    dA_minus_val = parmins[a_idx]\n",
    "                    dA_plus_val  = parmaxes[a_idx]\n",
    "                    dA_minus = abs(A_best - dA_minus_val) if dA_minus_val is not None else 0\n",
    "                    dA_plus  = abs(dA_plus_val - A_best)  if dA_plus_val  is not None else 0\n",
    "                else:\n",
    "                    A_best = 1; dA_minus = 0; dA_plus = 0\n",
    "                    \n",
    "                if fwhm_name in parnames:\n",
    "                    f_idx      = parnames.index(fwhm_name)\n",
    "                    F_best     = parvals[f_idx]\n",
    "                    dF_minus_val = parmins[f_idx]\n",
    "                    dF_plus_val  = parmaxes[f_idx]\n",
    "                    dF_minus = abs(F_best - dF_minus_val) if dF_minus_val is not None else 0\n",
    "                    dF_plus  = abs(dF_plus_val - F_best)  if dF_plus_val  is not None else 0\n",
    "                else:\n",
    "                    F_best = 1; dF_minus = 0; dF_plus = 0\n",
    "                    \n",
    "                frac_minus = np.sqrt((dA_minus/A_best)**2 + (2*dF_minus/F_best)**2) if A_best > 0 and F_best > 0 else 0\n",
    "                frac_plus  = np.sqrt((dA_plus /A_best)**2 + (2*dF_plus /F_best)**2) if A_best > 0 and F_best > 0 else 0\n",
    "                dR_minus = (total_cts * frac_minus) / exptime\n",
    "                dR_plus  = (total_cts * frac_plus)  / exptime\n",
    "                \n",
    "                rate_block_rows.append(\n",
    "                    f\"  {short:<6}: {rate:7.4f}  \"\\\n",
    "                    f\"(-{dR_minus:6.4f}/+{dR_plus:6.4f})\"\\\n",
    "                )\n",
    "            summary_output += \"\\n\" + \"\\n\".join(rate_block_rows) + \"\\n\"\n",
    "        else:\n",
    "            summary_output = fit_summary + param_table + '\\n\\n\\n\\n'\n",
    "\n",
    "        # Get images for plotting\n",
    "        plot_options = [\"data_fit\", \"model\", \"deviance\"]\n",
    "        n_plots = len(plot_options)\n",
    "        fig, axs = plt.subplots(1, n_plots, figsize=(10 * n_plots, 5 * n_plots))\n",
    "        if n_plots == 1: axs = [axs]\n",
    "        \n",
    "        plot_idx = 0\n",
    "        data_img = get_data_image()\n",
    "        data_vals = data_img.y\n",
    "        min_positive_val = np.min(data_vals[data_vals > 0]) if np.any(data_vals > 0) else 1e-9\n",
    "        display_floor = min_positive_val / 10.0\n",
    "        data_masked = np.maximum(data_vals, display_floor) \n",
    "        model_img = get_model_image()\n",
    "        model_vals = model_img.y\n",
    "        model_masked = np.maximum(model_vals, display_floor)\n",
    "        \n",
    "        d_vals = data_vals\n",
    "        m_vals = model_vals\n",
    "        D = 2.0 * (data_masked * np.log(data_masked / model_masked)) - (data_masked - model_masked)\n",
    "        D = np.where(m_vals <= 0, 2.0 * d_vals, D)\n",
    "        D = np.where((m_vals > 0) & (d_vals <= 0), 2.0 * m_vals, D)\n",
    "        resid_dev = np.sign(data_vals - model_vals) * np.sqrt(np.abs(D))\n",
    "        \n",
    "        vmax_display = np.max(data_vals)\n",
    "        log_norm = mcolors.LogNorm(\n",
    "            vmin=display_floor, \n",
    "            vmax=vmax_display if vmax_display > display_floor else display_floor + 1\n",
    "        )\n",
    "        \n",
    "        if \"data_fit\" in plot_options:\n",
    "            ax = axs[plot_idx]\n",
    "            im = ax.imshow(data_masked, origin='lower', cmap='gnuplot2', norm=log_norm,\n",
    "                           interpolation='nearest')\n",
    "            legend_elements = []\n",
    "            base_colors = ['white', 'cyan', 'lime', 'xkcd:light lavender']\n",
    "            linestyles = ['--', ':', '-.']\n",
    "            \n",
    "            for i, comp_name in enumerate(comp_names):\n",
    "                comp_vals = get_model_component_image(comp_name).y\n",
    "                if not np.any(comp_vals > 0): continue\n",
    "                color = base_colors[i % len(base_colors)]\n",
    "                linestyle = '--' if i < len(base_colors) else linestyles[(i // len(base_colors)) % len(linestyles)]\n",
    "                if n_components > 1:\n",
    "                    level = 0.2 * np.max(comp_vals)\n",
    "                    ax.contour(comp_vals, levels=[level], colors=[color],\n",
    "                               linestyles=linestyle, linewidths=2)\n",
    "                else:\n",
    "                    levels = np.linspace(np.min(comp_vals), np.max(comp_vals), 6)\n",
    "                    if len(np.unique(levels)) > 1:\n",
    "                        ax.contour(comp_vals, levels=levels[1:], colors=[color],\n",
    "                                   linestyles=linestyle, linewidths=2)\n",
    "                legend_elements.append(Line2D([0], [0], lw=2, linestyle=linestyle,\n",
    "                                              color=color, label=f\"{comp_name}\"))\n",
    "            if legend_elements:\n",
    "                ax.legend(handles=legend_elements, loc='upper right')\n",
    "            ax.set_title(f\"{observation} Data + Fit Overlay\")\n",
    "            ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "            fig.colorbar(im, ax=ax, label=\"Counts\", shrink=0.53)\n",
    "            plot_idx += 1\n",
    "\n",
    "        if \"model\" in plot_options:\n",
    "            ax = axs[plot_idx]\n",
    "            im = ax.imshow(model_masked, origin='lower', cmap='gnuplot2', norm=log_norm,\n",
    "                           interpolation='nearest')\n",
    "            ax.set_title(\"Model\")\n",
    "            ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "            fig.colorbar(im, ax=ax, label=\"Model Counts\", shrink=0.53)\n",
    "            plot_idx += 1\n",
    "\n",
    "        if \"deviance\" in plot_options:\n",
    "            ax = axs[plot_idx]\n",
    "            im = ax.imshow(np.abs(resid_dev), origin='lower', cmap='gnuplot2',\n",
    "                           norm=mcolors.Normalize(vmin=0, vmax=5),\n",
    "                           interpolation='nearest')\n",
    "            ax.set_title(\"Poisson Deviance Residuals\")\n",
    "            ax.set_xlabel(\"X Pixel\"); ax.set_ylabel(\"Y Pixel\")\n",
    "            fig.colorbar(im, ax=ax, label=\"|Residuals|\", shrink=0.53)\n",
    "            plot_idx += 1\n",
    "            \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # Return figs, do not close them\n",
    "        return summary_output, fig, corner_fig\n",
    "\n",
    "    # Main Worker Logic\n",
    "    # Initialize lists to store temp file paths\n",
    "    pdf_out_files = []\n",
    "    multi_pdf_out_files = []\n",
    "    \n",
    "    # Extract observation directory/name\n",
    "    obsid = os.path.dirname(os.path.dirname(infile))\n",
    "\n",
    "    # Get ObsID-specific coordinates\n",
    "    if obsid not in obsid_coords:\n",
    "        print(f\"!!! WARNING: ObsID {obsid} not in coordinate lookup table. Skipping.\")\n",
    "        return (obsid, \"\", \"\", \"\", \"\", \"\", [], []) # Return empty results\n",
    "    current_ra, current_dec = obsid_coords[obsid]\n",
    "    \n",
    "    # Get ObsID-specific MCMC scale factor\n",
    "    current_mcmc_scale = mcmc_scale_factors.get(obsid, 1.0)\n",
    "    \n",
    "    # Run initial data extraction and centroiding\n",
    "    date, exptime, pixel_x0_best, pixel_y0_best, cnt, qp_figs = data_extract_quickpos_iter(infile)\n",
    "    \n",
    "    # Store the header text\n",
    "    header_text = (\n",
    "        f\"Observation: {obsid}\\n\"\\\n",
    "        f\"Infile: {infile}\\n\"\\\n",
    "        f\"Date: {date}, Exptime: {exptime}\\n\"\\\n",
    "    )\n",
    "\n",
    "    # Stage 1: Centroid Fit\n",
    "    img_width = 40\n",
    "    cent_binsize = 1.0\n",
    "    src_psf_images(\n",
    "        obsid, infile, pixel_x0_best, pixel_y0_best, img_width,\n",
    "        wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "        binsize=cent_binsize, \n",
    "        psfimg=False, \n",
    "        empirical_psf=None\n",
    "    )\n",
    "    logical_width = img_width / cent_binsize\n",
    "    img_center = logical_width / 2.0 + 0.5\n",
    "    \n",
    "    centroid_fit_summary, centroid_fit_fig, cent_corner_fig = gaussian_image_fit(\n",
    "        obsid, 1, (img_center, img_center), cnt, (1.0 / cent_binsize),\n",
    "        prefix=\"centrg\", background=0.1, pos_max=(logical_width, logical_width),\n",
    "        use_mcmc=False, confirm=False\n",
    "    )\n",
    "    \n",
    "    if centroid_fit_summary is None:\n",
    "        print(f\"Centroid fit for {obsid} canceled. Skipping.\")\n",
    "        clean()\n",
    "        return (obsid, \"\", \"\", \"\", \"\", \"\", [], [])\n",
    "\n",
    "    # Save plot to a temporary PNG\n",
    "    temp_cent_fit_png = f\"2Dfits/temp_{obsid}_cent_fit.png\"\n",
    "    centroid_fit_fig.savefig(temp_cent_fit_png)\n",
    "    plt.close(centroid_fit_fig)\n",
    "    pdf_out_files.append(temp_cent_fit_png)\n",
    "    progress_queue.put(1) # <-- CHECKPOINT 1\n",
    "\n",
    "    # Get best-fit physical coordinates\n",
    "    d = get_data()\n",
    "    crval_x, crval_y = d.sky.crval\n",
    "    crpix_x, crpix_y = d.sky.crpix\n",
    "    cdelt_x, cdelt_y = d.sky.cdelt\n",
    "    \n",
    "    # Access the dynamically created model object\n",
    "    xphys_best = crval_x + (globals()['centrg1'].xpos.val - crpix_x) * cdelt_x\n",
    "    yphys_best = crval_y + (globals()['centrg1'].ypos.val - crpix_y) * cdelt_y\n",
    "\n",
    "    # Stage 2: Single-Component Source Fit\n",
    "    img_width = 10\n",
    "    src_binsize = 0.25\n",
    "    src_psf_images(\n",
    "        obsid, infile, xphys_best, yphys_best, img_width,\n",
    "        wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "        binsize=src_binsize, \n",
    "        psfimg=True, \n",
    "        empirical_psf=emp_psf_file\n",
    "    )\n",
    "    logical_width = img_width / src_binsize \n",
    "    img_center = logical_width / 2.0 + 0.5 \n",
    "    pixel_scale_guess = 1.0 / src_binsize \n",
    "    scaled_cnt_guess = cnt / (pixel_scale_guess**2)\n",
    "    scaled_fwhm_guess = 1.0 * pixel_scale_guess \n",
    "    \n",
    "    src_fit_summary, src_fit_fig, src_corner_fig = gaussian_image_fit(\n",
    "        obsid, 1, (img_center, img_center), scaled_cnt_guess, scaled_fwhm_guess,\n",
    "        prefix=\"srcg\", pos_max=(logical_width, logical_width),\n",
    "        use_mcmc=False, confirm=False\n",
    "    )\n",
    "    \n",
    "    if src_fit_summary is None:\n",
    "        print(f\"Source fit for {obsid} canceled. Skipping.\")\n",
    "        clean()\n",
    "        return (obsid, header_text, centroid_fit_summary, \"\", \"\", \"\", pdf_out_files, [])\n",
    "\n",
    "    # Save plot to temporary PNG\n",
    "    temp_src_fit_png = f\"2Dfits/temp_{obsid}_src_fit.png\"\n",
    "    src_fit_fig.savefig(temp_src_fit_png)\n",
    "    plt.close(src_fit_fig)\n",
    "    pdf_out_files.append(temp_src_fit_png)\n",
    "    progress_queue.put(1) # <-- CHECKPOINT 2\n",
    "\n",
    "    # Stage 3: Multi-Component Fit\n",
    "    # Access the dynamically created model object\n",
    "    srcfit_off_x = globals()['srcg1'].xpos.val - img_center \n",
    "    srcfit_off_y = globals()['srcg1'].ypos.val - img_center \n",
    "    src_ampl = globals()['srcg1'].ampl.val\n",
    "    src_fwhm = globals()['srcg1'].fwhm.val\n",
    "    \n",
    "    # This is your desired multi-fit setup\n",
    "    img_width = 40 \n",
    "    multi_binsize = 0.25\n",
    "    \n",
    "    src_psf_images(\n",
    "        obsid, infile, xphys_best, yphys_best, img_width,\n",
    "        wcs_ra=current_ra, wcs_dec=current_dec,\n",
    "        binsize=multi_binsize, \n",
    "        empirical_psf=emp_psf_file,\n",
    "    )\n",
    "    \n",
    "    logical_width = img_width / multi_binsize \n",
    "    img_center = logical_width / 2.0 + 0.5   \n",
    "    pixel_scale = src_binsize / multi_binsize \n",
    "    \n",
    "    new_xpos = img_center + (srcfit_off_x * pixel_scale)\n",
    "    new_ypos = img_center + (srcfit_off_y * pixel_scale)\n",
    "    scaled_src_fwhm = src_fwhm * pixel_scale\n",
    "    scaled_src_ampl = src_ampl / (pixel_scale**2)\n",
    "    \n",
    "    pixel_scale_guess = 1.0 / multi_binsize \n",
    "    scaled_cnt_ampl = cnt / (pixel_scale_guess**2)\n",
    "    scaled_default_fwhm = 1.0 * pixel_scale_guess \n",
    "\n",
    "    # This is your desired component setup\n",
    "    n_components = n_components_multi # Use config variable\n",
    "    positions = [(new_xpos, new_ypos)] + [(img_center, img_center)] * (n_components - 1)\n",
    "    amplitudes = [scaled_src_ampl] + [scaled_cnt_ampl] * (n_components - 1)\n",
    "    fwhms = [scaled_src_fwhm] + [scaled_default_fwhm] * (n_components - 1)\n",
    "\n",
    "    # This is your desired fit call\n",
    "    multi_fit_summary, multi_fit_fig, multi_corner_fig = gaussian_image_fit(\n",
    "        obsid, n_components, positions, amplitudes, fwhms,\n",
    "        prefix=\"g\", background=0.1, pos_max=(logical_width, logical_width),\n",
    "        pos_min=(0, 0), exptime=exptime, confirm=False, \n",
    "        use_mcmc=run_mcmc_multi,     # Use config variable\n",
    "        mcmc_iter=mcmc_iter_multi, # Use config variable\n",
    "        mcmc_scale=current_mcmc_scale\n",
    "    )\n",
    "\n",
    "    if multi_fit_summary is None:\n",
    "        print(f\"Multi-component fit for {obsid} canceled. Skipping.\")\n",
    "        clean()\n",
    "        return (obsid, header_text, centroid_fit_summary, src_fit_summary, \"\", \"\", pdf_out_files, [])\n",
    "\n",
    "    # Save fit plot to temp PNG\n",
    "    temp_multi_fit_png = f\"2Dfits/temp_{obsid}_multi_fit.png\"\n",
    "    multi_fit_fig.savefig(temp_multi_fit_png)\n",
    "    plt.close(multi_fit_fig)\n",
    "    pdf_out_files.append(temp_multi_fit_png)\n",
    "    multi_pdf_out_files.append(temp_multi_fit_png)\n",
    "\n",
    "    # Save corner plot to temp PNG\n",
    "    if multi_corner_fig is not None:\n",
    "        temp_multi_corner_png = f\"2Dfits/temp_{obsid}_multi_corner.png\"\n",
    "        multi_corner_fig.savefig(temp_multi_corner_png)\n",
    "        plt.close(multi_corner_fig)\n",
    "        pdf_out_files.append(temp_multi_corner_png)\n",
    "        multi_pdf_out_files.append(temp_multi_corner_png)\n",
    "\n",
    "    # Create the text for the multi-results file\n",
    "    multi_results_text = (\n",
    "        f\"Observation: {obsid}\\n\"\\\n",
    "        f\"Infile: {infile}\\n\"\\\n",
    "        f\"Date: {date}, Exptime: {exptime}\\n\"\\\n",
    "        f\"{multi_fit_summary}\\n\\n\"\\\n",
    "    )\n",
    "\n",
    "    # Clean up the local Sherpa session\n",
    "    clean()\n",
    "    progress_queue.put(1) # <-- CHECKPOINT 3\n",
    "\n",
    "    # Return all results for aggregation\n",
    "    return (\n",
    "        obsid,\n",
    "        header_text,\n",
    "        centroid_fit_summary,\n",
    "        src_fit_summary,\n",
    "        multi_fit_summary,\n",
    "        multi_results_text,\n",
    "        pdf_out_files,\n",
    "        multi_pdf_out_files\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6a9ef-571f-4bb3-94b7-7abcc06c28a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This guard is essential for multiprocessing in notebooks\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Run Configuration\n",
    "    \n",
    "    # Set the number of components for the multi-component fit\n",
    "    multi_n_components = 3\n",
    "    \n",
    "    # Set to True to run MCMC, or False for a fast test run\n",
    "    run_mcmc = True\n",
    "    \n",
    "    # Set the number of MCMC iterations\n",
    "    mcmc_iterations = 100000\n",
    "\n",
    "    # Static Parameters\n",
    "    \n",
    "    # Set the main working directory\n",
    "    os.chdir('/Users/leodrake/Documents/MIT/ss433/HRC_2024/')\n",
    "\n",
    "    # Define WCS coordinates for each observation\n",
    "    obsid_coords = {\n",
    "        \"26568\": (\"287.9565362\", \"4.9826061\"),\n",
    "        \"26569\": (\"287.9563218\", \"4.9827745\"),\n",
    "        \"26570\": (\"287.9563754\", \"4.9825322\"),\n",
    "        \"26571\": (\"287.9561693\", \"4.9827006\"),\n",
    "        \"26572\": (\"287.9565032\", \"4.9826636\"),\n",
    "        \"26573\": (\"287.9565444\", \"4.9826390\"),\n",
    "        \"26574\": (\"287.9562518\", \"4.9825651\"),\n",
    "        \"26575\": (\"287.9566969\", \"4.9828114\"),\n",
    "        \"26576\": (\"287.9566351\", \"4.9826718\"),\n",
    "        \"26577\": (\"287.9565238\", \"4.9826020\"),\n",
    "        \"26578\": (\"287.9566021\", \"4.9826800\"),\n",
    "        \"26579\": (\"287.9565733\", \"4.9825774\")\n",
    "    }\n",
    "\n",
    "    # Define tuned MCMC scale factors for each observation\n",
    "    mcmc_scale_factors = {\n",
    "        \"26568\": 0.4,   \"26569\": 0.03,  \"26570\": 0.25,\n",
    "        \"26571\": 0.03,  \"26572\": 0.1,   \"26573\": 0.25,\n",
    "        \"26574\": 0.5,   \"26575\": 0.2,   \"26576\": 0.3,\n",
    "        \"26577\": 0.5,   \"26578\": 0.6,   \"26579\": 0.4,\n",
    "    }\n",
    "\n",
    "    # Define the empirical PSF file to be used\n",
    "    emp_psf_file = \"/Users/leodrake/Documents/MIT/ss433/HRC_2024/empPSF_iARLac_v2025_2017-2025.fits\" \n",
    "\n",
    "    # Find all event files to be processed\n",
    "    event_files = sorted(glob.glob('*/repro/*splinecorr.fits'))[:]\n",
    "    \n",
    "    # Define output file names\n",
    "    pdf_out_filename = '2Dfits/0fit-plots.pdf'\n",
    "    multi_pdf_out_filename = '2Dfits/0multi-comp-plots.pdf'\n",
    "    results_filename = '2Dfits/0fit-results.txt'\n",
    "    multi_results_filename = '2Dfits/0multi-comp-fit-results.txt'\n",
    "    \n",
    "    # Define total number of steps for the progress bar\n",
    "    total_steps = len(event_files) * 3 # 3 checkpoints per file\n",
    "\n",
    "    # Run in Parallel\n",
    "    \n",
    "    # Create a Manager and a Queue for progress updates\n",
    "    manager = Manager()\n",
    "    progress_queue = manager.Queue()\n",
    "    \n",
    "    # Create the partial function, pre-loading it with the args\n",
    "    worker_func = partial(process_observation, \n",
    "                          progress_queue=progress_queue,\n",
    "                          obsid_coords=obsid_coords, \n",
    "                          mcmc_scale_factors=mcmc_scale_factors, \n",
    "                          emp_psf_file=emp_psf_file,\n",
    "                          # Pass the new config values to the worker\n",
    "                          n_components_multi=multi_n_components,\n",
    "                          run_mcmc_multi=run_mcmc,\n",
    "                          mcmc_iter_multi=mcmc_iterations\n",
    "                         )\n",
    "\n",
    "    # Set number of cores to use\n",
    "    num_processes = os.cpu_count()\n",
    "    print(f\"--- Starting parallel processing on {num_processes} cores ---\")\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    # Start the pool and run the jobs\n",
    "    with tqdm(total=total_steps, desc=\"Processing Observations\", bar_format=\"{l_bar}{r_bar}\") as pbar:\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            # Use map_async to run jobs without blocking the main thread\n",
    "            async_result = pool.map_async(worker_func, event_files)\n",
    "            \n",
    "            # Monitor the queue and update the progress bar\n",
    "            while not async_result.ready():\n",
    "                while not progress_queue.empty():\n",
    "                    pbar.update(progress_queue.get())\n",
    "                time.sleep(0.1) # Sleep briefly to prevent a busy-wait\n",
    "            \n",
    "            # Update the bar with any remaining items in the queue\n",
    "            while not progress_queue.empty():\n",
    "                pbar.update(progress_queue.get())\n",
    "                \n",
    "            # Get the final results from all workers\n",
    "            results = async_result.get()\n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\n--- Parallel processing complete in {(end_total_time - start_total_time) / 60.0:.2f} minutes ---\")\n",
    "    print(\"--- Aggregating all results... ---\")\n",
    "\n",
    "    # Aggregate Results\n",
    "    \n",
    "    # Sort results by obsid (first item in tuple) to ensure correct order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Initialize master lists for file paths\n",
    "    all_pdf_out_files = []\n",
    "    all_multi_pdf_out_files = []\n",
    "\n",
    "    # Open text files and write all results in order\n",
    "    with open(results_filename, 'w') as results_file, open(multi_results_filename, 'w') as multi_results_file:\n",
    "        for res in results:\n",
    "            # Unpack the tuple from the worker\n",
    "            (obsid, header_text, centroid_fit_summary, src_fit_summary, \n",
    "             multi_fit_summary, multi_results_text, \n",
    "             pdf_out_files_worker, multi_pdf_out_files_worker) = res\n",
    "            \n",
    "            # Write to 0fit-results.txt\n",
    "            results_file.write(header_text)\n",
    "            results_file.write(\"\\nCENTROID FIT SUMMARY:\\n\\n\")\n",
    "            results_file.write(centroid_fit_summary)\n",
    "            results_file.write(\"SOURCE FIT SUMMARY:\\n\\n\")\n",
    "            results_file.write(src_fit_summary)\n",
    "            results_file.write(\"MULTI-COMPONENT FIT SUMMARY:\\n\\n\")\n",
    "            results_file.write(multi_fit_summary)\n",
    "            \n",
    "            # Write to 0multi-comp-fit-results.txt\n",
    "            multi_results_file.write(multi_results_text)\n",
    "            \n",
    "            # Add this worker's PNG files to the master lists\n",
    "            all_pdf_out_files.extend(pdf_out_files_worker)\n",
    "            all_multi_pdf_out_files.extend(multi_pdf_out_files_worker)\n",
    "\n",
    "    print(\"Text files written.\")\n",
    "\n",
    "    # PDF Compilation and Cleanup\n",
    "    print('Compiling PDFs...\\n')\n",
    "\n",
    "    # Helper function to compile PNGs into a single PDF\n",
    "    def compile_pngs_to_pdf(pbar, png_files, pdf_filename):\n",
    "        if not png_files:\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(png_files[0]):\n",
    "            print(f\"ERROR: Cannot find file {png_files[0]} to start PDF.\")\n",
    "            return\n",
    "\n",
    "        images = []\n",
    "        img1 = Image.open(png_files[0]).convert('RGB')\n",
    "        pbar.update(1) # Update bar for first image\n",
    "        \n",
    "        for png_file in png_files[1:]:\n",
    "            if os.path.exists(png_file):\n",
    "                images.append(Image.open(png_file).convert('RGB'))\n",
    "            else:\n",
    "                print(f\"Warning: Missing file {png_file}, skipping.\")\n",
    "            pbar.update(1) # Update bar for each subsequent image\n",
    "        \n",
    "        img1.save(pdf_filename, \"PDF\", resolution=100.0, save_all=True, append_images=images)\n",
    "\n",
    "    # Create one single progress bar for all PDF compilation\n",
    "    total_plots_to_compile = len(all_pdf_out_files) + len(all_multi_pdf_out_files)\n",
    "    \n",
    "    with tqdm(total=total_plots_to_compile, desc=\"Compiling PDF Plots\", bar_format=\"{l_bar}{r_bar}\") as pbar:\n",
    "        try:\n",
    "            # Compile the main PDF (all plots)\n",
    "            compile_pngs_to_pdf(pbar, all_pdf_out_files, pdf_out_filename)\n",
    "            print(f\"Successfully compiled {pdf_out_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not compile {pdf_out_filename}: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Compile the multi-component-only PDF\n",
    "            compile_pngs_to_pdf(pbar, all_multi_pdf_out_files, multi_pdf_out_filename)\n",
    "            print(f\"Successfully compiled {multi_pdf_out_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not compile {multi_pdf_out_filename}: {e}\")\n",
    "\n",
    "    # Final cleanup\n",
    "    print(\"Cleaning up temporary PNG files...\")\n",
    "    temp_files_to_clean = glob.glob(\"2Dfits/temp_*.png\")\n",
    "    for f in temp_files_to_clean:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not remove {f}: {e}\")\n",
    "\n",
    "    print('Process Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8f165-374d-4a72-a070-1d7f57bc2309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CIAO-4.17)",
   "language": "python",
   "name": "ciao-4.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
